{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0             1    15634602   Hargrave          619    France  Female   42   \n",
       "1             2    15647311       Hill          608     Spain  Female   41   \n",
       "2             3    15619304       Onio          502    France  Female   42   \n",
       "3             4    15701354       Boni          699    France  Female   39   \n",
       "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
       "...         ...         ...        ...          ...       ...     ...  ...   \n",
       "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
       "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
       "9997       9998    15584532        Liu          709    France  Female   36   \n",
       "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
       "9999      10000    15628319     Walker          792    France  Female   28   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          2       0.00              1          1               1   \n",
       "1          1   83807.86              1          0               1   \n",
       "2          8  159660.80              3          1               0   \n",
       "3          1       0.00              2          0               0   \n",
       "4          2  125510.82              1          1               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "9995       5       0.00              2          1               0   \n",
       "9996      10   57369.61              1          1               1   \n",
       "9997       7       0.00              1          0               1   \n",
       "9998       3   75075.31              2          1               0   \n",
       "9999       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "...               ...     ...  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bankdata = pd.read_csv('bank.csv')\n",
    "bankdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dropping columns like CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Surname  CreditScore Geography  Gender  Age  Tenure    Balance  \\\n",
       "0      Hargrave          619    France  Female   42       2       0.00   \n",
       "1          Hill          608     Spain  Female   41       1   83807.86   \n",
       "2          Onio          502    France  Female   42       8  159660.80   \n",
       "3          Boni          699    France  Female   39       1       0.00   \n",
       "4      Mitchell          850     Spain  Female   43       2  125510.82   \n",
       "...         ...          ...       ...     ...  ...     ...        ...   \n",
       "9995   Obijiaku          771    France    Male   39       5       0.00   \n",
       "9996  Johnstone          516    France    Male   35      10   57369.61   \n",
       "9997        Liu          709    France  Female   36       7       0.00   \n",
       "9998  Sabbatini          772   Germany    Male   42       3   75075.31   \n",
       "9999     Walker          792    France  Female   28       4  130142.79   \n",
       "\n",
       "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0                 1          1               1        101348.88       1  \n",
       "1                 1          0               1        112542.58       0  \n",
       "2                 3          1               0        113931.57       1  \n",
       "3                 2          0               0         93826.63       0  \n",
       "4                 1          1               1         79084.10       0  \n",
       "...             ...        ...             ...              ...     ...  \n",
       "9995              2          1               0         96270.64       0  \n",
       "9996              1          1               1        101699.77       0  \n",
       "9997              1          0               1         42085.58       1  \n",
       "9998              2          1               0         92888.52       1  \n",
       "9999              1          1               0         38190.78       0  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_1 = bankdata.drop(['RowNumber','CustomerId'], axis=1)\n",
    "bankdata_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Surname          10000 non-null  object \n",
      " 1   CreditScore      10000 non-null  int64  \n",
      " 2   Geography        10000 non-null  object \n",
      " 3   Gender           10000 non-null  object \n",
      " 4   Age              10000 non-null  int64  \n",
      " 5   Tenure           10000 non-null  int64  \n",
      " 6   Balance          10000 non-null  float64\n",
      " 7   NumOfProducts    10000 non-null  int64  \n",
      " 8   HasCrCard        10000 non-null  int64  \n",
      " 9   IsActiveMember   10000 non-null  int64  \n",
      " 10  EstimatedSalary  10000 non-null  float64\n",
      " 11  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "bankdata_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Splitting Features as X_bankdata nd Target variable 'Exited' as y_bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Surname  CreditScore Geography  Gender  Age  Tenure    Balance  \\\n",
       "0  Hargrave          619    France  Female   42       2       0.00   \n",
       "1      Hill          608     Spain  Female   41       1   83807.86   \n",
       "2      Onio          502    France  Female   42       8  159660.80   \n",
       "3      Boni          699    France  Female   39       1       0.00   \n",
       "4  Mitchell          850     Spain  Female   43       2  125510.82   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0              1          1               1        101348.88  \n",
       "1              1          0               1        112542.58  \n",
       "2              3          1               0        113931.57  \n",
       "3              2          0               0         93826.63  \n",
       "4              1          1               1         79084.10  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata = bankdata_1.iloc[:, :-1]\n",
    "X_bankdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dropping 'surname' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surname is not a criteria to detarmine 'churn'. Hence dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0             619    France  Female   42       2       0.00              1   \n",
       "1             608     Spain  Female   41       1   83807.86              1   \n",
       "2             502    France  Female   42       8  159660.80              3   \n",
       "3             699    France  Female   39       1       0.00              2   \n",
       "4             850     Spain  Female   43       2  125510.82              1   \n",
       "...           ...       ...     ...  ...     ...        ...            ...   \n",
       "9995          771    France    Male   39       5       0.00              2   \n",
       "9996          516    France    Male   35      10   57369.61              1   \n",
       "9997          709    France  Female   36       7       0.00              1   \n",
       "9998          772   Germany    Male   42       3   75075.31              2   \n",
       "9999          792    France  Female   28       4  130142.79              1   \n",
       "\n",
       "      HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0             1               1        101348.88  \n",
       "1             0               1        112542.58  \n",
       "2             1               0        113931.57  \n",
       "3             0               0         93826.63  \n",
       "4             1               1         79084.10  \n",
       "...         ...             ...              ...  \n",
       "9995          1               0         96270.64  \n",
       "9996          1               1        101699.77  \n",
       "9997          0               1         42085.58  \n",
       "9998          1               0         92888.52  \n",
       "9999          1               0         38190.78  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata.drop(['Surname'], axis=1, inplace=True)\n",
    "X_bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5084\n",
       "2    4590\n",
       "3     266\n",
       "4      60\n",
       "Name: NumOfProducts, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata['NumOfProducts'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     1048\n",
       "1     1035\n",
       "7     1028\n",
       "8     1025\n",
       "5     1012\n",
       "3     1009\n",
       "4      989\n",
       "9      984\n",
       "6      967\n",
       "10     490\n",
       "0      413\n",
       "Name: Tenure, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata['Tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "France     5014\n",
       "Germany    2509\n",
       "Spain      2477\n",
       "Name: Geography, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male      5457\n",
       "Female    4543\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding for 'NumOfProducts' and 'Tenure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>28</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore  Age    Balance  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
       "0             619   42       0.00          1               1        101348.88   \n",
       "1             608   41   83807.86          0               1        112542.58   \n",
       "2             502   42  159660.80          1               0        113931.57   \n",
       "3             699   39       0.00          0               0         93826.63   \n",
       "4             850   43  125510.82          1               1         79084.10   \n",
       "...           ...  ...        ...        ...             ...              ...   \n",
       "9995          771   39       0.00          1               0         96270.64   \n",
       "9996          516   35   57369.61          1               1        101699.77   \n",
       "9997          709   36       0.00          0               1         42085.58   \n",
       "9998          772   42   75075.31          1               0         92888.52   \n",
       "9999          792   28  130142.79          1               0         38190.78   \n",
       "\n",
       "      NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  NumOfProducts_4  \\\n",
       "0                   1                0                0                0   \n",
       "1                   1                0                0                0   \n",
       "2                   0                0                1                0   \n",
       "3                   0                1                0                0   \n",
       "4                   1                0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "9995                0                1                0                0   \n",
       "9996                1                0                0                0   \n",
       "9997                1                0                0                0   \n",
       "9998                0                1                0                0   \n",
       "9999                1                0                0                0   \n",
       "\n",
       "      Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  Tenure_5  Tenure_6  \\\n",
       "0            0         0         1         0         0         0         0   \n",
       "1            0         1         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         1         0         0         0         0         0   \n",
       "4            0         0         1         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995         0         0         0         0         0         1         0   \n",
       "9996         0         0         0         0         0         0         0   \n",
       "9997         0         0         0         0         0         0         0   \n",
       "9998         0         0         0         1         0         0         0   \n",
       "9999         0         0         0         0         1         0         0   \n",
       "\n",
       "      Tenure_7  Tenure_8  Tenure_9  Tenure_10  Geography_France  \\\n",
       "0            0         0         0          0                 1   \n",
       "1            0         0         0          0                 0   \n",
       "2            0         1         0          0                 1   \n",
       "3            0         0         0          0                 1   \n",
       "4            0         0         0          0                 0   \n",
       "...        ...       ...       ...        ...               ...   \n",
       "9995         0         0         0          0                 1   \n",
       "9996         0         0         0          1                 1   \n",
       "9997         1         0         0          0                 1   \n",
       "9998         0         0         0          0                 0   \n",
       "9999         0         0         0          0                 1   \n",
       "\n",
       "      Geography_Germany  Geography_Spain  Gender_Female  Gender_Male  \n",
       "0                     0                0              1            0  \n",
       "1                     0                1              1            0  \n",
       "2                     0                0              1            0  \n",
       "3                     0                0              1            0  \n",
       "4                     0                1              1            0  \n",
       "...                 ...              ...            ...          ...  \n",
       "9995                  0                0              0            1  \n",
       "9996                  0                0              0            1  \n",
       "9997                  0                0              1            0  \n",
       "9998                  1                0              0            1  \n",
       "9999                  0                0              1            0  \n",
       "\n",
       "[10000 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotCols = ['NumOfProducts', 'Tenure','Geography','Gender']\n",
    "X_bankdata = pd.get_dummies(X_bankdata, columns = oneHotCols)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "X_bankdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to float since during neural network calculations, float is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bankdata = X_bankdata.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CreditScore        10000 non-null  float64\n",
      " 1   Age                10000 non-null  float64\n",
      " 2   Balance            10000 non-null  float64\n",
      " 3   HasCrCard          10000 non-null  float64\n",
      " 4   IsActiveMember     10000 non-null  float64\n",
      " 5   EstimatedSalary    10000 non-null  float64\n",
      " 6   NumOfProducts_1    10000 non-null  float64\n",
      " 7   NumOfProducts_2    10000 non-null  float64\n",
      " 8   NumOfProducts_3    10000 non-null  float64\n",
      " 9   NumOfProducts_4    10000 non-null  float64\n",
      " 10  Tenure_0           10000 non-null  float64\n",
      " 11  Tenure_1           10000 non-null  float64\n",
      " 12  Tenure_2           10000 non-null  float64\n",
      " 13  Tenure_3           10000 non-null  float64\n",
      " 14  Tenure_4           10000 non-null  float64\n",
      " 15  Tenure_5           10000 non-null  float64\n",
      " 16  Tenure_6           10000 non-null  float64\n",
      " 17  Tenure_7           10000 non-null  float64\n",
      " 18  Tenure_8           10000 non-null  float64\n",
      " 19  Tenure_9           10000 non-null  float64\n",
      " 20  Tenure_10          10000 non-null  float64\n",
      " 21  Geography_France   10000 non-null  float64\n",
      " 22  Geography_Germany  10000 non-null  float64\n",
      " 23  Geography_Spain    10000 non-null  float64\n",
      " 24  Gender_Female      10000 non-null  float64\n",
      " 25  Gender_Male        10000 non-null  float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "X_bankdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "0           619.0  42.0       0.00        1.0             1.0   \n",
       "1           608.0  41.0   83807.86        0.0             1.0   \n",
       "2           502.0  42.0  159660.80        1.0             0.0   \n",
       "3           699.0  39.0       0.00        0.0             0.0   \n",
       "4           850.0  43.0  125510.82        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "9995        771.0  39.0       0.00        1.0             0.0   \n",
       "9996        516.0  35.0   57369.61        1.0             1.0   \n",
       "9997        709.0  36.0       0.00        0.0             1.0   \n",
       "9998        772.0  42.0   75075.31        1.0             0.0   \n",
       "9999        792.0  28.0  130142.79        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "0           101348.88              1.0              0.0              0.0   \n",
       "1           112542.58              1.0              0.0              0.0   \n",
       "2           113931.57              0.0              0.0              1.0   \n",
       "3            93826.63              0.0              1.0              0.0   \n",
       "4            79084.10              1.0              0.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "9995         96270.64              0.0              1.0              0.0   \n",
       "9996        101699.77              1.0              0.0              0.0   \n",
       "9997         42085.58              1.0              0.0              0.0   \n",
       "9998         92888.52              0.0              1.0              0.0   \n",
       "9999         38190.78              1.0              0.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "0                 0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "1                 0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2                 0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3                 0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "4                 0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "9995              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9996              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9997              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9998              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "9999              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "0          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "2          0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "4          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "9995       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9996       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "9997       0.0       0.0       1.0       0.0       0.0        0.0   \n",
       "9998       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9999       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "0                  1.0                0.0              0.0            1.0   \n",
       "1                  0.0                0.0              1.0            1.0   \n",
       "2                  1.0                0.0              0.0            1.0   \n",
       "3                  1.0                0.0              0.0            1.0   \n",
       "4                  0.0                0.0              1.0            1.0   \n",
       "...                ...                ...              ...            ...   \n",
       "9995               1.0                0.0              0.0            0.0   \n",
       "9996               1.0                0.0              0.0            0.0   \n",
       "9997               1.0                0.0              0.0            1.0   \n",
       "9998               0.0                1.0              0.0            0.0   \n",
       "9999               1.0                0.0              0.0            1.0   \n",
       "\n",
       "      Gender_Male  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "...           ...  \n",
       "9995          1.0  \n",
       "9996          1.0  \n",
       "9997          0.0  \n",
       "9998          1.0  \n",
       "9999          0.0  \n",
       "\n",
       "[10000 rows x 26 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bankdata = bankdata_1.iloc[:,-1]\n",
    "y_bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bankdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Dividing into Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bankdata, y_bankdata, test_size = 0.3, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>709.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56214.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>797.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117916.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>470.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>101140.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50906.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>835.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>130420.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106276.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>626.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>132287.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51467.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5795        709.0  39.0       0.00        1.0             0.0   \n",
       "1490        797.0  31.0       0.00        1.0             0.0   \n",
       "3807        470.0  30.0  101140.76        1.0             1.0   \n",
       "3042        835.0  29.0  130420.20        0.0             0.0   \n",
       "4064        626.0  39.0  132287.92        1.0             1.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5795         56214.09              0.0              1.0              0.0   \n",
       "1490        117916.63              0.0              1.0              0.0   \n",
       "3807         50906.65              1.0              0.0              0.0   \n",
       "3042        106276.55              0.0              1.0              0.0   \n",
       "4064         51467.92              0.0              0.0              1.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5795              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1490              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3807              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3042              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5795       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "1490       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3807       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5795               1.0                0.0              0.0            0.0   \n",
       "1490               0.0                0.0              1.0            1.0   \n",
       "3807               1.0                0.0              0.0            0.0   \n",
       "3042               0.0                1.0              0.0            1.0   \n",
       "4064               0.0                1.0              0.0            0.0   \n",
       "\n",
       "      Gender_Male  \n",
       "5795          1.0  \n",
       "1490          0.0  \n",
       "3807          1.0  \n",
       "3042          0.0  \n",
       "4064          1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 26)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7000 entries, 5795 to 5994\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CreditScore        7000 non-null   float64\n",
      " 1   Age                7000 non-null   float64\n",
      " 2   Balance            7000 non-null   float64\n",
      " 3   HasCrCard          7000 non-null   float64\n",
      " 4   IsActiveMember     7000 non-null   float64\n",
      " 5   EstimatedSalary    7000 non-null   float64\n",
      " 6   NumOfProducts_1    7000 non-null   float64\n",
      " 7   NumOfProducts_2    7000 non-null   float64\n",
      " 8   NumOfProducts_3    7000 non-null   float64\n",
      " 9   NumOfProducts_4    7000 non-null   float64\n",
      " 10  Tenure_0           7000 non-null   float64\n",
      " 11  Tenure_1           7000 non-null   float64\n",
      " 12  Tenure_2           7000 non-null   float64\n",
      " 13  Tenure_3           7000 non-null   float64\n",
      " 14  Tenure_4           7000 non-null   float64\n",
      " 15  Tenure_5           7000 non-null   float64\n",
      " 16  Tenure_6           7000 non-null   float64\n",
      " 17  Tenure_7           7000 non-null   float64\n",
      " 18  Tenure_8           7000 non-null   float64\n",
      " 19  Tenure_9           7000 non-null   float64\n",
      " 20  Tenure_10          7000 non-null   float64\n",
      " 21  Geography_France   7000 non-null   float64\n",
      " 22  Geography_Germany  7000 non-null   float64\n",
      " 23  Geography_Spain    7000 non-null   float64\n",
      " 24  Gender_Female      7000 non-null   float64\n",
      " 25  Gender_Male        7000 non-null   float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>704.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>111525.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199484.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>641.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93148.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>555.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>120392.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177719.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>474.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>176311.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160213.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>610.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62232.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5876        704.0  39.0  111525.02        1.0             0.0   \n",
       "6555        641.0  35.0       0.00        1.0             0.0   \n",
       "1448        555.0  46.0  120392.99        1.0             0.0   \n",
       "3351        474.0  34.0  176311.36        1.0             0.0   \n",
       "231         610.0  40.0       0.00        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5876        199484.96              1.0              0.0              0.0   \n",
       "6555         93148.93              0.0              1.0              0.0   \n",
       "1448        177719.88              1.0              0.0              0.0   \n",
       "3351        160213.27              1.0              0.0              0.0   \n",
       "231          62232.60              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5876              0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "6555              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1448              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "3351              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "231               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "6555       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1448       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3351       0.0       0.0       0.0       0.0       1.0        0.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5876               1.0                0.0              0.0            0.0   \n",
       "6555               1.0                0.0              0.0            0.0   \n",
       "1448               0.0                1.0              0.0            1.0   \n",
       "3351               0.0                1.0              0.0            1.0   \n",
       "231                1.0                0.0              0.0            0.0   \n",
       "\n",
       "      Gender_Male  \n",
       "5876          1.0  \n",
       "6555          1.0  \n",
       "1448          0.0  \n",
       "3351          0.0  \n",
       "231           1.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 26)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3000 entries, 5876 to 676\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CreditScore        3000 non-null   float64\n",
      " 1   Age                3000 non-null   float64\n",
      " 2   Balance            3000 non-null   float64\n",
      " 3   HasCrCard          3000 non-null   float64\n",
      " 4   IsActiveMember     3000 non-null   float64\n",
      " 5   EstimatedSalary    3000 non-null   float64\n",
      " 6   NumOfProducts_1    3000 non-null   float64\n",
      " 7   NumOfProducts_2    3000 non-null   float64\n",
      " 8   NumOfProducts_3    3000 non-null   float64\n",
      " 9   NumOfProducts_4    3000 non-null   float64\n",
      " 10  Tenure_0           3000 non-null   float64\n",
      " 11  Tenure_1           3000 non-null   float64\n",
      " 12  Tenure_2           3000 non-null   float64\n",
      " 13  Tenure_3           3000 non-null   float64\n",
      " 14  Tenure_4           3000 non-null   float64\n",
      " 15  Tenure_5           3000 non-null   float64\n",
      " 16  Tenure_6           3000 non-null   float64\n",
      " 17  Tenure_7           3000 non-null   float64\n",
      " 18  Tenure_8           3000 non-null   float64\n",
      " 19  Tenure_9           3000 non-null   float64\n",
      " 20  Tenure_10          3000 non-null   float64\n",
      " 21  Geography_France   3000 non-null   float64\n",
      " 22  Geography_Germany  3000 non-null   float64\n",
      " 23  Geography_Spain    3000 non-null   float64\n",
      " 24  Gender_Female      3000 non-null   float64\n",
      " 25  Gender_Male        3000 non-null   float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 632.8 KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5795    0\n",
       "1490    0\n",
       "3807    0\n",
       "3042    0\n",
       "4064    1\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5876    0\n",
       "6555    0\n",
       "1448    1\n",
       "3351    0\n",
       "231     0\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of True 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2381, 1: 619}\n"
     ]
    }
   ],
   "source": [
    "# Print predicted 1 and 0's \n",
    "unique1, counts1 = np.unique(y_test, return_counts = True)\n",
    "print('\\nThe number of True 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique1, counts1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Normailizing Train and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Normalize the X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.preprocessing.normalization.Normalization object at 0x0000021BC8C3AF08>\n",
      "[6.5076428e+02 3.8854572e+01 7.6307844e+04 9.9459703e+04]\n",
      "[9.2735283e+03 1.0949428e+02 3.8906243e+09 3.3095567e+09]\n"
     ]
    }
   ],
   "source": [
    "# Create the Normalization layer and adapt it to train data. This will get us mean and variance of train data\n",
    "normalizer_train = Normalization()\n",
    "normalizer_train.adapt(np.array(X_train[['CreditScore','Age','Balance','EstimatedSalary']]))\n",
    "print(normalizer_train)\n",
    "print(normalizer_train.mean.numpy())\n",
    "print(normalizer_train.variance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6047375 ,  0.01389797, -1.2233748 , -0.7517218 ],\n",
       "       [ 1.5185565 , -0.7506316 , -1.2233748 ,  0.3208297 ],\n",
       "       [-1.8771117 , -0.8461978 ,  0.3981237 , -0.843979  ],\n",
       "       ...,\n",
       "       [ 0.28282404,  1.5429571 ,  0.8551371 ,  1.4717058 ],\n",
       "       [-0.51676756,  0.20503037, -1.2233748 ,  1.0629009 ],\n",
       "       [-0.73483795, -0.3683668 ,  0.9615021 , -1.29779   ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_to_normalize = np.array(X_train[['CreditScore','Age','Balance','EstimatedSalary']])\n",
    "\n",
    "normalizer_train(train_data_to_normalize).numpy() # apply the normalization to the selected same features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6047375 ,  1.5185565 , -1.8771117 , ...,  0.28282404,\n",
       "       -0.51676756, -0.73483795], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditScore_norm = normalizer_train(train_data_to_normalize).numpy()[:,0]\n",
    "creditScore_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>709.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56214.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>797.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117916.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.518556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>470.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>101140.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50906.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.877112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>835.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>130420.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106276.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.913160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>626.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>132287.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51467.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.257160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>676.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179066.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>778.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162809.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.321255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>678.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>129646.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>184125.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>601.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160607.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.516768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>580.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>136281.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24799.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5795        709.0  39.0       0.00        1.0             0.0   \n",
       "1490        797.0  31.0       0.00        1.0             0.0   \n",
       "3807        470.0  30.0  101140.76        1.0             1.0   \n",
       "3042        835.0  29.0  130420.20        0.0             0.0   \n",
       "4064        626.0  39.0  132287.92        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "6400        676.0  30.0       0.00        0.0             0.0   \n",
       "9160        778.0  24.0       0.00        1.0             1.0   \n",
       "9859        678.0  55.0  129646.91        1.0             1.0   \n",
       "1688        601.0  41.0       0.00        0.0             1.0   \n",
       "5994        580.0  35.0  136281.41        1.0             1.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5795         56214.09              0.0              1.0              0.0   \n",
       "1490        117916.63              0.0              1.0              0.0   \n",
       "3807         50906.65              1.0              0.0              0.0   \n",
       "3042        106276.55              0.0              1.0              0.0   \n",
       "4064         51467.92              0.0              0.0              1.0   \n",
       "...               ...              ...              ...              ...   \n",
       "6400        179066.58              0.0              1.0              0.0   \n",
       "9160        162809.20              0.0              1.0              0.0   \n",
       "9859        184125.10              1.0              0.0              0.0   \n",
       "1688        160607.06              0.0              1.0              0.0   \n",
       "5994         24799.47              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5795              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1490              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3807              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3042              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "6400              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9160              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "9859              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "1688              0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "5994              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5795       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "1490       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3807       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "6400       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9160       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9859       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1688       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5994       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5795               1.0                0.0              0.0            0.0   \n",
       "1490               0.0                0.0              1.0            1.0   \n",
       "3807               1.0                0.0              0.0            0.0   \n",
       "3042               0.0                1.0              0.0            1.0   \n",
       "4064               0.0                1.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "6400               0.0                0.0              1.0            1.0   \n",
       "9160               1.0                0.0              0.0            0.0   \n",
       "9859               0.0                1.0              0.0            0.0   \n",
       "1688               1.0                0.0              0.0            1.0   \n",
       "5994               0.0                1.0              0.0            0.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  \n",
       "5795          1.0          0.604738  \n",
       "1490          0.0          1.518556  \n",
       "3807          1.0         -1.877112  \n",
       "3042          0.0          1.913160  \n",
       "4064          1.0         -0.257160  \n",
       "...           ...               ...  \n",
       "6400          0.0          0.262055  \n",
       "9160          1.0          1.321255  \n",
       "9859          1.0          0.282824  \n",
       "1688          0.0         -0.516768  \n",
       "5994          1.0         -0.734838  \n",
       "\n",
       "[7000 rows x 27 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['CreditScore_norm'] = creditScore_norm.tolist()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01389797, -0.7506316 , -0.8461978 , ...,  1.5429571 ,\n",
       "        0.20503037, -0.3683668 ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Age_norm = normalizer_train(train_data_to_normalize).numpy()[:,1]\n",
    "Age_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>709.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56214.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>0.013898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>797.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117916.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.518556</td>\n",
       "      <td>-0.750632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>470.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>101140.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50906.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.877112</td>\n",
       "      <td>-0.846198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>835.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>130420.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106276.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.913160</td>\n",
       "      <td>-0.941764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>626.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>132287.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51467.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.257160</td>\n",
       "      <td>0.013898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>676.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179066.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262055</td>\n",
       "      <td>-0.846198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>778.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162809.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.321255</td>\n",
       "      <td>-1.419595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>678.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>129646.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>184125.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282824</td>\n",
       "      <td>1.542957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>601.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160607.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.516768</td>\n",
       "      <td>0.205030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>580.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>136281.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24799.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734838</td>\n",
       "      <td>-0.368367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5795        709.0  39.0       0.00        1.0             0.0   \n",
       "1490        797.0  31.0       0.00        1.0             0.0   \n",
       "3807        470.0  30.0  101140.76        1.0             1.0   \n",
       "3042        835.0  29.0  130420.20        0.0             0.0   \n",
       "4064        626.0  39.0  132287.92        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "6400        676.0  30.0       0.00        0.0             0.0   \n",
       "9160        778.0  24.0       0.00        1.0             1.0   \n",
       "9859        678.0  55.0  129646.91        1.0             1.0   \n",
       "1688        601.0  41.0       0.00        0.0             1.0   \n",
       "5994        580.0  35.0  136281.41        1.0             1.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5795         56214.09              0.0              1.0              0.0   \n",
       "1490        117916.63              0.0              1.0              0.0   \n",
       "3807         50906.65              1.0              0.0              0.0   \n",
       "3042        106276.55              0.0              1.0              0.0   \n",
       "4064         51467.92              0.0              0.0              1.0   \n",
       "...               ...              ...              ...              ...   \n",
       "6400        179066.58              0.0              1.0              0.0   \n",
       "9160        162809.20              0.0              1.0              0.0   \n",
       "9859        184125.10              1.0              0.0              0.0   \n",
       "1688        160607.06              0.0              1.0              0.0   \n",
       "5994         24799.47              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5795              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1490              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3807              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3042              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "6400              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9160              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "9859              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "1688              0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "5994              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5795       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "1490       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3807       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "6400       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9160       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9859       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1688       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5994       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5795               1.0                0.0              0.0            0.0   \n",
       "1490               0.0                0.0              1.0            1.0   \n",
       "3807               1.0                0.0              0.0            0.0   \n",
       "3042               0.0                1.0              0.0            1.0   \n",
       "4064               0.0                1.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "6400               0.0                0.0              1.0            1.0   \n",
       "9160               1.0                0.0              0.0            0.0   \n",
       "9859               0.0                1.0              0.0            0.0   \n",
       "1688               1.0                0.0              0.0            1.0   \n",
       "5994               0.0                1.0              0.0            0.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  Age_norm  \n",
       "5795          1.0          0.604738  0.013898  \n",
       "1490          0.0          1.518556 -0.750632  \n",
       "3807          1.0         -1.877112 -0.846198  \n",
       "3042          0.0          1.913160 -0.941764  \n",
       "4064          1.0         -0.257160  0.013898  \n",
       "...           ...               ...       ...  \n",
       "6400          0.0          0.262055 -0.846198  \n",
       "9160          1.0          1.321255 -1.419595  \n",
       "9859          1.0          0.282824  1.542957  \n",
       "1688          0.0         -0.516768  0.205030  \n",
       "5994          1.0         -0.734838 -0.368367  \n",
       "\n",
       "[7000 rows x 28 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Age_norm'] = Age_norm.tolist()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2233748, -1.2233748,  0.3981237, ...,  0.8551371, -1.2233748,\n",
       "        0.9615021], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Balance_norm = normalizer_train(train_data_to_normalize).numpy()[:,2]\n",
    "Balance_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>709.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56214.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>-1.223375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>797.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117916.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.518556</td>\n",
       "      <td>-0.750632</td>\n",
       "      <td>-1.223375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>470.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>101140.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50906.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.877112</td>\n",
       "      <td>-0.846198</td>\n",
       "      <td>0.398124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>835.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>130420.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106276.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.913160</td>\n",
       "      <td>-0.941764</td>\n",
       "      <td>0.867535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>626.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>132287.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51467.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.257160</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.897478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>676.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179066.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262055</td>\n",
       "      <td>-0.846198</td>\n",
       "      <td>-1.223375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>778.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162809.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.321255</td>\n",
       "      <td>-1.419595</td>\n",
       "      <td>-1.223375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>678.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>129646.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>184125.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282824</td>\n",
       "      <td>1.542957</td>\n",
       "      <td>0.855137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>601.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160607.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.516768</td>\n",
       "      <td>0.205030</td>\n",
       "      <td>-1.223375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>580.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>136281.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24799.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734838</td>\n",
       "      <td>-0.368367</td>\n",
       "      <td>0.961502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5795        709.0  39.0       0.00        1.0             0.0   \n",
       "1490        797.0  31.0       0.00        1.0             0.0   \n",
       "3807        470.0  30.0  101140.76        1.0             1.0   \n",
       "3042        835.0  29.0  130420.20        0.0             0.0   \n",
       "4064        626.0  39.0  132287.92        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "6400        676.0  30.0       0.00        0.0             0.0   \n",
       "9160        778.0  24.0       0.00        1.0             1.0   \n",
       "9859        678.0  55.0  129646.91        1.0             1.0   \n",
       "1688        601.0  41.0       0.00        0.0             1.0   \n",
       "5994        580.0  35.0  136281.41        1.0             1.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5795         56214.09              0.0              1.0              0.0   \n",
       "1490        117916.63              0.0              1.0              0.0   \n",
       "3807         50906.65              1.0              0.0              0.0   \n",
       "3042        106276.55              0.0              1.0              0.0   \n",
       "4064         51467.92              0.0              0.0              1.0   \n",
       "...               ...              ...              ...              ...   \n",
       "6400        179066.58              0.0              1.0              0.0   \n",
       "9160        162809.20              0.0              1.0              0.0   \n",
       "9859        184125.10              1.0              0.0              0.0   \n",
       "1688        160607.06              0.0              1.0              0.0   \n",
       "5994         24799.47              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5795              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1490              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3807              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3042              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "6400              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9160              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "9859              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "1688              0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "5994              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5795       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "1490       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3807       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "6400       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9160       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9859       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1688       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5994       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5795               1.0                0.0              0.0            0.0   \n",
       "1490               0.0                0.0              1.0            1.0   \n",
       "3807               1.0                0.0              0.0            0.0   \n",
       "3042               0.0                1.0              0.0            1.0   \n",
       "4064               0.0                1.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "6400               0.0                0.0              1.0            1.0   \n",
       "9160               1.0                0.0              0.0            0.0   \n",
       "9859               0.0                1.0              0.0            0.0   \n",
       "1688               1.0                0.0              0.0            1.0   \n",
       "5994               0.0                1.0              0.0            0.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  Age_norm  Balance_norm  \n",
       "5795          1.0          0.604738  0.013898     -1.223375  \n",
       "1490          0.0          1.518556 -0.750632     -1.223375  \n",
       "3807          1.0         -1.877112 -0.846198      0.398124  \n",
       "3042          0.0          1.913160 -0.941764      0.867535  \n",
       "4064          1.0         -0.257160  0.013898      0.897478  \n",
       "...           ...               ...       ...           ...  \n",
       "6400          0.0          0.262055 -0.846198     -1.223375  \n",
       "9160          1.0          1.321255 -1.419595     -1.223375  \n",
       "9859          1.0          0.282824  1.542957      0.855137  \n",
       "1688          0.0         -0.516768  0.205030     -1.223375  \n",
       "5994          1.0         -0.734838 -0.368367      0.961502  \n",
       "\n",
       "[7000 rows x 29 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Balance_norm'] = Balance_norm.tolist()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7517218,  0.3208297, -0.843979 , ...,  1.4717058,  1.0629009,\n",
       "       -1.29779  ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EstimatedSalary_norm = normalizer_train(train_data_to_normalize).numpy()[:,3]\n",
    "EstimatedSalary_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>709.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56214.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>-0.751722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>797.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117916.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.518556</td>\n",
       "      <td>-0.750632</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>0.320830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>470.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>101140.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50906.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.877112</td>\n",
       "      <td>-0.846198</td>\n",
       "      <td>0.398124</td>\n",
       "      <td>-0.843979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>835.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>130420.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106276.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.913160</td>\n",
       "      <td>-0.941764</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>0.118495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>626.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>132287.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51467.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.257160</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.897478</td>\n",
       "      <td>-0.834223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>676.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179066.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262055</td>\n",
       "      <td>-0.846198</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>1.383776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>778.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162809.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.321255</td>\n",
       "      <td>-1.419595</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>1.101180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>678.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>129646.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>184125.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282824</td>\n",
       "      <td>1.542957</td>\n",
       "      <td>0.855137</td>\n",
       "      <td>1.471706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>601.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160607.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.516768</td>\n",
       "      <td>0.205030</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>1.062901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>580.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>136281.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24799.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734838</td>\n",
       "      <td>-0.368367</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>-1.297790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5795        709.0  39.0       0.00        1.0             0.0   \n",
       "1490        797.0  31.0       0.00        1.0             0.0   \n",
       "3807        470.0  30.0  101140.76        1.0             1.0   \n",
       "3042        835.0  29.0  130420.20        0.0             0.0   \n",
       "4064        626.0  39.0  132287.92        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "6400        676.0  30.0       0.00        0.0             0.0   \n",
       "9160        778.0  24.0       0.00        1.0             1.0   \n",
       "9859        678.0  55.0  129646.91        1.0             1.0   \n",
       "1688        601.0  41.0       0.00        0.0             1.0   \n",
       "5994        580.0  35.0  136281.41        1.0             1.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5795         56214.09              0.0              1.0              0.0   \n",
       "1490        117916.63              0.0              1.0              0.0   \n",
       "3807         50906.65              1.0              0.0              0.0   \n",
       "3042        106276.55              0.0              1.0              0.0   \n",
       "4064         51467.92              0.0              0.0              1.0   \n",
       "...               ...              ...              ...              ...   \n",
       "6400        179066.58              0.0              1.0              0.0   \n",
       "9160        162809.20              0.0              1.0              0.0   \n",
       "9859        184125.10              1.0              0.0              0.0   \n",
       "1688        160607.06              0.0              1.0              0.0   \n",
       "5994         24799.47              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5795              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1490              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3807              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3042              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "6400              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9160              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "9859              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "1688              0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "5994              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5795       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "1490       0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3807       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "6400       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9160       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9859       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1688       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5994       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5795               1.0                0.0              0.0            0.0   \n",
       "1490               0.0                0.0              1.0            1.0   \n",
       "3807               1.0                0.0              0.0            0.0   \n",
       "3042               0.0                1.0              0.0            1.0   \n",
       "4064               0.0                1.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "6400               0.0                0.0              1.0            1.0   \n",
       "9160               1.0                0.0              0.0            0.0   \n",
       "9859               0.0                1.0              0.0            0.0   \n",
       "1688               1.0                0.0              0.0            1.0   \n",
       "5994               0.0                1.0              0.0            0.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  Age_norm  Balance_norm  \\\n",
       "5795          1.0          0.604738  0.013898     -1.223375   \n",
       "1490          0.0          1.518556 -0.750632     -1.223375   \n",
       "3807          1.0         -1.877112 -0.846198      0.398124   \n",
       "3042          0.0          1.913160 -0.941764      0.867535   \n",
       "4064          1.0         -0.257160  0.013898      0.897478   \n",
       "...           ...               ...       ...           ...   \n",
       "6400          0.0          0.262055 -0.846198     -1.223375   \n",
       "9160          1.0          1.321255 -1.419595     -1.223375   \n",
       "9859          1.0          0.282824  1.542957      0.855137   \n",
       "1688          0.0         -0.516768  0.205030     -1.223375   \n",
       "5994          1.0         -0.734838 -0.368367      0.961502   \n",
       "\n",
       "      EstimatedSalary_norm  \n",
       "5795             -0.751722  \n",
       "1490              0.320830  \n",
       "3807             -0.843979  \n",
       "3042              0.118495  \n",
       "4064             -0.834223  \n",
       "...                    ...  \n",
       "6400              1.383776  \n",
       "9160              1.101180  \n",
       "9859              1.471706  \n",
       "1688              1.062901  \n",
       "5994             -1.297790  \n",
       "\n",
       "[7000 rows x 30 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['EstimatedSalary_norm'] = EstimatedSalary_norm.tolist()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the non normalized feature values since we have normalized features in diff columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['CreditScore','Age','Balance','EstimatedSalary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>-0.751722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.518556</td>\n",
       "      <td>-0.750632</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>0.320830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.877112</td>\n",
       "      <td>-0.846198</td>\n",
       "      <td>0.398124</td>\n",
       "      <td>-0.843979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.913160</td>\n",
       "      <td>-0.941764</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>0.118495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.257160</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.897478</td>\n",
       "      <td>-0.834223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262055</td>\n",
       "      <td>-0.846198</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>1.383776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.321255</td>\n",
       "      <td>-1.419595</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>1.101180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282824</td>\n",
       "      <td>1.542957</td>\n",
       "      <td>0.855137</td>\n",
       "      <td>1.471706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.516768</td>\n",
       "      <td>0.205030</td>\n",
       "      <td>-1.223375</td>\n",
       "      <td>1.062901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734838</td>\n",
       "      <td>-0.368367</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>-1.297790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "5795        1.0             0.0              0.0              1.0   \n",
       "1490        1.0             0.0              0.0              1.0   \n",
       "3807        1.0             1.0              1.0              0.0   \n",
       "3042        0.0             0.0              0.0              1.0   \n",
       "4064        1.0             1.0              0.0              0.0   \n",
       "...         ...             ...              ...              ...   \n",
       "6400        0.0             0.0              0.0              1.0   \n",
       "9160        1.0             1.0              0.0              1.0   \n",
       "9859        1.0             1.0              1.0              0.0   \n",
       "1688        0.0             1.0              0.0              1.0   \n",
       "5994        1.0             1.0              0.0              1.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "5795              0.0              0.0       0.0       0.0       0.0   \n",
       "1490              0.0              0.0       0.0       0.0       0.0   \n",
       "3807              0.0              0.0       0.0       0.0       0.0   \n",
       "3042              0.0              0.0       0.0       0.0       0.0   \n",
       "4064              1.0              0.0       0.0       0.0       0.0   \n",
       "...               ...              ...       ...       ...       ...   \n",
       "6400              0.0              0.0       0.0       0.0       0.0   \n",
       "9160              0.0              0.0       0.0       0.0       0.0   \n",
       "9859              0.0              0.0       0.0       0.0       0.0   \n",
       "1688              0.0              0.0       0.0       1.0       0.0   \n",
       "5994              0.0              0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "5795       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "1490       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3807       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6400       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "9160       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9859       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1688       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5994       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "5795        0.0               1.0                0.0              0.0   \n",
       "1490        0.0               0.0                0.0              1.0   \n",
       "3807        0.0               1.0                0.0              0.0   \n",
       "3042        1.0               0.0                1.0              0.0   \n",
       "4064        1.0               0.0                1.0              0.0   \n",
       "...         ...               ...                ...              ...   \n",
       "6400        0.0               0.0                0.0              1.0   \n",
       "9160        0.0               1.0                0.0              0.0   \n",
       "9859        0.0               0.0                1.0              0.0   \n",
       "1688        0.0               1.0                0.0              0.0   \n",
       "5994        1.0               0.0                1.0              0.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  CreditScore_norm  Age_norm  Balance_norm  \\\n",
       "5795            0.0          1.0          0.604738  0.013898     -1.223375   \n",
       "1490            1.0          0.0          1.518556 -0.750632     -1.223375   \n",
       "3807            0.0          1.0         -1.877112 -0.846198      0.398124   \n",
       "3042            1.0          0.0          1.913160 -0.941764      0.867535   \n",
       "4064            0.0          1.0         -0.257160  0.013898      0.897478   \n",
       "...             ...          ...               ...       ...           ...   \n",
       "6400            1.0          0.0          0.262055 -0.846198     -1.223375   \n",
       "9160            0.0          1.0          1.321255 -1.419595     -1.223375   \n",
       "9859            0.0          1.0          0.282824  1.542957      0.855137   \n",
       "1688            1.0          0.0         -0.516768  0.205030     -1.223375   \n",
       "5994            0.0          1.0         -0.734838 -0.368367      0.961502   \n",
       "\n",
       "      EstimatedSalary_norm  \n",
       "5795             -0.751722  \n",
       "1490              0.320830  \n",
       "3807             -0.843979  \n",
       "3042              0.118495  \n",
       "4064             -0.834223  \n",
       "...                    ...  \n",
       "6400              1.383776  \n",
       "9160              1.101180  \n",
       "9859              1.471706  \n",
       "1688              1.062901  \n",
       "5994             -1.297790  \n",
       "\n",
       "[7000 rows x 26 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2  Normalize the X_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.preprocessing.normalization.Normalization object at 0x0000021BC9E45C48>\n",
      "[6.4997931e+02 3.9078667e+01 7.6901336e+04 1.0156149e+05]\n",
      "[9.4977559e+03 1.1108848e+02 3.8984530e+09 3.2983624e+09]\n"
     ]
    }
   ],
   "source": [
    "# Create the Normalization layer and adapt it to train data. This will get us mean and variance of train data\n",
    "normalizer_test = Normalization()\n",
    "normalizer_test.adapt(np.array(X_test[['CreditScore','Age','Balance','EstimatedSalary']]))\n",
    "print(normalizer_test)\n",
    "print(normalizer_test.mean.numpy())\n",
    "print(normalizer_test.variance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5543061 , -0.00746374,  0.5545327 ,  1.7050518 ],\n",
       "       [-0.09213665, -0.3869757 , -1.2316512 , -0.14648026],\n",
       "       [-0.9745823 ,  0.6566822 ,  0.696562  ,  1.3260764 ],\n",
       "       ...,\n",
       "       [ 0.82109195, -0.3869757 ,  1.2583638 , -0.83305013],\n",
       "       [-1.5799811 , -0.8613657 ,  0.22521938, -1.2843066 ],\n",
       "       [-0.58466446, -0.10234173,  0.18955089, -0.6014389 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_to_normalize = np.array(X_test[['CreditScore','Age','Balance','EstimatedSalary']])\n",
    "\n",
    "normalizer_test(test_data_to_normalize).numpy() # apply the normalization to the selected same features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate out the normalized credit score from all normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5543061 , -0.09213665, -0.9745823 , ...,  0.82109195,\n",
       "       -1.5799811 , -0.58466446], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditScore_norm1 = normalizer_test(test_data_to_normalize).numpy()[:,0]\n",
    "creditScore_norm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalized credit score to data frame X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>704.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>111525.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199484.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>641.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93148.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.092137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>555.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>120392.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177719.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.974582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>474.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>176311.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160213.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.805723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>610.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62232.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>460.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>127559.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126952.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.949377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>776.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63908.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.293098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>730.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>155470.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53718.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.821092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5657</th>\n",
       "      <td>496.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90963.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27802.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.579981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>593.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>88736.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67020.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.584664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5876        704.0  39.0  111525.02        1.0             0.0   \n",
       "6555        641.0  35.0       0.00        1.0             0.0   \n",
       "1448        555.0  46.0  120392.99        1.0             0.0   \n",
       "3351        474.0  34.0  176311.36        1.0             0.0   \n",
       "231         610.0  40.0       0.00        1.0             0.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "749         460.0  46.0  127559.97        1.0             1.0   \n",
       "5373        776.0  30.0       0.00        0.0             1.0   \n",
       "485         730.0  35.0  155470.55        1.0             1.0   \n",
       "5657        496.0  30.0   90963.49        0.0             1.0   \n",
       "676         593.0  38.0   88736.44        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5876        199484.96              1.0              0.0              0.0   \n",
       "6555         93148.93              0.0              1.0              0.0   \n",
       "1448        177719.88              1.0              0.0              0.0   \n",
       "3351        160213.27              1.0              0.0              0.0   \n",
       "231          62232.60              0.0              1.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "749         126952.50              0.0              1.0              0.0   \n",
       "5373         63908.86              0.0              1.0              0.0   \n",
       "485          53718.28              1.0              0.0              0.0   \n",
       "5657         27802.00              1.0              0.0              0.0   \n",
       "676          67020.03              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5876              0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "6555              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1448              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "3351              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "231               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "749               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "5373              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "485               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "5657              0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "676               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "6555       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1448       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3351       0.0       0.0       0.0       0.0       1.0        0.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "749        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5373       0.0       1.0       0.0       0.0       0.0        0.0   \n",
       "485        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5657       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "676        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5876               1.0                0.0              0.0            0.0   \n",
       "6555               1.0                0.0              0.0            0.0   \n",
       "1448               0.0                1.0              0.0            1.0   \n",
       "3351               0.0                1.0              0.0            1.0   \n",
       "231                1.0                0.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "749                0.0                1.0              0.0            0.0   \n",
       "5373               0.0                0.0              1.0            0.0   \n",
       "485                1.0                0.0              0.0            1.0   \n",
       "5657               1.0                0.0              0.0            0.0   \n",
       "676                0.0                0.0              1.0            1.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  \n",
       "5876          1.0          0.554306  \n",
       "6555          1.0         -0.092137  \n",
       "1448          0.0         -0.974582  \n",
       "3351          0.0         -1.805723  \n",
       "231           1.0         -0.410228  \n",
       "...           ...               ...  \n",
       "749           1.0         -1.949377  \n",
       "5373          1.0          1.293098  \n",
       "485           0.0          0.821092  \n",
       "5657          1.0         -1.579981  \n",
       "676           0.0         -0.584664  \n",
       "\n",
       "[3000 rows x 27 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['CreditScore_norm'] = creditScore_norm1.tolist()\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate out the normalized credit score from all normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00746374, -0.3869757 ,  0.6566822 , ..., -0.3869757 ,\n",
       "       -0.8613657 , -0.10234173], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Age_norm1 = normalizer_test(test_data_to_normalize).numpy()[:,1]\n",
    "Age_norm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalized credit score to data frame X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>704.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>111525.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199484.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>-0.007464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>641.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93148.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.092137</td>\n",
       "      <td>-0.386976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>555.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>120392.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177719.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.974582</td>\n",
       "      <td>0.656682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>474.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>176311.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160213.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.805723</td>\n",
       "      <td>-0.481854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>610.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62232.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410228</td>\n",
       "      <td>0.087414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>460.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>127559.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126952.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.949377</td>\n",
       "      <td>0.656682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>776.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63908.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.293098</td>\n",
       "      <td>-0.861366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>730.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>155470.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53718.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.821092</td>\n",
       "      <td>-0.386976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5657</th>\n",
       "      <td>496.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90963.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27802.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.579981</td>\n",
       "      <td>-0.861366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>593.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>88736.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67020.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.584664</td>\n",
       "      <td>-0.102342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5876        704.0  39.0  111525.02        1.0             0.0   \n",
       "6555        641.0  35.0       0.00        1.0             0.0   \n",
       "1448        555.0  46.0  120392.99        1.0             0.0   \n",
       "3351        474.0  34.0  176311.36        1.0             0.0   \n",
       "231         610.0  40.0       0.00        1.0             0.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "749         460.0  46.0  127559.97        1.0             1.0   \n",
       "5373        776.0  30.0       0.00        0.0             1.0   \n",
       "485         730.0  35.0  155470.55        1.0             1.0   \n",
       "5657        496.0  30.0   90963.49        0.0             1.0   \n",
       "676         593.0  38.0   88736.44        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5876        199484.96              1.0              0.0              0.0   \n",
       "6555         93148.93              0.0              1.0              0.0   \n",
       "1448        177719.88              1.0              0.0              0.0   \n",
       "3351        160213.27              1.0              0.0              0.0   \n",
       "231          62232.60              0.0              1.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "749         126952.50              0.0              1.0              0.0   \n",
       "5373         63908.86              0.0              1.0              0.0   \n",
       "485          53718.28              1.0              0.0              0.0   \n",
       "5657         27802.00              1.0              0.0              0.0   \n",
       "676          67020.03              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5876              0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "6555              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1448              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "3351              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "231               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "749               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "5373              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "485               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "5657              0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "676               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "6555       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1448       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3351       0.0       0.0       0.0       0.0       1.0        0.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "749        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5373       0.0       1.0       0.0       0.0       0.0        0.0   \n",
       "485        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5657       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "676        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5876               1.0                0.0              0.0            0.0   \n",
       "6555               1.0                0.0              0.0            0.0   \n",
       "1448               0.0                1.0              0.0            1.0   \n",
       "3351               0.0                1.0              0.0            1.0   \n",
       "231                1.0                0.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "749                0.0                1.0              0.0            0.0   \n",
       "5373               0.0                0.0              1.0            0.0   \n",
       "485                1.0                0.0              0.0            1.0   \n",
       "5657               1.0                0.0              0.0            0.0   \n",
       "676                0.0                0.0              1.0            1.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  Age_norm  \n",
       "5876          1.0          0.554306 -0.007464  \n",
       "6555          1.0         -0.092137 -0.386976  \n",
       "1448          0.0         -0.974582  0.656682  \n",
       "3351          0.0         -1.805723 -0.481854  \n",
       "231           1.0         -0.410228  0.087414  \n",
       "...           ...               ...       ...  \n",
       "749           1.0         -1.949377  0.656682  \n",
       "5373          1.0          1.293098 -0.861366  \n",
       "485           0.0          0.821092 -0.386976  \n",
       "5657          1.0         -1.579981 -0.861366  \n",
       "676           0.0         -0.584664 -0.102342  \n",
       "\n",
       "[3000 rows x 28 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['Age_norm'] = Age_norm1.tolist()\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate out the normalized credit score from all normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5545327 , -1.2316512 ,  0.696562  , ...,  1.2583638 ,\n",
       "        0.22521938,  0.18955089], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Balance_norm1 = normalizer_test(test_data_to_normalize).numpy()[:,2]\n",
    "Balance_norm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalized credit score to data frame X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>704.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>111525.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199484.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>0.554533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>641.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93148.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.092137</td>\n",
       "      <td>-0.386976</td>\n",
       "      <td>-1.231651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>555.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>120392.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177719.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.974582</td>\n",
       "      <td>0.656682</td>\n",
       "      <td>0.696562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>474.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>176311.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160213.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.805723</td>\n",
       "      <td>-0.481854</td>\n",
       "      <td>1.592150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>610.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62232.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410228</td>\n",
       "      <td>0.087414</td>\n",
       "      <td>-1.231651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>460.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>127559.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126952.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.949377</td>\n",
       "      <td>0.656682</td>\n",
       "      <td>0.811348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>776.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63908.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.293098</td>\n",
       "      <td>-0.861366</td>\n",
       "      <td>-1.231651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>730.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>155470.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53718.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.821092</td>\n",
       "      <td>-0.386976</td>\n",
       "      <td>1.258364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5657</th>\n",
       "      <td>496.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90963.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27802.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.579981</td>\n",
       "      <td>-0.861366</td>\n",
       "      <td>0.225219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>593.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>88736.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67020.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.584664</td>\n",
       "      <td>-0.102342</td>\n",
       "      <td>0.189551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5876        704.0  39.0  111525.02        1.0             0.0   \n",
       "6555        641.0  35.0       0.00        1.0             0.0   \n",
       "1448        555.0  46.0  120392.99        1.0             0.0   \n",
       "3351        474.0  34.0  176311.36        1.0             0.0   \n",
       "231         610.0  40.0       0.00        1.0             0.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "749         460.0  46.0  127559.97        1.0             1.0   \n",
       "5373        776.0  30.0       0.00        0.0             1.0   \n",
       "485         730.0  35.0  155470.55        1.0             1.0   \n",
       "5657        496.0  30.0   90963.49        0.0             1.0   \n",
       "676         593.0  38.0   88736.44        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5876        199484.96              1.0              0.0              0.0   \n",
       "6555         93148.93              0.0              1.0              0.0   \n",
       "1448        177719.88              1.0              0.0              0.0   \n",
       "3351        160213.27              1.0              0.0              0.0   \n",
       "231          62232.60              0.0              1.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "749         126952.50              0.0              1.0              0.0   \n",
       "5373         63908.86              0.0              1.0              0.0   \n",
       "485          53718.28              1.0              0.0              0.0   \n",
       "5657         27802.00              1.0              0.0              0.0   \n",
       "676          67020.03              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5876              0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "6555              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1448              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "3351              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "231               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "749               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "5373              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "485               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "5657              0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "676               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "6555       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1448       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3351       0.0       0.0       0.0       0.0       1.0        0.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "749        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5373       0.0       1.0       0.0       0.0       0.0        0.0   \n",
       "485        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5657       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "676        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5876               1.0                0.0              0.0            0.0   \n",
       "6555               1.0                0.0              0.0            0.0   \n",
       "1448               0.0                1.0              0.0            1.0   \n",
       "3351               0.0                1.0              0.0            1.0   \n",
       "231                1.0                0.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "749                0.0                1.0              0.0            0.0   \n",
       "5373               0.0                0.0              1.0            0.0   \n",
       "485                1.0                0.0              0.0            1.0   \n",
       "5657               1.0                0.0              0.0            0.0   \n",
       "676                0.0                0.0              1.0            1.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  Age_norm  Balance_norm  \n",
       "5876          1.0          0.554306 -0.007464      0.554533  \n",
       "6555          1.0         -0.092137 -0.386976     -1.231651  \n",
       "1448          0.0         -0.974582  0.656682      0.696562  \n",
       "3351          0.0         -1.805723 -0.481854      1.592150  \n",
       "231           1.0         -0.410228  0.087414     -1.231651  \n",
       "...           ...               ...       ...           ...  \n",
       "749           1.0         -1.949377  0.656682      0.811348  \n",
       "5373          1.0          1.293098 -0.861366     -1.231651  \n",
       "485           0.0          0.821092 -0.386976      1.258364  \n",
       "5657          1.0         -1.579981 -0.861366      0.225219  \n",
       "676           0.0         -0.584664 -0.102342      0.189551  \n",
       "\n",
       "[3000 rows x 29 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['Balance_norm'] = Balance_norm1.tolist()\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate out the normalized credit score from all normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7050518 , -0.14648026,  1.3260764 , ..., -0.83305013,\n",
       "       -1.2843066 , -0.6014389 ], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EstimatedSalary_norm1 = normalizer_test(test_data_to_normalize).numpy()[:,3]\n",
    "EstimatedSalary_norm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalized credit score to data frame X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgunn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>704.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>111525.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199484.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>0.554533</td>\n",
       "      <td>1.705052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>641.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93148.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.092137</td>\n",
       "      <td>-0.386976</td>\n",
       "      <td>-1.231651</td>\n",
       "      <td>-0.146480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>555.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>120392.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177719.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.974582</td>\n",
       "      <td>0.656682</td>\n",
       "      <td>0.696562</td>\n",
       "      <td>1.326076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>474.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>176311.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160213.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.805723</td>\n",
       "      <td>-0.481854</td>\n",
       "      <td>1.592150</td>\n",
       "      <td>1.021250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>610.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62232.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410228</td>\n",
       "      <td>0.087414</td>\n",
       "      <td>-1.231651</td>\n",
       "      <td>-0.684798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>460.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>127559.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126952.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.949377</td>\n",
       "      <td>0.656682</td>\n",
       "      <td>0.811348</td>\n",
       "      <td>0.442110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>776.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63908.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.293098</td>\n",
       "      <td>-0.861366</td>\n",
       "      <td>-1.231651</td>\n",
       "      <td>-0.655611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>730.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>155470.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53718.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.821092</td>\n",
       "      <td>-0.386976</td>\n",
       "      <td>1.258364</td>\n",
       "      <td>-0.833050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5657</th>\n",
       "      <td>496.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90963.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27802.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.579981</td>\n",
       "      <td>-0.861366</td>\n",
       "      <td>0.225219</td>\n",
       "      <td>-1.284307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>593.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>88736.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67020.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.584664</td>\n",
       "      <td>-0.102342</td>\n",
       "      <td>0.189551</td>\n",
       "      <td>-0.601439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "5876        704.0  39.0  111525.02        1.0             0.0   \n",
       "6555        641.0  35.0       0.00        1.0             0.0   \n",
       "1448        555.0  46.0  120392.99        1.0             0.0   \n",
       "3351        474.0  34.0  176311.36        1.0             0.0   \n",
       "231         610.0  40.0       0.00        1.0             0.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "749         460.0  46.0  127559.97        1.0             1.0   \n",
       "5373        776.0  30.0       0.00        0.0             1.0   \n",
       "485         730.0  35.0  155470.55        1.0             1.0   \n",
       "5657        496.0  30.0   90963.49        0.0             1.0   \n",
       "676         593.0  38.0   88736.44        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "5876        199484.96              1.0              0.0              0.0   \n",
       "6555         93148.93              0.0              1.0              0.0   \n",
       "1448        177719.88              1.0              0.0              0.0   \n",
       "3351        160213.27              1.0              0.0              0.0   \n",
       "231          62232.60              0.0              1.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "749         126952.50              0.0              1.0              0.0   \n",
       "5373         63908.86              0.0              1.0              0.0   \n",
       "485          53718.28              1.0              0.0              0.0   \n",
       "5657         27802.00              1.0              0.0              0.0   \n",
       "676          67020.03              0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "5876              0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "6555              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1448              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "3351              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "231               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "749               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "5373              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "485               0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "5657              0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "676               0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "6555       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1448       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3351       0.0       0.0       0.0       0.0       1.0        0.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "749        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5373       0.0       1.0       0.0       0.0       0.0        0.0   \n",
       "485        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "5657       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "676        0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "5876               1.0                0.0              0.0            0.0   \n",
       "6555               1.0                0.0              0.0            0.0   \n",
       "1448               0.0                1.0              0.0            1.0   \n",
       "3351               0.0                1.0              0.0            1.0   \n",
       "231                1.0                0.0              0.0            0.0   \n",
       "...                ...                ...              ...            ...   \n",
       "749                0.0                1.0              0.0            0.0   \n",
       "5373               0.0                0.0              1.0            0.0   \n",
       "485                1.0                0.0              0.0            1.0   \n",
       "5657               1.0                0.0              0.0            0.0   \n",
       "676                0.0                0.0              1.0            1.0   \n",
       "\n",
       "      Gender_Male  CreditScore_norm  Age_norm  Balance_norm  \\\n",
       "5876          1.0          0.554306 -0.007464      0.554533   \n",
       "6555          1.0         -0.092137 -0.386976     -1.231651   \n",
       "1448          0.0         -0.974582  0.656682      0.696562   \n",
       "3351          0.0         -1.805723 -0.481854      1.592150   \n",
       "231           1.0         -0.410228  0.087414     -1.231651   \n",
       "...           ...               ...       ...           ...   \n",
       "749           1.0         -1.949377  0.656682      0.811348   \n",
       "5373          1.0          1.293098 -0.861366     -1.231651   \n",
       "485           0.0          0.821092 -0.386976      1.258364   \n",
       "5657          1.0         -1.579981 -0.861366      0.225219   \n",
       "676           0.0         -0.584664 -0.102342      0.189551   \n",
       "\n",
       "      EstimatedSalary_norm  \n",
       "5876              1.705052  \n",
       "6555             -0.146480  \n",
       "1448              1.326076  \n",
       "3351              1.021250  \n",
       "231              -0.684798  \n",
       "...                    ...  \n",
       "749               0.442110  \n",
       "5373             -0.655611  \n",
       "485              -0.833050  \n",
       "5657             -1.284307  \n",
       "676              -0.601439  \n",
       "\n",
       "[3000 rows x 30 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['EstimatedSalary_norm'] = EstimatedSalary_norm1.tolist()\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the non normalized feature values since we have normalized features in diff columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['CreditScore','Age','Balance','EstimatedSalary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>0.554533</td>\n",
       "      <td>1.705052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.092137</td>\n",
       "      <td>-0.386976</td>\n",
       "      <td>-1.231651</td>\n",
       "      <td>-0.146480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.974582</td>\n",
       "      <td>0.656682</td>\n",
       "      <td>0.696562</td>\n",
       "      <td>1.326076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.805723</td>\n",
       "      <td>-0.481854</td>\n",
       "      <td>1.592150</td>\n",
       "      <td>1.021250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410228</td>\n",
       "      <td>0.087414</td>\n",
       "      <td>-1.231651</td>\n",
       "      <td>-0.684798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.949377</td>\n",
       "      <td>0.656682</td>\n",
       "      <td>0.811348</td>\n",
       "      <td>0.442110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.293098</td>\n",
       "      <td>-0.861366</td>\n",
       "      <td>-1.231651</td>\n",
       "      <td>-0.655611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.821092</td>\n",
       "      <td>-0.386976</td>\n",
       "      <td>1.258364</td>\n",
       "      <td>-0.833050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.579981</td>\n",
       "      <td>-0.861366</td>\n",
       "      <td>0.225219</td>\n",
       "      <td>-1.284307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.584664</td>\n",
       "      <td>-0.102342</td>\n",
       "      <td>0.189551</td>\n",
       "      <td>-0.601439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "5876        1.0             0.0              1.0              0.0   \n",
       "6555        1.0             0.0              0.0              1.0   \n",
       "1448        1.0             0.0              1.0              0.0   \n",
       "3351        1.0             0.0              1.0              0.0   \n",
       "231         1.0             0.0              0.0              1.0   \n",
       "...         ...             ...              ...              ...   \n",
       "749         1.0             1.0              0.0              1.0   \n",
       "5373        0.0             1.0              0.0              1.0   \n",
       "485         1.0             1.0              1.0              0.0   \n",
       "5657        0.0             1.0              1.0              0.0   \n",
       "676         1.0             0.0              0.0              1.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "5876              0.0              0.0       0.0       0.0       1.0   \n",
       "6555              0.0              0.0       0.0       0.0       0.0   \n",
       "1448              0.0              0.0       0.0       0.0       0.0   \n",
       "3351              0.0              0.0       0.0       0.0       0.0   \n",
       "231               0.0              0.0       1.0       0.0       0.0   \n",
       "...               ...              ...       ...       ...       ...   \n",
       "749               0.0              0.0       0.0       0.0       0.0   \n",
       "5373              0.0              0.0       0.0       0.0       0.0   \n",
       "485               0.0              0.0       1.0       0.0       0.0   \n",
       "5657              0.0              0.0       1.0       0.0       0.0   \n",
       "676               0.0              0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "6555       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "1448       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3351       0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "749        0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5373       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "485        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5657       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "676        0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "5876        0.0               1.0                0.0              0.0   \n",
       "6555        0.0               1.0                0.0              0.0   \n",
       "1448        0.0               0.0                1.0              0.0   \n",
       "3351        0.0               0.0                1.0              0.0   \n",
       "231         0.0               1.0                0.0              0.0   \n",
       "...         ...               ...                ...              ...   \n",
       "749         0.0               0.0                1.0              0.0   \n",
       "5373        0.0               0.0                0.0              1.0   \n",
       "485         0.0               1.0                0.0              0.0   \n",
       "5657        0.0               1.0                0.0              0.0   \n",
       "676         0.0               0.0                0.0              1.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  CreditScore_norm  Age_norm  Balance_norm  \\\n",
       "5876            0.0          1.0          0.554306 -0.007464      0.554533   \n",
       "6555            0.0          1.0         -0.092137 -0.386976     -1.231651   \n",
       "1448            1.0          0.0         -0.974582  0.656682      0.696562   \n",
       "3351            1.0          0.0         -1.805723 -0.481854      1.592150   \n",
       "231             0.0          1.0         -0.410228  0.087414     -1.231651   \n",
       "...             ...          ...               ...       ...           ...   \n",
       "749             0.0          1.0         -1.949377  0.656682      0.811348   \n",
       "5373            0.0          1.0          1.293098 -0.861366     -1.231651   \n",
       "485             1.0          0.0          0.821092 -0.386976      1.258364   \n",
       "5657            0.0          1.0         -1.579981 -0.861366      0.225219   \n",
       "676             1.0          0.0         -0.584664 -0.102342      0.189551   \n",
       "\n",
       "      EstimatedSalary_norm  \n",
       "5876              1.705052  \n",
       "6555             -0.146480  \n",
       "1448              1.326076  \n",
       "3351              1.021250  \n",
       "231              -0.684798  \n",
       "...                    ...  \n",
       "749               0.442110  \n",
       "5373             -0.655611  \n",
       "485              -0.833050  \n",
       "5657             -1.284307  \n",
       "676              -0.601439  \n",
       "\n",
       "[3000 rows x 26 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Initializing & Building multiple models and implementaing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Case 1- Compiling model . Will use the gradient descent optimization , with 'binary_crossentropy' loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model.add(Dense(32, activation = 'relu'))\n",
    "bankdata_model.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer = optimizers.Adam(lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model.compile(optimizer = gd_optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 0s 60us/sample - loss: 0.6388 - accuracy: 0.6291\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4688 - accuracy: 0.7979\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4245 - accuracy: 0.8046\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4010 - accuracy: 0.8154\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3868 - accuracy: 0.8250\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3766 - accuracy: 0.8339\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3702 - accuracy: 0.8383\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3653 - accuracy: 0.8414\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3617 - accuracy: 0.8436\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3583 - accuracy: 0.8469\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3557 - accuracy: 0.8481\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3532 - accuracy: 0.8507\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3511 - accuracy: 0.8516\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.3491 - accuracy: 0.8539\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3475 - accuracy: 0.8544\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3456 - accuracy: 0.8550\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3441 - accuracy: 0.8570\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3427 - accuracy: 0.8576\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3417 - accuracy: 0.8580\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3400 - accuracy: 0.8584\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3395 - accuracy: 0.8609\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3379 - accuracy: 0.8613\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3370 - accuracy: 0.8596\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3355 - accuracy: 0.8601\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3346 - accuracy: 0.8614\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3340 - accuracy: 0.8616\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3329 - accuracy: 0.8614\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3323 - accuracy: 0.8624\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3310 - accuracy: 0.8640\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3306 - accuracy: 0.8613\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3300 - accuracy: 0.8641\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3290 - accuracy: 0.8643\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3285 - accuracy: 0.8643\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3283 - accuracy: 0.8646\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3272 - accuracy: 0.8647\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3263 - accuracy: 0.8656\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3260 - accuracy: 0.8650\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3258 - accuracy: 0.8660\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3251 - accuracy: 0.8657\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3249 - accuracy: 0.8657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bca484a88>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loss is : [0.6387757446084704, 0.4688453653029033, 0.42453912113394054, 0.4010474703141621, 0.3867990289415632, 0.37662059707301004, 0.37017357988016947, 0.36534894577094484, 0.3616945892572403, 0.35834499980722156, 0.3556670035634722, 0.35319024537290844, 0.35109105493341175, 0.34911076511655537, 0.3475488956485476, 0.3456477288688932, 0.3441279019628252, 0.3426783574478967, 0.3416727419410433, 0.33999654012066977, 0.33951832354068756, 0.3378882386854717, 0.3369566329887935, 0.33553375942366465, 0.33455100655555725, 0.3339563650744302, 0.33287981152534485, 0.33227800684315817, 0.3310479521751404, 0.3306135811976024, 0.330000211085592, 0.3290136882237026, 0.32846524459975107, 0.32833301595279146, 0.32717833135809216, 0.32628371885844637, 0.3260332303387778, 0.3257502040692738, 0.3251246575798307, 0.3248718295778547]\n"
     ]
    }
   ],
   "source": [
    "print('Train data loss is :', bankdata_model.history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loss is : 0.3248718295778547\n"
     ]
    }
   ],
   "source": [
    "print('Train data loss is :', bankdata_model.history.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data accuracy is : 0.8657143\n"
     ]
    }
   ],
   "source": [
    "print('Train data accuracy is :', bankdata_model.history.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.3565 - accuracy: 0.8590\n"
     ]
    }
   ],
   "source": [
    "eva_results = bankdata_model.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.3565266535282135, 0.859]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model.metrics_names)\n",
    "print(eva_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data loss is : 0.3565266535282135\n"
     ]
    }
   ],
   "source": [
    "print('Test data loss is :', eva_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data accuracy is : 0.859\n"
     ]
    }
   ],
   "source": [
    "print('Test data accuracy is :', eva_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 16us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 5us/sample\n",
      "[[0.15578951]\n",
      " [0.01063805]\n",
      " [0.7440114 ]\n",
      " ...\n",
      " [0.13904649]\n",
      " [0.04842578]\n",
      " [0.21528824]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value = bankdata_model.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class = (Y_pred_value > 0.5).astype(int)\n",
    "print(Y_pred_value_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.3565 - accuracy: 0.8590\n",
      "Accuracy of Model with Adam optimizer Case1:0.859\n",
      "Recall_score: 0.47495961227786754\n",
      "Precision_score: 0.75\n",
      "F-score: 0.5816023738872402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2283,   98],\n",
       "       [ 325,  294]], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer Case1:'+ str(bankdata_model.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class)))\n",
    "confusion_matrix(y_test.values, Y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.3565 - accuracy: 0.8590\n",
      "Accuracy of Model with Adam optimizer Case 1 :0.859\n",
      "Recall_score: 0.47495961227786754\n",
      "Precision_score: 0.75\n",
      "F-score: 0.5816023738872402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2283,   98],\n",
       "       [ 325,  294]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer Case 1 :'+ str(bankdata_model.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam = confusion_matrix(y_test.values, Y_pred_value_class)\n",
    "df_cm_adam = pd.DataFrame(cm_adam, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a results df to hold results of diff runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([[1,'Adam',3,64,'elu',32,'relu',1,'sigmoid', 0, 'No layer',0,'No layer',0,0, 0,0,0,0,0]], \n",
    "                          columns = ['Case X','Model details','Num of layers','Layer 1 nodes','Layer1 Act F','Layer 2 nodes','Layer2 Act F',\n",
    "                            'Layer 3 nodes','Layer3 Act F', 'Layer 4 nodes', 'Layer4 Act F','Layer 5 nodes', 'Layer5 Act F',\n",
    "                                     'Train data-loss','Train data-Accuracy', 'Test data-Loss',\n",
    "                            'Test data-Accuracy','Recall score','Precision score','F score'])\n",
    "\n",
    "row_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 1,'Adam',3,64,'elu',32,'relu',1,'sigmoid', 0, 'No layer', 0,'No layer',\n",
    "                   bankdata_model.history.history['loss'][-1],\n",
    "                            bankdata_model.history.history['accuracy'][-1], eva_results[0],eva_results[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class), precision_score(y_test.values, Y_pred_value_class), \n",
    "                            f1_score(y_test.values,Y_pred_value_class)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "results_df = results_df.drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.47496</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527               0.859       0.47496   \n",
       "\n",
       "   Precision score   F score  \n",
       "1             0.75  0.581602  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2  Case 2 using Nadam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model_nadam.add(Dense(32, activation = 'relu'))\n",
    "bankdata_model_nadam.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam = optimizers.Nadam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam.compile(optimizer = gd_optimizer_nadam, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 1s 84us/sample - loss: 0.6258 - accuracy: 0.6781\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.4805 - accuracy: 0.8047\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4250 - accuracy: 0.8087\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4006 - accuracy: 0.8173\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3867 - accuracy: 0.8276\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3773 - accuracy: 0.8329\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3705 - accuracy: 0.8376\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3659 - accuracy: 0.8419\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.3618 - accuracy: 0.8453\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3588 - accuracy: 0.8454\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3563 - accuracy: 0.8473\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3537 - accuracy: 0.8494\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3517 - accuracy: 0.8500\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3497 - accuracy: 0.8514\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3483 - accuracy: 0.8539\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3465 - accuracy: 0.8546\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3453 - accuracy: 0.8567\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3438 - accuracy: 0.8559\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3427 - accuracy: 0.8577\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3412 - accuracy: 0.8580\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3408 - accuracy: 0.8597\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3393 - accuracy: 0.8591\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3384 - accuracy: 0.8599\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3375 - accuracy: 0.8600\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3363 - accuracy: 0.8600\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3357 - accuracy: 0.8611\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3346 - accuracy: 0.8617\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3339 - accuracy: 0.8633\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3330 - accuracy: 0.8624\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3324 - accuracy: 0.8641\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3316 - accuracy: 0.8617\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3306 - accuracy: 0.8640\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3303 - accuracy: 0.8639\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3299 - accuracy: 0.8629\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3290 - accuracy: 0.8639\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3281 - accuracy: 0.8640\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3276 - accuracy: 0.8649\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3263 - accuracy: 0.8643\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3262 - accuracy: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bcbc6bec8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 72us/sample - loss: 0.3516 - accuracy: 0.8633\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam = bankdata_model_nadam.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.35158412345250445, 0.86333334]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam.metrics_names)\n",
    "print(eva_results_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 18us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam = bankdata_model_nadam.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 6us/sample\n",
      "[[0.15257612]\n",
      " [0.00975756]\n",
      " [0.7073016 ]\n",
      " ...\n",
      " [0.1205968 ]\n",
      " [0.04891653]\n",
      " [0.15532085]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam = bankdata_model_nadam.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam = (Y_pred_value_nadam > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.3516 - accuracy: 0.8633\n",
      "Accuracy of Model with Nadam optimizer :0.86333334\n",
      "Recall_score: 0.44426494345718903\n",
      "Precision_score: 0.8064516129032258\n",
      "F-score: 0.5729166666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2315,   66],\n",
       "       [ 344,  275]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.3516 - accuracy: 0.8633\n",
      "Accuracy of Model with Nadam optimizer :0.86333334\n",
      "Recall_score: 0.44426494345718903\n",
      "Precision_score: 0.8064516129032258\n",
      "F-score: 0.5729166666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2315,   66],\n",
       "       [ 344,  275]], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam = confusion_matrix(y_test.values, Y_pred_value_class_nadam)\n",
    "df_cm_nadam = pd.DataFrame(cm_nadam, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 2,'Nadam',3,64,'elu',32,'relu',1,'sigmoid', 0, 'No layer', 0,'No layer',\n",
    "                   bankdata_model_nadam.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam.history.history['accuracy'][-1], eva_results_nadam[0],eva_results_nadam[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam), f1_score(y_test.values,Y_pred_value_class_nadam)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3  Case 3 using Adam optimizer  with LeakyRelu activation function , since it gives a small graddient when z<0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam2.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model_adam2.add(Dense(32, LeakyReLU(alpha=0.1)))\n",
    "bankdata_model_adam2.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_adam2 = optimizers.Adam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam2.compile(optimizer = gd_optimizer_adam2, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_adam2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 0s 50us/sample - loss: 0.6259 - accuracy: 0.6643\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.4911 - accuracy: 0.7953\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.4396 - accuracy: 0.7987\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4148 - accuracy: 0.8086\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3982 - accuracy: 0.8181\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3858 - accuracy: 0.8254\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3770 - accuracy: 0.8317\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3705 - accuracy: 0.8367\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3661 - accuracy: 0.8399\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3623 - accuracy: 0.8447\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3593 - accuracy: 0.8461\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3570 - accuracy: 0.8481\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3547 - accuracy: 0.8496\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3526 - accuracy: 0.8519\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3504 - accuracy: 0.8513\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3490 - accuracy: 0.8531\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3471 - accuracy: 0.8553\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3456 - accuracy: 0.8571\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3444 - accuracy: 0.8583\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3431 - accuracy: 0.8587\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3420 - accuracy: 0.8581\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3412 - accuracy: 0.8586\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3398 - accuracy: 0.8599\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3386 - accuracy: 0.8604\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3378 - accuracy: 0.8617\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3370 - accuracy: 0.8626\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3356 - accuracy: 0.8621\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3361 - accuracy: 0.8609\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3342 - accuracy: 0.8619\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3331 - accuracy: 0.8619\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3318 - accuracy: 0.8624\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3315 - accuracy: 0.8609\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3304 - accuracy: 0.8636\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.83 - 0s 5us/sample - loss: 0.3295 - accuracy: 0.8636\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3285 - accuracy: 0.8649\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3281 - accuracy: 0.8657\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3267 - accuracy: 0.8639\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3262 - accuracy: 0.8654\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3257 - accuracy: 0.8657\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3252 - accuracy: 0.8647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bcd10bb88>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_adam2.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 75us/sample - loss: 0.3500 - accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "eva_results_adam2 = bankdata_model_adam2.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.349987863222758, 0.862]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_adam2.metrics_names)\n",
    "print(eva_results_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 16us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_adam2 = bankdata_model_adam2.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 6us/sample\n",
      "[[0.2211838 ]\n",
      " [0.0096463 ]\n",
      " [0.7501248 ]\n",
      " ...\n",
      " [0.14569418]\n",
      " [0.05843699]\n",
      " [0.14777245]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_adam2 = bankdata_model_adam2.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_adam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2631, 1: 369}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_adam2 = (Y_pred_value_adam2 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_adam2)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_adam2, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.3500 - accuracy: 0.8620\n",
      "Accuracy of Model with Adam optimizer :0.862\n",
      "Recall_score: 0.46365105008077545\n",
      "Precision_score: 0.7777777777777778\n",
      "F-score: 0.5809716599190283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2299,   82],\n",
       "       [ 332,  287]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_adam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_adam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_adam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.3500 - accuracy: 0.8620\n",
      "Accuracy of Model with Adam optimizer :0.862\n",
      "Recall_score: 0.46365105008077545\n",
      "Precision_score: 0.7777777777777778\n",
      "F-score: 0.5809716599190283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2299,   82],\n",
       "       [ 332,  287]], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_adam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_adam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_adam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_adam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam2 = confusion_matrix(y_test.values, Y_pred_value_class_adam2)\n",
    "df_cm_adam2 = pd.DataFrame(cm_adam2, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam2, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 3,'Adam',3,64,'elu',32,'LeakyReLU',1,'sigmoid', 0, 'No layer',0,'No layer',\n",
    "                   bankdata_model_adam2.history.history['loss'][-1],\n",
    "                            bankdata_model_adam2.history.history['accuracy'][-1], eva_results_adam2[0],eva_results_adam2[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_adam2), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_adam2), f1_score(y_test.values,Y_pred_value_class_adam2)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Case 4 using Adam optimizer  with relu in first layer and also using LeakyRelu activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam3 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam3.add(Dense(64, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_adam3.add(Dense(32, LeakyReLU(alpha=0.1)))\n",
    "bankdata_model_adam3.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_adam3 = optimizers.Adam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam3.compile(optimizer = gd_optimizer_adam3, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_adam3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 0s 45us/sample - loss: 0.6022 - accuracy: 0.7327\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4978 - accuracy: 0.7974\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.4639 - accuracy: 0.7974\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4384 - accuracy: 0.7981\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.4186 - accuracy: 0.8043\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4025 - accuracy: 0.8134\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3895 - accuracy: 0.8216\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3785 - accuracy: 0.8341\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3700 - accuracy: 0.8390\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3634 - accuracy: 0.8416\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3589 - accuracy: 0.8470\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3534 - accuracy: 0.8499\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3498 - accuracy: 0.8521\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3464 - accuracy: 0.8543\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3447 - accuracy: 0.8567\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3415 - accuracy: 0.8590\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3398 - accuracy: 0.8586\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3385 - accuracy: 0.8599\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3360 - accuracy: 0.8624\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3338 - accuracy: 0.8607\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3319 - accuracy: 0.8636\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3306 - accuracy: 0.8624\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3286 - accuracy: 0.8631\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3270 - accuracy: 0.8640\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3256 - accuracy: 0.8646\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3246 - accuracy: 0.8654\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3227 - accuracy: 0.8660\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3220 - accuracy: 0.8660\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3206 - accuracy: 0.8683\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3194 - accuracy: 0.8670\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3176 - accuracy: 0.8674\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3168 - accuracy: 0.8687\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3154 - accuracy: 0.8703\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3141 - accuracy: 0.8683\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3131 - accuracy: 0.8709\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3122 - accuracy: 0.8720\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3109 - accuracy: 0.8701\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3098 - accuracy: 0.8717\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3099 - accuracy: 0.8709\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3085 - accuracy: 0.8710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bce4d63c8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_adam3.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 73us/sample - loss: 0.3608 - accuracy: 0.8567\n"
     ]
    }
   ],
   "source": [
    "eva_results_adam3 = bankdata_model_adam3.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.36082669949531554, 0.8566667]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_adam3.metrics_names)\n",
    "print(eva_results_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 18us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_adam3 = bankdata_model_adam3.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 11us/sample\n",
      "[[0.21968356]\n",
      " [0.00943146]\n",
      " [0.7020907 ]\n",
      " ...\n",
      " [0.19268517]\n",
      " [0.08496214]\n",
      " [0.22616063]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_adam3 = bankdata_model_adam3.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_adam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2611, 1: 389}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_adam3 = (Y_pred_value_adam3 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_adam3)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_adam3, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.3608 - accuracy: 0.8567\n",
      "Accuracy of Model with Adam optimizer :0.8566667\n",
      "Recall_score: 0.4668820678513732\n",
      "Precision_score: 0.7429305912596401\n",
      "F-score: 0.5734126984126985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2281,  100],\n",
       "       [ 330,  289]], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_adam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_adam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_adam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.3608 - accuracy: 0.8567\n",
      "Accuracy of Model  with Adam optimizer :0.8566667\n",
      "Recall_score: 0.4668820678513732\n",
      "Precision_score: 0.7429305912596401\n",
      "F-score: 0.5734126984126985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2281,  100],\n",
       "       [ 330,  289]], dtype=int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model  with Adam optimizer :'+ str(bankdata_model_adam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_adam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_adam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_adam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_adam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam3 = confusion_matrix(y_test.values, Y_pred_value_class_adam3)\n",
    "df_cm_adam3 = pd.DataFrame(cm_adam3, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam3, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 4,'Adam',3,64,'relu',32,'LeakyReLU',1,'sigmoid', 0, 'No layer',0, 'No layer',\n",
    "                   bankdata_model_adam3.history.history['loss'][-1], bankdata_model_adam3.history.history['accuracy'][-1], \n",
    "                   eva_results_adam3[0],eva_results_adam3[1],recall_score(y_test.values,Y_pred_value_class_adam3), \n",
    "                   precision_score(y_test.values, Y_pred_value_class_adam3), \n",
    "                   f1_score(y_test.values,Y_pred_value_class_adam3)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "4         0.742931  0.573413  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5  Case 5 using Adam optimizer  with LeakyRelu activation function , with 4 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam4 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model , with total 5 layers. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam4.add(Dense(128, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_adam4.add(Dense(64, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_adam4.add(Dense(32, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_adam4.add(Dense(16, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_adam4.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_adam4 = optimizers.Adam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam4.compile(optimizer = gd_optimizer_adam4, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 14,337\n",
      "Trainable params: 14,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_adam4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/280\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.6147 - accuracy: 0.7203\n",
      "Epoch 2/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.4723 - accuracy: 0.7974\n",
      "Epoch 3/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4260 - accuracy: 0.7983\n",
      "Epoch 4/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3986 - accuracy: 0.8149\n",
      "Epoch 5/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3805 - accuracy: 0.8306\n",
      "Epoch 6/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3674 - accuracy: 0.8391\n",
      "Epoch 7/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3563 - accuracy: 0.8490\n",
      "Epoch 8/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3488 - accuracy: 0.8500\n",
      "Epoch 9/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3432 - accuracy: 0.8551\n",
      "Epoch 10/280\n",
      "7000/7000 [==============================] - 0s 14us/sample - loss: 0.3383 - accuracy: 0.8600\n",
      "Epoch 11/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3332 - accuracy: 0.8597\n",
      "Epoch 12/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3291 - accuracy: 0.8644\n",
      "Epoch 13/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3247 - accuracy: 0.8651\n",
      "Epoch 14/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3218 - accuracy: 0.8686\n",
      "Epoch 15/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3169 - accuracy: 0.8706\n",
      "Epoch 16/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3163 - accuracy: 0.8694\n",
      "Epoch 17/280\n",
      "7000/7000 [==============================] - 0s 17us/sample - loss: 0.3127 - accuracy: 0.8720\n",
      "Epoch 18/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.3104 - accuracy: 0.8724\n",
      "Epoch 19/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3069 - accuracy: 0.8763\n",
      "Epoch 20/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3035 - accuracy: 0.8787\n",
      "Epoch 21/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2990 - accuracy: 0.8777\n",
      "Epoch 22/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2991 - accuracy: 0.8814\n",
      "Epoch 23/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2972 - accuracy: 0.8790\n",
      "Epoch 24/280\n",
      "7000/7000 [==============================] - 0s 14us/sample - loss: 0.2937 - accuracy: 0.8817\n",
      "Epoch 25/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2893 - accuracy: 0.8836\n",
      "Epoch 26/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2841 - accuracy: 0.8880\n",
      "Epoch 27/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2827 - accuracy: 0.8886\n",
      "Epoch 28/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2816 - accuracy: 0.8861\n",
      "Epoch 29/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2787 - accuracy: 0.8897\n",
      "Epoch 30/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2810 - accuracy: 0.8869\n",
      "Epoch 31/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2770 - accuracy: 0.8904\n",
      "Epoch 32/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.2720 - accuracy: 0.8931\n",
      "Epoch 33/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2676 - accuracy: 0.8940\n",
      "Epoch 34/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2657 - accuracy: 0.8960\n",
      "Epoch 35/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2635 - accuracy: 0.8991\n",
      "Epoch 36/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2627 - accuracy: 0.8980\n",
      "Epoch 37/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2559 - accuracy: 0.9016\n",
      "Epoch 38/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.2550 - accuracy: 0.9000\n",
      "Epoch 39/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2540 - accuracy: 0.8987\n",
      "Epoch 40/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2487 - accuracy: 0.9041\n",
      "Epoch 41/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2546 - accuracy: 0.8996\n",
      "Epoch 42/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2455 - accuracy: 0.9020\n",
      "Epoch 43/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2410 - accuracy: 0.9069\n",
      "Epoch 44/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2357 - accuracy: 0.9081\n",
      "Epoch 45/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2354 - accuracy: 0.9081\n",
      "Epoch 46/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.2330 - accuracy: 0.9111\n",
      "Epoch 47/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2341 - accuracy: 0.9114\n",
      "Epoch 48/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2298 - accuracy: 0.9137\n",
      "Epoch 49/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2280 - accuracy: 0.9080\n",
      "Epoch 50/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2225 - accuracy: 0.9150\n",
      "Epoch 51/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2200 - accuracy: 0.9157\n",
      "Epoch 52/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2140 - accuracy: 0.9170\n",
      "Epoch 53/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2144 - accuracy: 0.9179\n",
      "Epoch 54/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2247 - accuracy: 0.9097\n",
      "Epoch 55/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2143 - accuracy: 0.9140\n",
      "Epoch 56/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2125 - accuracy: 0.9191\n",
      "Epoch 57/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2055 - accuracy: 0.9229\n",
      "Epoch 58/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1995 - accuracy: 0.9239\n",
      "Epoch 59/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1972 - accuracy: 0.9243\n",
      "Epoch 60/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1925 - accuracy: 0.9269\n",
      "Epoch 61/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1936 - accuracy: 0.9234\n",
      "Epoch 62/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1948 - accuracy: 0.9256\n",
      "Epoch 63/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1882 - accuracy: 0.9289\n",
      "Epoch 64/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1915 - accuracy: 0.9270\n",
      "Epoch 65/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1846 - accuracy: 0.9313\n",
      "Epoch 66/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1878 - accuracy: 0.9293\n",
      "Epoch 67/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1735 - accuracy: 0.9351\n",
      "Epoch 68/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1706 - accuracy: 0.9363\n",
      "Epoch 69/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1714 - accuracy: 0.9374\n",
      "Epoch 70/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1648 - accuracy: 0.9429\n",
      "Epoch 71/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1713 - accuracy: 0.9339\n",
      "Epoch 72/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1785 - accuracy: 0.9317\n",
      "Epoch 73/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1662 - accuracy: 0.9386\n",
      "Epoch 74/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1601 - accuracy: 0.9413\n",
      "Epoch 75/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1596 - accuracy: 0.9377\n",
      "Epoch 76/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1518 - accuracy: 0.9456\n",
      "Epoch 77/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1545 - accuracy: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1498 - accuracy: 0.9457\n",
      "Epoch 79/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1522 - accuracy: 0.9433\n",
      "Epoch 80/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1591 - accuracy: 0.9393\n",
      "Epoch 81/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1507 - accuracy: 0.9414\n",
      "Epoch 82/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1535 - accuracy: 0.9417\n",
      "Epoch 83/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1467 - accuracy: 0.9439\n",
      "Epoch 84/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1398 - accuracy: 0.9480\n",
      "Epoch 85/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1464 - accuracy: 0.9450\n",
      "Epoch 86/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1486 - accuracy: 0.9419\n",
      "Epoch 87/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1400 - accuracy: 0.9454\n",
      "Epoch 88/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1323 - accuracy: 0.9527\n",
      "Epoch 89/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1368 - accuracy: 0.9466\n",
      "Epoch 90/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1275 - accuracy: 0.9539\n",
      "Epoch 91/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1271 - accuracy: 0.9556\n",
      "Epoch 92/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1290 - accuracy: 0.9507\n",
      "Epoch 93/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1194 - accuracy: 0.9583\n",
      "Epoch 94/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1299 - accuracy: 0.9510\n",
      "Epoch 95/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1318 - accuracy: 0.9489\n",
      "Epoch 96/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1378 - accuracy: 0.9456\n",
      "Epoch 97/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1250 - accuracy: 0.9533\n",
      "Epoch 98/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1136 - accuracy: 0.9584\n",
      "Epoch 99/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1122 - accuracy: 0.9571\n",
      "Epoch 100/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1083 - accuracy: 0.9631\n",
      "Epoch 101/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1124 - accuracy: 0.9559\n",
      "Epoch 102/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1320 - accuracy: 0.9494\n",
      "Epoch 103/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1434 - accuracy: 0.9401\n",
      "Epoch 104/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1178 - accuracy: 0.9550\n",
      "Epoch 105/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1030 - accuracy: 0.9637\n",
      "Epoch 106/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0999 - accuracy: 0.9663\n",
      "Epoch 107/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0999 - accuracy: 0.9643\n",
      "Epoch 108/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0995 - accuracy: 0.9649\n",
      "Epoch 109/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1074 - accuracy: 0.9601\n",
      "Epoch 110/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0971 - accuracy: 0.9661\n",
      "Epoch 111/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0924 - accuracy: 0.9674\n",
      "Epoch 112/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0904 - accuracy: 0.9703\n",
      "Epoch 113/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0913 - accuracy: 0.9693\n",
      "Epoch 114/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0873 - accuracy: 0.9711\n",
      "Epoch 115/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0872 - accuracy: 0.9716\n",
      "Epoch 116/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0907 - accuracy: 0.9676\n",
      "Epoch 117/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0865 - accuracy: 0.9699\n",
      "Epoch 118/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0917 - accuracy: 0.9673\n",
      "Epoch 119/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0954 - accuracy: 0.9639\n",
      "Epoch 120/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0850 - accuracy: 0.9727\n",
      "Epoch 121/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0990 - accuracy: 0.9613\n",
      "Epoch 122/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1014 - accuracy: 0.9590\n",
      "Epoch 123/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0879 - accuracy: 0.9681\n",
      "Epoch 124/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0821 - accuracy: 0.9699\n",
      "Epoch 125/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0782 - accuracy: 0.9729\n",
      "Epoch 126/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0814 - accuracy: 0.9711\n",
      "Epoch 127/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0863 - accuracy: 0.9660\n",
      "Epoch 128/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0892 - accuracy: 0.9684\n",
      "Epoch 129/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0813 - accuracy: 0.9711\n",
      "Epoch 130/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0782 - accuracy: 0.9719\n",
      "Epoch 131/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0732 - accuracy: 0.9741\n",
      "Epoch 132/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0798 - accuracy: 0.9704\n",
      "Epoch 133/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0734 - accuracy: 0.9757\n",
      "Epoch 134/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0705 - accuracy: 0.9737\n",
      "Epoch 135/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0660 - accuracy: 0.9781\n",
      "Epoch 136/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0666 - accuracy: 0.9773\n",
      "Epoch 137/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0805 - accuracy: 0.9686\n",
      "Epoch 138/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0831 - accuracy: 0.9671\n",
      "Epoch 139/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0677 - accuracy: 0.9756\n",
      "Epoch 140/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0700 - accuracy: 0.9751\n",
      "Epoch 141/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0681 - accuracy: 0.9773\n",
      "Epoch 142/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0670 - accuracy: 0.9754\n",
      "Epoch 143/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0602 - accuracy: 0.9799\n",
      "Epoch 144/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0677 - accuracy: 0.9750\n",
      "Epoch 145/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0646 - accuracy: 0.9777\n",
      "Epoch 146/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0642 - accuracy: 0.9767\n",
      "Epoch 147/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0633 - accuracy: 0.9769\n",
      "Epoch 148/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0664 - accuracy: 0.9757\n",
      "Epoch 149/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0654 - accuracy: 0.9767\n",
      "Epoch 150/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0795 - accuracy: 0.9690\n",
      "Epoch 151/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0659 - accuracy: 0.9751\n",
      "Epoch 152/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0546 - accuracy: 0.9821\n",
      "Epoch 153/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0469 - accuracy: 0.9864\n",
      "Epoch 154/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0464 - accuracy: 0.9847\n",
      "Epoch 155/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0458 - accuracy: 0.9856\n",
      "Epoch 156/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0505 - accuracy: 0.9834\n",
      "Epoch 157/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0467 - accuracy: 0.9846\n",
      "Epoch 158/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0432 - accuracy: 0.9863\n",
      "Epoch 159/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0475 - accuracy: 0.9833\n",
      "Epoch 160/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0422 - accuracy: 0.9881\n",
      "Epoch 161/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0439 - accuracy: 0.9850\n",
      "Epoch 162/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0509 - accuracy: 0.9811\n",
      "Epoch 163/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0405 - accuracy: 0.9880\n",
      "Epoch 164/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0437 - accuracy: 0.9857\n",
      "Epoch 165/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0420 - accuracy: 0.9866\n",
      "Epoch 166/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0410 - accuracy: 0.9859\n",
      "Epoch 167/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0585 - accuracy: 0.9776\n",
      "Epoch 168/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0582 - accuracy: 0.9784\n",
      "Epoch 169/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0469 - accuracy: 0.9840\n",
      "Epoch 170/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0411 - accuracy: 0.9857\n",
      "Epoch 171/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0432 - accuracy: 0.9851\n",
      "Epoch 172/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0705 - accuracy: 0.9700\n",
      "Epoch 173/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0813 - accuracy: 0.9666\n",
      "Epoch 174/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0736 - accuracy: 0.9697\n",
      "Epoch 175/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0729 - accuracy: 0.9707\n",
      "Epoch 176/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0615 - accuracy: 0.9777\n",
      "Epoch 177/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0425 - accuracy: 0.9860\n",
      "Epoch 178/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0363 - accuracy: 0.9889\n",
      "Epoch 179/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0323 - accuracy: 0.9906\n",
      "Epoch 180/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0379 - accuracy: 0.9870\n",
      "Epoch 181/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0426 - accuracy: 0.9854\n",
      "Epoch 182/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0420 - accuracy: 0.9840\n",
      "Epoch 183/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0347 - accuracy: 0.9894\n",
      "Epoch 184/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0304 - accuracy: 0.9914\n",
      "Epoch 185/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0297 - accuracy: 0.9919\n",
      "Epoch 186/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0351 - accuracy: 0.9883\n",
      "Epoch 187/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0477 - accuracy: 0.9814\n",
      "Epoch 188/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0503 - accuracy: 0.9816\n",
      "Epoch 189/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0366 - accuracy: 0.9876\n",
      "Epoch 190/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0282 - accuracy: 0.9929\n",
      "Epoch 191/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0254 - accuracy: 0.9934\n",
      "Epoch 192/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0226 - accuracy: 0.9934\n",
      "Epoch 193/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0222 - accuracy: 0.9941\n",
      "Epoch 194/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0206 - accuracy: 0.9953\n",
      "Epoch 195/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0219 - accuracy: 0.9947\n",
      "Epoch 196/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0225 - accuracy: 0.9936\n",
      "Epoch 197/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0201 - accuracy: 0.9956\n",
      "Epoch 198/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0210 - accuracy: 0.9941\n",
      "Epoch 199/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0197 - accuracy: 0.9946\n",
      "Epoch 200/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0219 - accuracy: 0.9947\n",
      "Epoch 201/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0208 - accuracy: 0.9954\n",
      "Epoch 202/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0230 - accuracy: 0.9930\n",
      "Epoch 203/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0228 - accuracy: 0.9941\n",
      "Epoch 204/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0203 - accuracy: 0.9944\n",
      "Epoch 205/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0229 - accuracy: 0.9929\n",
      "Epoch 206/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0279 - accuracy: 0.9907\n",
      "Epoch 207/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0297 - accuracy: 0.9890\n",
      "Epoch 208/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0335 - accuracy: 0.9887\n",
      "Epoch 209/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0326 - accuracy: 0.9891\n",
      "Epoch 210/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0324 - accuracy: 0.9886\n",
      "Epoch 211/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0260 - accuracy: 0.9924\n",
      "Epoch 212/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0274 - accuracy: 0.9909\n",
      "Epoch 213/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0236 - accuracy: 0.9923\n",
      "Epoch 214/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0181 - accuracy: 0.9957\n",
      "Epoch 215/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0142 - accuracy: 0.9967\n",
      "Epoch 216/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0184 - accuracy: 0.9954\n",
      "Epoch 217/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0169 - accuracy: 0.9953\n",
      "Epoch 218/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0219 - accuracy: 0.9934\n",
      "Epoch 219/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0242 - accuracy: 0.9920\n",
      "Epoch 220/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0366 - accuracy: 0.9851\n",
      "Epoch 221/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0373 - accuracy: 0.9859\n",
      "Epoch 222/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0283 - accuracy: 0.9899\n",
      "Epoch 223/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0258 - accuracy: 0.9903\n",
      "Epoch 224/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0254 - accuracy: 0.9900\n",
      "Epoch 225/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0347 - accuracy: 0.9874\n",
      "Epoch 226/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0310 - accuracy: 0.9887\n",
      "Epoch 227/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0246 - accuracy: 0.9920\n",
      "Epoch 228/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0447 - accuracy: 0.9829\n",
      "Epoch 229/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0377 - accuracy: 0.9856\n",
      "Epoch 230/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0373 - accuracy: 0.9870\n",
      "Epoch 231/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0289 - accuracy: 0.9901\n",
      "Epoch 232/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0186 - accuracy: 0.9953\n",
      "Epoch 233/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0247 - accuracy: 0.9914\n",
      "Epoch 234/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0312 - accuracy: 0.9877\n",
      "Epoch 235/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0376 - accuracy: 0.9854\n",
      "Epoch 236/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0367 - accuracy: 0.9849\n",
      "Epoch 237/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0346 - accuracy: 0.9871\n",
      "Epoch 238/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0366 - accuracy: 0.9860\n",
      "Epoch 239/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0288 - accuracy: 0.9903\n",
      "Epoch 240/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0241 - accuracy: 0.9914\n",
      "Epoch 241/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0203 - accuracy: 0.9926\n",
      "Epoch 242/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0172 - accuracy: 0.9949\n",
      "Epoch 243/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0177 - accuracy: 0.9947\n",
      "Epoch 244/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0130 - accuracy: 0.9971\n",
      "Epoch 245/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0144 - accuracy: 0.9959\n",
      "Epoch 246/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0143 - accuracy: 0.9963\n",
      "Epoch 247/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0108 - accuracy: 0.9974\n",
      "Epoch 248/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0083 - accuracy: 0.9989\n",
      "Epoch 249/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0079 - accuracy: 0.9991\n",
      "Epoch 250/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0094 - accuracy: 0.9980\n",
      "Epoch 251/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0093 - accuracy: 0.9981\n",
      "Epoch 252/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0101 - accuracy: 0.9979\n",
      "Epoch 253/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0089 - accuracy: 0.9984\n",
      "Epoch 254/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0087 - accuracy: 0.9987\n",
      "Epoch 255/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 256/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0079 - accuracy: 0.9986\n",
      "Epoch 257/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0084 - accuracy: 0.9987\n",
      "Epoch 258/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0098 - accuracy: 0.9976\n",
      "Epoch 259/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0086 - accuracy: 0.9980\n",
      "Epoch 260/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0144 - accuracy: 0.9960\n",
      "Epoch 261/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0148 - accuracy: 0.9954\n",
      "Epoch 262/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0294 - accuracy: 0.9900\n",
      "Epoch 263/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2258 - accuracy: 0.9410\n",
      "Epoch 264/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1652 - accuracy: 0.9511\n",
      "Epoch 265/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0906 - accuracy: 0.9669\n",
      "Epoch 266/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0506 - accuracy: 0.9807\n",
      "Epoch 267/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0348 - accuracy: 0.9873\n",
      "Epoch 268/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0253 - accuracy: 0.9916\n",
      "Epoch 269/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0133 - accuracy: 0.9966\n",
      "Epoch 270/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0104 - accuracy: 0.9989\n",
      "Epoch 271/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0077 - accuracy: 0.9993\n",
      "Epoch 272/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0078 - accuracy: 0.9991\n",
      "Epoch 273/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0069 - accuracy: 0.9993\n",
      "Epoch 274/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0071 - accuracy: 0.9994\n",
      "Epoch 275/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0077 - accuracy: 0.9990\n",
      "Epoch 276/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0074 - accuracy: 0.9993\n",
      "Epoch 277/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0063 - accuracy: 0.9994\n",
      "Epoch 278/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0059 - accuracy: 0.9996\n",
      "Epoch 279/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0057 - accuracy: 0.9996\n",
      "Epoch 280/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0060 - accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bce8c2988>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_adam4.fit(X_train, y_train, batch_size = 500, epochs = 280, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows model overfitted to Train data , with accuracy reaching 0.9997 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 94us/sample - loss: 1.8825 - accuracy: 0.7967\n"
     ]
    }
   ],
   "source": [
    "eva_results_adam4 = bankdata_model_adam4.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[1.8824576695760091, 0.7966667]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_adam4.metrics_names)\n",
    "print(eva_results_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It can be seen that with Test data , accuracy drops , since model was overfitted to Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 23us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_adam4 = bankdata_model_adam4.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 11us/sample\n",
      "[[1.9396774e-02]\n",
      " [3.0608335e-06]\n",
      " [4.6065473e-03]\n",
      " ...\n",
      " [9.9998999e-01]\n",
      " [5.2769786e-07]\n",
      " [1.6500064e-05]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_adam4 = bankdata_model_adam4.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_adam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2409, 1: 591}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_adam4 = (Y_pred_value_adam4 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_adam4)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_adam4, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 42us/sample - loss: 1.8825 - accuracy: 0.7967\n",
      "Accuracy of Model with Adam optimizer :0.7966667\n",
      "Recall_score: 0.48465266558966075\n",
      "Precision_score: 0.5076142131979695\n",
      "F-score: 0.4958677685950413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2090,  291],\n",
       "       [ 319,  300]], dtype=int64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_adam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_adam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_adam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 35us/sample - loss: 1.8825 - accuracy: 0.7967\n",
      "Accuracy of Model with Adam optimizer :0.7966667\n",
      "Recall_score: 0.48465266558966075\n",
      "Precision_score: 0.5076142131979695\n",
      "F-score: 0.4958677685950413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2090,  291],\n",
       "       [ 319,  300]], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_adam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_adam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_adam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_adam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam4 = confusion_matrix(y_test.values, Y_pred_value_class_adam4)\n",
    "df_cm_adam4 = pd.DataFrame(cm_adam4, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam4, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 5,'Adam',5,128,'relu',64,'LeakyReLU',32,'LeakyReLU',16,'LeakyReLU',1,'sigmoid', \n",
    "                   bankdata_model_adam4.history.history['loss'][-1],\n",
    "                            bankdata_model_adam4.history.history['accuracy'][-1], eva_results_adam4[0],eva_results_adam4[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_adam4), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_adam4), f1_score(y_test.values,Y_pred_value_class_adam4)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.882458</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "5    LeakyReLU              1      sigmoid         0.005982   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "5             0.999429        1.882458            0.796667      0.484653   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "4         0.742931  0.573413  \n",
       "5         0.507614  0.495868  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Case 6 using NAdam optimizer  with LeakyRelu activation function , with 4 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model , with total 5 layers. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam2.add(Dense(128, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_nadam2.add(Dense(64, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam2.add(Dense(32, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam2.add(Dense(16, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam2.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam2 = optimizers.Nadam(lr = 0.001, beta_1 = 0.8) ## use beta_1=0.8 (default=0.9) , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam2.compile(optimizer = gd_optimizer_nadam2, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 14,337\n",
      "Trainable params: 14,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/280\n",
      "7000/7000 [==============================] - 1s 120us/sample - loss: 0.5559 - accuracy: 0.7311\n",
      "Epoch 2/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4174 - accuracy: 0.8094\n",
      "Epoch 3/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3854 - accuracy: 0.8287\n",
      "Epoch 4/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3688 - accuracy: 0.8421\n",
      "Epoch 5/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.3591 - accuracy: 0.8453\n",
      "Epoch 6/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3504 - accuracy: 0.8523\n",
      "Epoch 7/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3421 - accuracy: 0.8587\n",
      "Epoch 8/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3373 - accuracy: 0.8589\n",
      "Epoch 9/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3370 - accuracy: 0.8593\n",
      "Epoch 10/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3305 - accuracy: 0.8626\n",
      "Epoch 11/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3255 - accuracy: 0.8644\n",
      "Epoch 12/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3270 - accuracy: 0.8659\n",
      "Epoch 13/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3243 - accuracy: 0.8679\n",
      "Epoch 14/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.3196 - accuracy: 0.8697\n",
      "Epoch 15/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3155 - accuracy: 0.8713\n",
      "Epoch 16/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3177 - accuracy: 0.8680\n",
      "Epoch 17/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3105 - accuracy: 0.8721\n",
      "Epoch 18/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3099 - accuracy: 0.8719\n",
      "Epoch 19/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3066 - accuracy: 0.8739\n",
      "Epoch 20/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3055 - accuracy: 0.8767\n",
      "Epoch 21/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3036 - accuracy: 0.8757\n",
      "Epoch 22/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3009 - accuracy: 0.8774\n",
      "Epoch 23/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3069 - accuracy: 0.8740\n",
      "Epoch 24/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2982 - accuracy: 0.8784\n",
      "Epoch 25/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2964 - accuracy: 0.8800\n",
      "Epoch 26/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2925 - accuracy: 0.8837\n",
      "Epoch 27/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2944 - accuracy: 0.8806\n",
      "Epoch 28/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2902 - accuracy: 0.8809\n",
      "Epoch 29/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2942 - accuracy: 0.8811\n",
      "Epoch 30/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3035 - accuracy: 0.8759\n",
      "Epoch 31/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2845 - accuracy: 0.8850\n",
      "Epoch 32/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2947 - accuracy: 0.8811\n",
      "Epoch 33/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2826 - accuracy: 0.8876\n",
      "Epoch 34/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2796 - accuracy: 0.8854\n",
      "Epoch 35/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2780 - accuracy: 0.8877\n",
      "Epoch 36/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2830 - accuracy: 0.8816\n",
      "Epoch 37/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2863 - accuracy: 0.8810\n",
      "Epoch 38/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2762 - accuracy: 0.8873\n",
      "Epoch 39/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2740 - accuracy: 0.8907\n",
      "Epoch 40/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2688 - accuracy: 0.8917\n",
      "Epoch 41/280\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.88 - 0s 7us/sample - loss: 0.2799 - accuracy: 0.8844\n",
      "Epoch 42/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2643 - accuracy: 0.8930\n",
      "Epoch 43/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2775 - accuracy: 0.8874\n",
      "Epoch 44/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2653 - accuracy: 0.8919\n",
      "Epoch 45/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2700 - accuracy: 0.8893\n",
      "Epoch 46/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2727 - accuracy: 0.8909\n",
      "Epoch 47/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2537 - accuracy: 0.8970\n",
      "Epoch 48/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2607 - accuracy: 0.8936\n",
      "Epoch 49/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2545 - accuracy: 0.8994\n",
      "Epoch 50/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2571 - accuracy: 0.8960\n",
      "Epoch 51/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2642 - accuracy: 0.8947\n",
      "Epoch 52/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2590 - accuracy: 0.8936\n",
      "Epoch 53/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2493 - accuracy: 0.8976\n",
      "Epoch 54/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2424 - accuracy: 0.9013\n",
      "Epoch 55/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2564 - accuracy: 0.8933\n",
      "Epoch 56/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2541 - accuracy: 0.8966\n",
      "Epoch 57/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2462 - accuracy: 0.8961\n",
      "Epoch 58/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2462 - accuracy: 0.8989\n",
      "Epoch 59/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2354 - accuracy: 0.9029\n",
      "Epoch 60/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2472 - accuracy: 0.8970\n",
      "Epoch 61/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2618 - accuracy: 0.8936\n",
      "Epoch 62/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2499 - accuracy: 0.8967\n",
      "Epoch 63/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2480 - accuracy: 0.8986\n",
      "Epoch 64/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2266 - accuracy: 0.9104\n",
      "Epoch 65/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2240 - accuracy: 0.9126\n",
      "Epoch 66/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2409 - accuracy: 0.9039\n",
      "Epoch 67/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2226 - accuracy: 0.9114\n",
      "Epoch 68/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2285 - accuracy: 0.9006\n",
      "Epoch 69/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2254 - accuracy: 0.9079\n",
      "Epoch 70/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2454 - accuracy: 0.8973\n",
      "Epoch 71/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2443 - accuracy: 0.9024\n",
      "Epoch 72/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2146 - accuracy: 0.9146\n",
      "Epoch 73/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2388 - accuracy: 0.9034\n",
      "Epoch 74/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2105 - accuracy: 0.9154\n",
      "Epoch 75/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2390 - accuracy: 0.9020\n",
      "Epoch 76/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2233 - accuracy: 0.9100\n",
      "Epoch 77/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2100 - accuracy: 0.9169\n",
      "Epoch 78/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2143 - accuracy: 0.9127\n",
      "Epoch 79/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2305 - accuracy: 0.9083\n",
      "Epoch 80/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1970 - accuracy: 0.9216\n",
      "Epoch 81/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1927 - accuracy: 0.9259\n",
      "Epoch 82/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2434 - accuracy: 0.9000\n",
      "Epoch 83/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2035 - accuracy: 0.9187\n",
      "Epoch 84/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2005 - accuracy: 0.9189\n",
      "Epoch 85/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2072 - accuracy: 0.9164\n",
      "Epoch 86/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2022 - accuracy: 0.9217\n",
      "Epoch 87/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2245 - accuracy: 0.9093\n",
      "Epoch 88/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2031 - accuracy: 0.9176\n",
      "Epoch 89/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2006 - accuracy: 0.9181\n",
      "Epoch 90/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1925 - accuracy: 0.9221\n",
      "Epoch 91/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2262 - accuracy: 0.9070\n",
      "Epoch 92/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1936 - accuracy: 0.9224\n",
      "Epoch 93/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1887 - accuracy: 0.9250\n",
      "Epoch 94/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2096 - accuracy: 0.9171\n",
      "Epoch 95/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1969 - accuracy: 0.9226\n",
      "Epoch 96/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1914 - accuracy: 0.9241\n",
      "Epoch 97/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1702 - accuracy: 0.9354\n",
      "Epoch 98/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2038 - accuracy: 0.9171\n",
      "Epoch 99/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1923 - accuracy: 0.9229\n",
      "Epoch 100/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1728 - accuracy: 0.9339\n",
      "Epoch 101/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2120 - accuracy: 0.9100\n",
      "Epoch 102/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1652 - accuracy: 0.9379\n",
      "Epoch 103/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1983 - accuracy: 0.9186\n",
      "Epoch 104/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1864 - accuracy: 0.9254\n",
      "Epoch 105/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1929 - accuracy: 0.9213\n",
      "Epoch 106/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1654 - accuracy: 0.9357\n",
      "Epoch 107/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1941 - accuracy: 0.9220\n",
      "Epoch 108/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1626 - accuracy: 0.9383\n",
      "Epoch 109/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1739 - accuracy: 0.9324\n",
      "Epoch 110/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1495 - accuracy: 0.9449\n",
      "Epoch 111/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2220 - accuracy: 0.9121\n",
      "Epoch 112/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1621 - accuracy: 0.9361\n",
      "Epoch 113/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1612 - accuracy: 0.9380\n",
      "Epoch 114/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2068 - accuracy: 0.9143\n",
      "Epoch 115/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1540 - accuracy: 0.9407\n",
      "Epoch 116/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1698 - accuracy: 0.9289\n",
      "Epoch 117/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1657 - accuracy: 0.9356\n",
      "Epoch 118/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1545 - accuracy: 0.9377\n",
      "Epoch 119/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1672 - accuracy: 0.9373\n",
      "Epoch 120/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1920 - accuracy: 0.9224\n",
      "Epoch 121/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1692 - accuracy: 0.9351\n",
      "Epoch 122/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1457 - accuracy: 0.9457\n",
      "Epoch 123/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1528 - accuracy: 0.9394\n",
      "Epoch 124/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1911 - accuracy: 0.9211\n",
      "Epoch 125/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1378 - accuracy: 0.9469\n",
      "Epoch 126/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1521 - accuracy: 0.9400\n",
      "Epoch 127/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1687 - accuracy: 0.9283\n",
      "Epoch 128/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1338 - accuracy: 0.9493\n",
      "Epoch 129/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1341 - accuracy: 0.9490\n",
      "Epoch 130/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1715 - accuracy: 0.9324\n",
      "Epoch 131/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1341 - accuracy: 0.9500\n",
      "Epoch 132/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1501 - accuracy: 0.9364\n",
      "Epoch 133/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1650 - accuracy: 0.9359\n",
      "Epoch 134/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1497 - accuracy: 0.9389\n",
      "Epoch 135/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1408 - accuracy: 0.9457\n",
      "Epoch 136/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1773 - accuracy: 0.9276\n",
      "Epoch 137/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1210 - accuracy: 0.9569\n",
      "Epoch 138/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1196 - accuracy: 0.9581\n",
      "Epoch 139/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1674 - accuracy: 0.9333\n",
      "Epoch 140/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1779 - accuracy: 0.9243\n",
      "Epoch 141/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1195 - accuracy: 0.9569\n",
      "Epoch 142/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1210 - accuracy: 0.9566\n",
      "Epoch 143/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1098 - accuracy: 0.9614\n",
      "Epoch 144/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1654 - accuracy: 0.9337\n",
      "Epoch 145/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1653 - accuracy: 0.9361\n",
      "Epoch 146/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1203 - accuracy: 0.9566\n",
      "Epoch 147/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1303 - accuracy: 0.9477\n",
      "Epoch 148/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1339 - accuracy: 0.9457\n",
      "Epoch 149/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1104 - accuracy: 0.9614\n",
      "Epoch 150/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1317 - accuracy: 0.9463\n",
      "Epoch 151/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1007 - accuracy: 0.9660\n",
      "Epoch 152/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1490 - accuracy: 0.9374\n",
      "Epoch 153/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1487 - accuracy: 0.9447\n",
      "Epoch 154/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0998 - accuracy: 0.9673\n",
      "Epoch 155/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1566 - accuracy: 0.9376\n",
      "Epoch 156/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1104 - accuracy: 0.9580\n",
      "Epoch 157/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1586 - accuracy: 0.9350\n",
      "Epoch 158/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1191 - accuracy: 0.9516\n",
      "Epoch 159/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0962 - accuracy: 0.9687\n",
      "Epoch 160/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0937 - accuracy: 0.9686\n",
      "Epoch 161/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0979 - accuracy: 0.9661\n",
      "Epoch 162/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1668 - accuracy: 0.9290\n",
      "Epoch 163/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0997 - accuracy: 0.9647\n",
      "Epoch 164/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0987 - accuracy: 0.9633\n",
      "Epoch 165/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1420 - accuracy: 0.9437\n",
      "Epoch 166/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1398 - accuracy: 0.9496\n",
      "Epoch 167/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0936 - accuracy: 0.9683\n",
      "Epoch 168/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0960 - accuracy: 0.9666\n",
      "Epoch 169/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1163 - accuracy: 0.9574\n",
      "Epoch 170/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0895 - accuracy: 0.9693\n",
      "Epoch 171/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2103 - accuracy: 0.9257\n",
      "Epoch 172/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0947 - accuracy: 0.9676\n",
      "Epoch 173/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0932 - accuracy: 0.9687\n",
      "Epoch 174/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0825 - accuracy: 0.9756\n",
      "Epoch 175/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0815 - accuracy: 0.9751\n",
      "Epoch 176/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1056 - accuracy: 0.9581\n",
      "Epoch 177/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0788 - accuracy: 0.9749\n",
      "Epoch 178/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2105 - accuracy: 0.9334\n",
      "Epoch 179/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0884 - accuracy: 0.9691\n",
      "Epoch 180/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0860 - accuracy: 0.9706\n",
      "Epoch 181/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1803 - accuracy: 0.9330\n",
      "Epoch 182/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0822 - accuracy: 0.9716\n",
      "Epoch 183/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0776 - accuracy: 0.9750\n",
      "Epoch 184/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0851 - accuracy: 0.9694\n",
      "Epoch 185/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0726 - accuracy: 0.9773\n",
      "Epoch 186/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0698 - accuracy: 0.9804\n",
      "Epoch 187/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1659 - accuracy: 0.9327\n",
      "Epoch 188/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0780 - accuracy: 0.9747\n",
      "Epoch 189/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0906 - accuracy: 0.9667\n",
      "Epoch 190/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0774 - accuracy: 0.9724\n",
      "Epoch 191/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0821 - accuracy: 0.9696\n",
      "Epoch 192/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1143 - accuracy: 0.9539\n",
      "Epoch 193/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0661 - accuracy: 0.9814\n",
      "Epoch 194/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0784 - accuracy: 0.9710\n",
      "Epoch 195/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0850 - accuracy: 0.9670\n",
      "Epoch 196/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0627 - accuracy: 0.9814\n",
      "Epoch 197/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0821 - accuracy: 0.9697\n",
      "Epoch 198/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0686 - accuracy: 0.9780\n",
      "Epoch 199/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2112 - accuracy: 0.9239\n",
      "Epoch 200/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1227 - accuracy: 0.9539\n",
      "Epoch 201/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0640 - accuracy: 0.9823\n",
      "Epoch 202/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0617 - accuracy: 0.9807\n",
      "Epoch 203/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0656 - accuracy: 0.9786\n",
      "Epoch 204/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0555 - accuracy: 0.9870\n",
      "Epoch 205/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0529 - accuracy: 0.9866\n",
      "Epoch 206/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1239 - accuracy: 0.9531\n",
      "Epoch 207/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1044 - accuracy: 0.9694\n",
      "Epoch 208/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0561 - accuracy: 0.9847\n",
      "Epoch 209/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0511 - accuracy: 0.9879\n",
      "Epoch 210/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0653 - accuracy: 0.9786\n",
      "Epoch 211/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0528 - accuracy: 0.9873\n",
      "Epoch 212/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0641 - accuracy: 0.9773\n",
      "Epoch 213/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1779 - accuracy: 0.9447\n",
      "Epoch 214/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0635 - accuracy: 0.9803\n",
      "Epoch 215/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0529 - accuracy: 0.9850\n",
      "Epoch 216/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0507 - accuracy: 0.9847\n",
      "Epoch 217/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0578 - accuracy: 0.9834\n",
      "Epoch 218/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2532 - accuracy: 0.9310\n",
      "Epoch 219/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0554 - accuracy: 0.9856\n",
      "Epoch 220/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0480 - accuracy: 0.9890\n",
      "Epoch 221/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0453 - accuracy: 0.9901\n",
      "Epoch 222/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0457 - accuracy: 0.9893\n",
      "Epoch 223/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0428 - accuracy: 0.9907\n",
      "Epoch 224/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0433 - accuracy: 0.9899\n",
      "Epoch 225/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0413 - accuracy: 0.9901\n",
      "Epoch 226/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3139 - accuracy: 0.9267\n",
      "Epoch 227/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0579 - accuracy: 0.9847\n",
      "Epoch 228/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0467 - accuracy: 0.9909\n",
      "Epoch 229/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0431 - accuracy: 0.9909\n",
      "Epoch 230/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0450 - accuracy: 0.9881\n",
      "Epoch 231/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0431 - accuracy: 0.9899\n",
      "Epoch 232/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0432 - accuracy: 0.9894\n",
      "Epoch 233/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0391 - accuracy: 0.9920\n",
      "Epoch 234/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0402 - accuracy: 0.9901\n",
      "Epoch 235/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0383 - accuracy: 0.9923\n",
      "Epoch 236/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0362 - accuracy: 0.9927\n",
      "Epoch 237/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0354 - accuracy: 0.9931\n",
      "Epoch 238/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0502 - accuracy: 0.9836\n",
      "Epoch 239/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2453 - accuracy: 0.9447\n",
      "Epoch 240/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0496 - accuracy: 0.9860\n",
      "Epoch 241/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0365 - accuracy: 0.9936\n",
      "Epoch 242/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0341 - accuracy: 0.9944\n",
      "Epoch 243/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0328 - accuracy: 0.9946\n",
      "Epoch 244/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0333 - accuracy: 0.9951\n",
      "Epoch 245/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0322 - accuracy: 0.9941\n",
      "Epoch 246/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0376 - accuracy: 0.9919\n",
      "Epoch 247/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0306 - accuracy: 0.9944\n",
      "Epoch 248/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0287 - accuracy: 0.9943\n",
      "Epoch 249/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0290 - accuracy: 0.9954\n",
      "Epoch 250/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0321 - accuracy: 0.9930\n",
      "Epoch 251/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0278 - accuracy: 0.9957\n",
      "Epoch 252/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1005 - accuracy: 0.9681\n",
      "Epoch 253/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3605 - accuracy: 0.9331\n",
      "Epoch 254/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0453 - accuracy: 0.9897\n",
      "Epoch 255/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0363 - accuracy: 0.9940\n",
      "Epoch 256/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0317 - accuracy: 0.9960\n",
      "Epoch 257/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0295 - accuracy: 0.9967\n",
      "Epoch 258/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0295 - accuracy: 0.9957\n",
      "Epoch 259/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0272 - accuracy: 0.9961\n",
      "Epoch 260/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0273 - accuracy: 0.9966\n",
      "Epoch 261/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0263 - accuracy: 0.9971\n",
      "Epoch 262/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0253 - accuracy: 0.9969\n",
      "Epoch 263/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0240 - accuracy: 0.9977\n",
      "Epoch 264/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0294 - accuracy: 0.9944\n",
      "Epoch 265/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0251 - accuracy: 0.9967\n",
      "Epoch 266/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0231 - accuracy: 0.9973\n",
      "Epoch 267/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0233 - accuracy: 0.9971\n",
      "Epoch 268/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0223 - accuracy: 0.9971\n",
      "Epoch 269/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0209 - accuracy: 0.9976\n",
      "Epoch 270/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0214 - accuracy: 0.9976\n",
      "Epoch 271/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1776 - accuracy: 0.9699\n",
      "Epoch 272/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3214 - accuracy: 0.9047\n",
      "Epoch 273/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0570 - accuracy: 0.9797\n",
      "Epoch 274/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0342 - accuracy: 0.9929\n",
      "Epoch 275/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0308 - accuracy: 0.9951\n",
      "Epoch 276/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0260 - accuracy: 0.9966\n",
      "Epoch 277/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0244 - accuracy: 0.9977\n",
      "Epoch 278/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0233 - accuracy: 0.9970\n",
      "Epoch 279/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0229 - accuracy: 0.9977\n",
      "Epoch 280/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0215 - accuracy: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bcbb56408>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam2.fit(X_train, y_train, batch_size = 500, epochs = 280, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows model overfitted to Train data , with accuracy reaching 0.9997 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 111us/sample - loss: 1.0671 - accuracy: 0.8093\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam2 = bankdata_model_nadam2.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[1.06713347474734, 0.8093333]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam2.metrics_names)\n",
    "print(eva_results_nadam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It can be seen that with Test data , accuracy drops , since model was overfitted to Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6  Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 28us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam2 = bankdata_model_nadam2.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 17us/sample\n",
      "[[6.2594446e-04]\n",
      " [7.6288379e-06]\n",
      " [8.1054264e-01]\n",
      " ...\n",
      " [1.6191301e-03]\n",
      " [5.6769371e-05]\n",
      " [6.2419587e-01]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam2 = bankdata_model_nadam2.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2437, 1: 563}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam2 = (Y_pred_value_nadam2 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam2)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_nadam2, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 39us/sample - loss: 1.0671 - accuracy: 0.8093\n",
      "Accuracy of Model with Nadam optimizer :0.8093333\n",
      "Recall_score: 0.4927302100161551\n",
      "Precision_score: 0.5417406749555951\n",
      "F-score: 0.5160744500846024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2123,  258],\n",
       "       [ 314,  305]], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 34us/sample - loss: 1.0671 - accuracy: 0.8093\n",
      "Accuracy of Model with Nadam optimizer :0.8093333\n",
      "Recall_score: 0.4927302100161551\n",
      "Precision_score: 0.5417406749555951\n",
      "F-score: 0.5160744500846024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2123,  258],\n",
       "       [ 314,  305]], dtype=int64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam2 = confusion_matrix(y_test.values, Y_pred_value_class_nadam2)\n",
    "df_cm_nadam2 = pd.DataFrame(cm_nadam2, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam2, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 6,'Nadam',5,128,'relu',64,'LeakyReLU',32,'LeakyReLU',16,'LeakyReLU',1,'sigmoid', \n",
    "                   bankdata_model_nadam2.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam2.history.history['accuracy'][-1], eva_results_nadam2[0],eva_results_nadam2[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam2), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam2), f1_score(y_test.values,Y_pred_value_class_nadam2)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.882458</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.997857</td>\n",
       "      <td>1.067133</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.492730</td>\n",
       "      <td>0.541741</td>\n",
       "      <td>0.516074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "5    LeakyReLU              1      sigmoid         0.005982   \n",
       "6    LeakyReLU              1      sigmoid         0.021518   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "5             0.999429        1.882458            0.796667      0.484653   \n",
       "6             0.997857        1.067133            0.809333      0.492730   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "4         0.742931  0.573413  \n",
       "5         0.507614  0.495868  \n",
       "6         0.541741  0.516074  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Case 7 using NAdam optimizer  with LeakyRelu activation function , with 4 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam3 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model , with total 4 layers. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam3.add(Dense(64, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_nadam3.add(Dense(32, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam3.add(Dense(16, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam3.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam3 = optimizers.Nadam(lr = 0.001, beta_1 = 0.8) ## use beta_1=0.8 (default=0.9) , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam3.compile(optimizer = gd_optimizer_nadam3, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/280\n",
      "7000/7000 [==============================] - 1s 93us/sample - loss: 0.5380 - accuracy: 0.7866\n",
      "Epoch 2/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.4478 - accuracy: 0.7981\n",
      "Epoch 3/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.4125 - accuracy: 0.8091\n",
      "Epoch 4/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3932 - accuracy: 0.8226\n",
      "Epoch 5/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3798 - accuracy: 0.8286\n",
      "Epoch 6/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3698 - accuracy: 0.8399\n",
      "Epoch 7/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3619 - accuracy: 0.8453\n",
      "Epoch 8/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3556 - accuracy: 0.8503\n",
      "Epoch 9/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3506 - accuracy: 0.8516\n",
      "Epoch 10/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3474 - accuracy: 0.8556\n",
      "Epoch 11/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3436 - accuracy: 0.8571\n",
      "Epoch 12/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3408 - accuracy: 0.8567\n",
      "Epoch 13/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3372 - accuracy: 0.8626\n",
      "Epoch 14/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3348 - accuracy: 0.8573\n",
      "Epoch 15/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3328 - accuracy: 0.8604\n",
      "Epoch 16/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3304 - accuracy: 0.8613\n",
      "Epoch 17/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3286 - accuracy: 0.8629\n",
      "Epoch 18/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3266 - accuracy: 0.8649\n",
      "Epoch 19/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3250 - accuracy: 0.8663\n",
      "Epoch 20/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3237 - accuracy: 0.8646\n",
      "Epoch 21/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3211 - accuracy: 0.8673\n",
      "Epoch 22/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8696\n",
      "Epoch 23/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3183 - accuracy: 0.8694\n",
      "Epoch 24/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3169 - accuracy: 0.8690\n",
      "Epoch 25/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3155 - accuracy: 0.8694\n",
      "Epoch 26/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3127 - accuracy: 0.8721\n",
      "Epoch 27/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3128 - accuracy: 0.8724\n",
      "Epoch 28/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3116 - accuracy: 0.8741\n",
      "Epoch 29/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3099 - accuracy: 0.8736\n",
      "Epoch 30/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3085 - accuracy: 0.8737\n",
      "Epoch 31/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3062 - accuracy: 0.8724\n",
      "Epoch 32/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3069 - accuracy: 0.8740\n",
      "Epoch 33/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3056 - accuracy: 0.8733\n",
      "Epoch 34/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3043 - accuracy: 0.8750\n",
      "Epoch 35/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3041 - accuracy: 0.8736\n",
      "Epoch 36/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3004 - accuracy: 0.8756\n",
      "Epoch 37/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2998 - accuracy: 0.8760\n",
      "Epoch 38/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2984 - accuracy: 0.8771\n",
      "Epoch 39/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2975 - accuracy: 0.8767\n",
      "Epoch 40/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2966 - accuracy: 0.8780\n",
      "Epoch 41/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2952 - accuracy: 0.8800\n",
      "Epoch 42/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2953 - accuracy: 0.8769\n",
      "Epoch 43/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2956 - accuracy: 0.8783\n",
      "Epoch 44/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2935 - accuracy: 0.8791\n",
      "Epoch 45/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2910 - accuracy: 0.8777\n",
      "Epoch 46/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2900 - accuracy: 0.8814\n",
      "Epoch 47/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2890 - accuracy: 0.8813\n",
      "Epoch 48/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2873 - accuracy: 0.8804\n",
      "Epoch 49/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2889 - accuracy: 0.8816\n",
      "Epoch 50/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2867 - accuracy: 0.8830\n",
      "Epoch 51/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2855 - accuracy: 0.8827\n",
      "Epoch 52/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2829 - accuracy: 0.8844\n",
      "Epoch 53/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2853 - accuracy: 0.8830\n",
      "Epoch 54/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2820 - accuracy: 0.8844\n",
      "Epoch 55/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2822 - accuracy: 0.8836\n",
      "Epoch 56/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2810 - accuracy: 0.8876\n",
      "Epoch 57/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2811 - accuracy: 0.8857\n",
      "Epoch 58/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2804 - accuracy: 0.8847\n",
      "Epoch 59/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2778 - accuracy: 0.8883\n",
      "Epoch 60/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2798 - accuracy: 0.8841\n",
      "Epoch 61/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2770 - accuracy: 0.8867\n",
      "Epoch 62/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2752 - accuracy: 0.8863\n",
      "Epoch 63/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2746 - accuracy: 0.8876\n",
      "Epoch 64/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2773 - accuracy: 0.8849\n",
      "Epoch 65/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2717 - accuracy: 0.8897\n",
      "Epoch 66/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2710 - accuracy: 0.8891\n",
      "Epoch 67/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2712 - accuracy: 0.8899\n",
      "Epoch 68/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2690 - accuracy: 0.8906\n",
      "Epoch 69/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2710 - accuracy: 0.8903\n",
      "Epoch 70/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2679 - accuracy: 0.8920\n",
      "Epoch 71/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2647 - accuracy: 0.8950\n",
      "Epoch 72/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2704 - accuracy: 0.8917\n",
      "Epoch 73/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2662 - accuracy: 0.8910\n",
      "Epoch 74/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2643 - accuracy: 0.8959\n",
      "Epoch 75/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2651 - accuracy: 0.8949\n",
      "Epoch 76/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2657 - accuracy: 0.8919\n",
      "Epoch 77/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2631 - accuracy: 0.8949\n",
      "Epoch 78/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2620 - accuracy: 0.8959\n",
      "Epoch 79/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2600 - accuracy: 0.8944\n",
      "Epoch 80/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2614 - accuracy: 0.8971\n",
      "Epoch 81/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2598 - accuracy: 0.8960\n",
      "Epoch 82/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2606 - accuracy: 0.8961\n",
      "Epoch 83/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2553 - accuracy: 0.8987\n",
      "Epoch 84/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2599 - accuracy: 0.8956\n",
      "Epoch 85/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2586 - accuracy: 0.8990\n",
      "Epoch 86/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2593 - accuracy: 0.8970\n",
      "Epoch 87/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2534 - accuracy: 0.8994\n",
      "Epoch 88/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2528 - accuracy: 0.9007\n",
      "Epoch 89/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2536 - accuracy: 0.8996\n",
      "Epoch 90/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2544 - accuracy: 0.8984\n",
      "Epoch 91/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2497 - accuracy: 0.9009\n",
      "Epoch 92/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2485 - accuracy: 0.9006\n",
      "Epoch 93/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2500 - accuracy: 0.8999\n",
      "Epoch 94/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2506 - accuracy: 0.9009\n",
      "Epoch 95/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2501 - accuracy: 0.8979\n",
      "Epoch 96/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2520 - accuracy: 0.8991\n",
      "Epoch 97/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2577 - accuracy: 0.8973\n",
      "Epoch 98/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2470 - accuracy: 0.9009\n",
      "Epoch 99/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2468 - accuracy: 0.9024\n",
      "Epoch 100/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2491 - accuracy: 0.8993\n",
      "Epoch 101/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2476 - accuracy: 0.9013\n",
      "Epoch 102/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2436 - accuracy: 0.9033\n",
      "Epoch 103/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2470 - accuracy: 0.9029\n",
      "Epoch 104/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2420 - accuracy: 0.9056\n",
      "Epoch 105/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2414 - accuracy: 0.9039\n",
      "Epoch 106/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2441 - accuracy: 0.9001\n",
      "Epoch 107/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2412 - accuracy: 0.9067\n",
      "Epoch 108/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2410 - accuracy: 0.9036\n",
      "Epoch 109/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2384 - accuracy: 0.9057\n",
      "Epoch 110/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2450 - accuracy: 0.9037\n",
      "Epoch 111/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2365 - accuracy: 0.9077\n",
      "Epoch 112/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2345 - accuracy: 0.9074\n",
      "Epoch 113/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2381 - accuracy: 0.9066\n",
      "Epoch 114/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2411 - accuracy: 0.9049\n",
      "Epoch 115/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2355 - accuracy: 0.9061\n",
      "Epoch 116/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2336 - accuracy: 0.9109\n",
      "Epoch 117/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2370 - accuracy: 0.9073\n",
      "Epoch 118/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2331 - accuracy: 0.9073\n",
      "Epoch 119/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2348 - accuracy: 0.9071\n",
      "Epoch 120/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2331 - accuracy: 0.9073\n",
      "Epoch 121/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2354 - accuracy: 0.9056\n",
      "Epoch 122/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2337 - accuracy: 0.9050\n",
      "Epoch 123/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2292 - accuracy: 0.9116\n",
      "Epoch 124/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2284 - accuracy: 0.9090\n",
      "Epoch 125/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2328 - accuracy: 0.9067\n",
      "Epoch 126/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2360 - accuracy: 0.9057\n",
      "Epoch 127/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2264 - accuracy: 0.9117\n",
      "Epoch 128/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2287 - accuracy: 0.9093\n",
      "Epoch 129/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2273 - accuracy: 0.9104\n",
      "Epoch 130/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2307 - accuracy: 0.9100\n",
      "Epoch 131/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2389 - accuracy: 0.9040\n",
      "Epoch 132/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2258 - accuracy: 0.9114\n",
      "Epoch 133/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2261 - accuracy: 0.9123\n",
      "Epoch 134/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2241 - accuracy: 0.9120\n",
      "Epoch 135/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2286 - accuracy: 0.9113\n",
      "Epoch 136/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2245 - accuracy: 0.9121\n",
      "Epoch 137/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2210 - accuracy: 0.9139\n",
      "Epoch 138/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2225 - accuracy: 0.9136\n",
      "Epoch 139/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2217 - accuracy: 0.9131\n",
      "Epoch 140/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2237 - accuracy: 0.9144\n",
      "Epoch 141/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2200 - accuracy: 0.9149\n",
      "Epoch 142/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2228 - accuracy: 0.9120\n",
      "Epoch 143/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2166 - accuracy: 0.9153\n",
      "Epoch 144/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2210 - accuracy: 0.9153\n",
      "Epoch 145/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2255 - accuracy: 0.9106\n",
      "Epoch 146/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2175 - accuracy: 0.9133\n",
      "Epoch 147/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2180 - accuracy: 0.9164\n",
      "Epoch 148/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2165 - accuracy: 0.9136\n",
      "Epoch 149/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2332 - accuracy: 0.9064\n",
      "Epoch 150/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2246 - accuracy: 0.9113\n",
      "Epoch 151/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2136 - accuracy: 0.9177\n",
      "Epoch 152/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2154 - accuracy: 0.9143\n",
      "Epoch 153/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2183 - accuracy: 0.9151\n",
      "Epoch 154/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2137 - accuracy: 0.9157\n",
      "Epoch 155/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2131 - accuracy: 0.9164\n",
      "Epoch 156/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2215 - accuracy: 0.9119\n",
      "Epoch 157/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2129 - accuracy: 0.9157\n",
      "Epoch 158/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2213 - accuracy: 0.9119\n",
      "Epoch 159/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2132 - accuracy: 0.9149\n",
      "Epoch 160/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2204 - accuracy: 0.9103\n",
      "Epoch 161/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2078 - accuracy: 0.9189\n",
      "Epoch 162/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2074 - accuracy: 0.9184\n",
      "Epoch 163/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2097 - accuracy: 0.9194\n",
      "Epoch 164/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2111 - accuracy: 0.9161\n",
      "Epoch 165/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2100 - accuracy: 0.9189\n",
      "Epoch 166/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2237 - accuracy: 0.9081\n",
      "Epoch 167/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2058 - accuracy: 0.9204\n",
      "Epoch 168/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2076 - accuracy: 0.9190\n",
      "Epoch 169/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.2103 - accuracy: 0.9201\n",
      "Epoch 170/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2076 - accuracy: 0.9181\n",
      "Epoch 171/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2093 - accuracy: 0.9163\n",
      "Epoch 172/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2105 - accuracy: 0.9149\n",
      "Epoch 173/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2066 - accuracy: 0.9177\n",
      "Epoch 174/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2051 - accuracy: 0.9189\n",
      "Epoch 175/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2059 - accuracy: 0.9176\n",
      "Epoch 176/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2137 - accuracy: 0.9153\n",
      "Epoch 177/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2044 - accuracy: 0.9199\n",
      "Epoch 178/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1996 - accuracy: 0.9226\n",
      "Epoch 179/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2036 - accuracy: 0.9216\n",
      "Epoch 180/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2003 - accuracy: 0.9226\n",
      "Epoch 181/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2097 - accuracy: 0.9164\n",
      "Epoch 182/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.1999 - accuracy: 0.9229\n",
      "Epoch 183/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1987 - accuracy: 0.9226\n",
      "Epoch 184/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2020 - accuracy: 0.9191\n",
      "Epoch 185/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1972 - accuracy: 0.9233\n",
      "Epoch 186/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2039 - accuracy: 0.9196\n",
      "Epoch 187/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2056 - accuracy: 0.9203\n",
      "Epoch 188/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2021 - accuracy: 0.9159\n",
      "Epoch 189/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2010 - accuracy: 0.9217\n",
      "Epoch 190/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1951 - accuracy: 0.9261\n",
      "Epoch 191/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1951 - accuracy: 0.9220\n",
      "Epoch 192/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1959 - accuracy: 0.9249\n",
      "Epoch 193/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1946 - accuracy: 0.9254\n",
      "Epoch 194/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2004 - accuracy: 0.9211\n",
      "Epoch 195/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2001 - accuracy: 0.9234\n",
      "Epoch 196/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1958 - accuracy: 0.9220\n",
      "Epoch 197/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1968 - accuracy: 0.9200\n",
      "Epoch 198/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1909 - accuracy: 0.9261\n",
      "Epoch 199/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2047 - accuracy: 0.9193\n",
      "Epoch 200/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1961 - accuracy: 0.9259\n",
      "Epoch 201/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2022 - accuracy: 0.9177\n",
      "Epoch 202/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1974 - accuracy: 0.9224\n",
      "Epoch 203/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1929 - accuracy: 0.9261\n",
      "Epoch 204/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1980 - accuracy: 0.9220\n",
      "Epoch 205/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1899 - accuracy: 0.9256\n",
      "Epoch 206/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1899 - accuracy: 0.9286\n",
      "Epoch 207/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.1905 - accuracy: 0.9254\n",
      "Epoch 208/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2150 - accuracy: 0.9161\n",
      "Epoch 209/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1924 - accuracy: 0.9260\n",
      "Epoch 210/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1915 - accuracy: 0.9244\n",
      "Epoch 211/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1861 - accuracy: 0.9264\n",
      "Epoch 212/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1966 - accuracy: 0.9230\n",
      "Epoch 213/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1969 - accuracy: 0.9230\n",
      "Epoch 214/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1930 - accuracy: 0.9240\n",
      "Epoch 215/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1906 - accuracy: 0.9249\n",
      "Epoch 216/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1940 - accuracy: 0.9229\n",
      "Epoch 217/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1847 - accuracy: 0.9297\n",
      "Epoch 218/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1910 - accuracy: 0.9247\n",
      "Epoch 219/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1835 - accuracy: 0.9296\n",
      "Epoch 220/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1938 - accuracy: 0.9231\n",
      "Epoch 221/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1906 - accuracy: 0.9241\n",
      "Epoch 222/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1883 - accuracy: 0.9277\n",
      "Epoch 223/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.1917 - accuracy: 0.9247\n",
      "Epoch 224/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1824 - accuracy: 0.9271\n",
      "Epoch 225/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2012 - accuracy: 0.9174\n",
      "Epoch 226/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1804 - accuracy: 0.9310\n",
      "Epoch 227/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1819 - accuracy: 0.9300\n",
      "Epoch 228/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1810 - accuracy: 0.9281\n",
      "Epoch 229/280\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.1795 - accuracy: 0.9309\n",
      "Epoch 230/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1830 - accuracy: 0.9309\n",
      "Epoch 231/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1823 - accuracy: 0.9287\n",
      "Epoch 232/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1874 - accuracy: 0.9250\n",
      "Epoch 233/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1854 - accuracy: 0.9263\n",
      "Epoch 234/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1823 - accuracy: 0.9271\n",
      "Epoch 235/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1768 - accuracy: 0.9319\n",
      "Epoch 236/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1786 - accuracy: 0.9319\n",
      "Epoch 237/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1821 - accuracy: 0.9260\n",
      "Epoch 238/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1772 - accuracy: 0.9301\n",
      "Epoch 239/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1749 - accuracy: 0.9323\n",
      "Epoch 240/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1893 - accuracy: 0.9210\n",
      "Epoch 241/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1770 - accuracy: 0.9304\n",
      "Epoch 242/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1851 - accuracy: 0.9246\n",
      "Epoch 243/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1758 - accuracy: 0.9321\n",
      "Epoch 244/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1743 - accuracy: 0.9310\n",
      "Epoch 245/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1735 - accuracy: 0.9324\n",
      "Epoch 246/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1798 - accuracy: 0.9293\n",
      "Epoch 247/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1740 - accuracy: 0.9339\n",
      "Epoch 248/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1835 - accuracy: 0.9281\n",
      "Epoch 249/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1806 - accuracy: 0.9287\n",
      "Epoch 250/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1765 - accuracy: 0.9303\n",
      "Epoch 251/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1825 - accuracy: 0.9283\n",
      "Epoch 252/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1744 - accuracy: 0.9310\n",
      "Epoch 253/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1853 - accuracy: 0.9247\n",
      "Epoch 254/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1766 - accuracy: 0.9300\n",
      "Epoch 255/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1769 - accuracy: 0.9320\n",
      "Epoch 256/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1735 - accuracy: 0.9331\n",
      "Epoch 257/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1698 - accuracy: 0.9339\n",
      "Epoch 258/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1707 - accuracy: 0.9331\n",
      "Epoch 259/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1774 - accuracy: 0.9321\n",
      "Epoch 260/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1724 - accuracy: 0.9331\n",
      "Epoch 261/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1763 - accuracy: 0.9307\n",
      "Epoch 262/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1757 - accuracy: 0.9290\n",
      "Epoch 263/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1668 - accuracy: 0.9341\n",
      "Epoch 264/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1705 - accuracy: 0.9350\n",
      "Epoch 265/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1761 - accuracy: 0.9319\n",
      "Epoch 266/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1663 - accuracy: 0.9344\n",
      "Epoch 267/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1655 - accuracy: 0.9373\n",
      "Epoch 268/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1764 - accuracy: 0.9289\n",
      "Epoch 269/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1656 - accuracy: 0.9353\n",
      "Epoch 270/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1678 - accuracy: 0.9340\n",
      "Epoch 271/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1653 - accuracy: 0.9346\n",
      "Epoch 272/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1747 - accuracy: 0.9309\n",
      "Epoch 273/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1676 - accuracy: 0.9346\n",
      "Epoch 274/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1835 - accuracy: 0.9240\n",
      "Epoch 275/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1646 - accuracy: 0.9361\n",
      "Epoch 276/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1625 - accuracy: 0.9370\n",
      "Epoch 277/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1622 - accuracy: 0.9386\n",
      "Epoch 278/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1646 - accuracy: 0.9347\n",
      "Epoch 279/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1623 - accuracy: 0.9369\n",
      "Epoch 280/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1819 - accuracy: 0.9247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bcd1809c8>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam3.fit(X_train, y_train, batch_size = 500, epochs = 280, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 97us/sample - loss: 0.5976 - accuracy: 0.7977\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam3 = bankdata_model_nadam3.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.5976356331507365, 0.79766667]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam3.metrics_names)\n",
    "print(eva_results_nadam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7  Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 18us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam3 = bankdata_model_nadam3.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 11us/sample\n",
      "[[1.4803293e-01]\n",
      " [8.5298467e-04]\n",
      " [7.6656920e-01]\n",
      " ...\n",
      " [7.5238220e-02]\n",
      " [2.4021238e-04]\n",
      " [6.9066066e-01]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam3 = bankdata_model_nadam3.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2292, 1: 708}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam3 = (Y_pred_value_nadam3 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam3)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_nadam3, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.5976 - accuracy: 0.7977\n",
      "Accuracy of Model with Nadam optimizer :0.79766667\n",
      "Recall_score: 0.5815831987075929\n",
      "Precision_score: 0.5084745762711864\n",
      "F-score: 0.5425772418990203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2033,  348],\n",
       "       [ 259,  360]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.5976 - accuracy: 0.7977\n",
      "Accuracy of Model with Nadam optimizer :0.79766667\n",
      "Recall_score: 0.5815831987075929\n",
      "Precision_score: 0.5084745762711864\n",
      "F-score: 0.5425772418990203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2033,  348],\n",
       "       [ 259,  360]], dtype=int64)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam3 = confusion_matrix(y_test.values, Y_pred_value_class_nadam3)\n",
    "df_cm_nadam3 = pd.DataFrame(cm_nadam3, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam3, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 7,'Nadam',4,64,'relu',32,'LeakyReLU',16,'LeakyReLU',1,'sigmoid',0,'No layer', \n",
    "                   bankdata_model_nadam3.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam3.history.history['accuracy'][-1], eva_results_nadam3[0],eva_results_nadam3[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam3), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam3), f1_score(y_test.values,Y_pred_value_class_nadam3)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.882458</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.997857</td>\n",
       "      <td>1.067133</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.492730</td>\n",
       "      <td>0.541741</td>\n",
       "      <td>0.516074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.181892</td>\n",
       "      <td>0.924714</td>\n",
       "      <td>0.597636</td>\n",
       "      <td>0.797667</td>\n",
       "      <td>0.581583</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.542577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "5    LeakyReLU              1      sigmoid         0.005982   \n",
       "6    LeakyReLU              1      sigmoid         0.021518   \n",
       "7      sigmoid              0     No layer         0.181892   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "5             0.999429        1.882458            0.796667      0.484653   \n",
       "6             0.997857        1.067133            0.809333      0.492730   \n",
       "7             0.924714        0.597636            0.797667      0.581583   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "4         0.742931  0.573413  \n",
       "5         0.507614  0.495868  \n",
       "6         0.541741  0.516074  \n",
       "7         0.508475  0.542577  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.997857</td>\n",
       "      <td>1.067133</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.492730</td>\n",
       "      <td>0.541741</td>\n",
       "      <td>0.516074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.181892</td>\n",
       "      <td>0.924714</td>\n",
       "      <td>0.597636</td>\n",
       "      <td>0.797667</td>\n",
       "      <td>0.581583</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.542577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.882458</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "1       1          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "1             32         relu              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "1     No layer              0     No layer         0.324872   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "6    LeakyReLU              1      sigmoid         0.021518   \n",
       "7      sigmoid              0     No layer         0.181892   \n",
       "5    LeakyReLU              1      sigmoid         0.005982   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "6             0.997857        1.067133            0.809333      0.492730   \n",
       "7             0.924714        0.597636            0.797667      0.581583   \n",
       "5             0.999429        1.882458            0.796667      0.484653   \n",
       "\n",
       "   Precision score   F score  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "1         0.750000  0.581602  \n",
       "4         0.742931  0.573413  \n",
       "6         0.541741  0.516074  \n",
       "7         0.508475  0.542577  \n",
       "5         0.507614  0.495868  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df1 = results_df.sort_values(by = ['Test data-Accuracy', 'F score'], ascending = False)\n",
    "results_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8  Case 8 using Nadam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam4 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam4.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model_nadam4.add(Dense(32, activation = 'selu'))\n",
    "bankdata_model_nadam4.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam4 = optimizers.Nadam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam4.compile(optimizer = gd_optimizer_nadam4, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 1s 81us/sample - loss: 0.6267 - accuracy: 0.6504\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.4451 - accuracy: 0.8074\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4054 - accuracy: 0.8147\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3903 - accuracy: 0.8217\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3811 - accuracy: 0.8277\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3751 - accuracy: 0.8330\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3713 - accuracy: 0.8367\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3684 - accuracy: 0.8397\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3655 - accuracy: 0.8426\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3637 - accuracy: 0.8444\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3617 - accuracy: 0.8444\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3603 - accuracy: 0.8467\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3589 - accuracy: 0.8469\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3575 - accuracy: 0.8479\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.85 - 0s 4us/sample - loss: 0.3562 - accuracy: 0.8504\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3546 - accuracy: 0.8493\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3534 - accuracy: 0.8507\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3526 - accuracy: 0.8499\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3516 - accuracy: 0.8506\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3502 - accuracy: 0.8531\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3498 - accuracy: 0.8530\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3485 - accuracy: 0.8539\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3479 - accuracy: 0.8530\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3464 - accuracy: 0.8543\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3462 - accuracy: 0.8557\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3455 - accuracy: 0.8560\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3443 - accuracy: 0.8569\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3434 - accuracy: 0.8569\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3427 - accuracy: 0.8579\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3421 - accuracy: 0.8601\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3417 - accuracy: 0.8584\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3406 - accuracy: 0.8574\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3399 - accuracy: 0.8591\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3397 - accuracy: 0.8574\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3386 - accuracy: 0.8597\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3382 - accuracy: 0.8609\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3376 - accuracy: 0.8597\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3366 - accuracy: 0.8604\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3362 - accuracy: 0.8616\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3355 - accuracy: 0.8613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bd1c9c0c8>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam4.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 67us/sample - loss: 0.3577 - accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam4 = bankdata_model_nadam4.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.3577149305343628, 0.86]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam4.metrics_names)\n",
    "print(eva_results_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 16us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam4 = bankdata_model_nadam4.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 5us/sample\n",
      "[[0.23126592]\n",
      " [0.0131988 ]\n",
      " [0.81086487]\n",
      " ...\n",
      " [0.13101937]\n",
      " [0.03619527]\n",
      " [0.14271986]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam4 = bankdata_model_nadam4.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam4 = (Y_pred_value_nadam4 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.3577 - accuracy: 0.8600\n",
      "Accuracy of Model with Nadam optimizer :0.86\n",
      "Recall_score: 0.47495961227786754\n",
      "Precision_score: 0.7557840616966581\n",
      "F-score: 0.5833333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2286,   95],\n",
       "       [ 325,  294]], dtype=int64)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 32us/sample - loss: 0.3577 - accuracy: 0.8600\n",
      "Accuracy of Model with Nadam optimizer :0.86\n",
      "Recall_score: 0.47495961227786754\n",
      "Precision_score: 0.7557840616966581\n",
      "F-score: 0.5833333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2286,   95],\n",
       "       [ 325,  294]], dtype=int64)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam4 = confusion_matrix(y_test.values, Y_pred_value_class_nadam4)\n",
    "df_cm_nadam4 = pd.DataFrame(cm_nadam4, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam4, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 8,'Nadam',3,64,'elu',32,'selu',1,'sigmoid', 0, 'No layer', 0,'No layer',\n",
    "                   bankdata_model_nadam4.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam4.history.history['accuracy'][-1], eva_results_nadam4[0],eva_results_nadam4[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam4), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam4), f1_score(y_test.values,Y_pred_value_class_nadam4)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.882458</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.997857</td>\n",
       "      <td>1.067133</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.492730</td>\n",
       "      <td>0.541741</td>\n",
       "      <td>0.516074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.181892</td>\n",
       "      <td>0.924714</td>\n",
       "      <td>0.597636</td>\n",
       "      <td>0.797667</td>\n",
       "      <td>0.581583</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.542577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>selu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.335527</td>\n",
       "      <td>0.861286</td>\n",
       "      <td>0.357715</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.755784</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "8       8         Nadam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "8             32         selu              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324872   \n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "5    LeakyReLU              1      sigmoid         0.005982   \n",
       "6    LeakyReLU              1      sigmoid         0.021518   \n",
       "7      sigmoid              0     No layer         0.181892   \n",
       "8     No layer              0     No layer         0.335527   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "5             0.999429        1.882458            0.796667      0.484653   \n",
       "6             0.997857        1.067133            0.809333      0.492730   \n",
       "7             0.924714        0.597636            0.797667      0.581583   \n",
       "8             0.861286        0.357715            0.860000      0.474960   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.750000  0.581602  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "4         0.742931  0.573413  \n",
       "5         0.507614  0.495868  \n",
       "6         0.541741  0.516074  \n",
       "7         0.508475  0.542577  \n",
       "8         0.755784  0.583333  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.351584</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.864714</td>\n",
       "      <td>0.349988</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>selu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.335527</td>\n",
       "      <td>0.861286</td>\n",
       "      <td>0.357715</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.755784</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.356527</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.581602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.308470</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.360827</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.573413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.997857</td>\n",
       "      <td>1.067133</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.492730</td>\n",
       "      <td>0.541741</td>\n",
       "      <td>0.516074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.181892</td>\n",
       "      <td>0.924714</td>\n",
       "      <td>0.597636</td>\n",
       "      <td>0.797667</td>\n",
       "      <td>0.581583</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.542577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.882458</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "8       8         Nadam              3             64          elu   \n",
       "1       1          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "8             32         selu              1      sigmoid              0   \n",
       "1             32         relu              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "2     No layer              0     No layer         0.326212   \n",
       "3     No layer              0     No layer         0.325175   \n",
       "8     No layer              0     No layer         0.335527   \n",
       "1     No layer              0     No layer         0.324872   \n",
       "4     No layer              0     No layer         0.308470   \n",
       "6    LeakyReLU              1      sigmoid         0.021518   \n",
       "7      sigmoid              0     No layer         0.181892   \n",
       "5    LeakyReLU              1      sigmoid         0.005982   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "2             0.865429        0.351584            0.863333      0.444265   \n",
       "3             0.864714        0.349988            0.862000      0.463651   \n",
       "8             0.861286        0.357715            0.860000      0.474960   \n",
       "1             0.865714        0.356527            0.859000      0.474960   \n",
       "4             0.871000        0.360827            0.856667      0.466882   \n",
       "6             0.997857        1.067133            0.809333      0.492730   \n",
       "7             0.924714        0.597636            0.797667      0.581583   \n",
       "5             0.999429        1.882458            0.796667      0.484653   \n",
       "\n",
       "   Precision score   F score  \n",
       "2         0.806452  0.572917  \n",
       "3         0.777778  0.580972  \n",
       "8         0.755784  0.583333  \n",
       "1         0.750000  0.581602  \n",
       "4         0.742931  0.573413  \n",
       "6         0.541741  0.516074  \n",
       "7         0.508475  0.542577  \n",
       "5         0.507614  0.495868  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df1 = results_df.sort_values(by = ['Test data-Accuracy', 'F score'], ascending = False)\n",
    "results_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations :\n",
    "    1> The best accuracy score on test data is given by Model 2 . This model used :\n",
    "        a) Optimizer = NAdam\n",
    "        b) 3 layers\n",
    "        c) Activation functions - elu / relu and sigmoid\n",
    "    2> Management should use the Model 2 to predict persons who are most likely to exit . By concentrating efforts \n",
    "        on this group , management can try to reduce churn rate.\n",
    "    3> Higher number of layers and nodes do not necessarly result in better accuracy scores. Example Model 5 and 6 \n",
    "        used 5 layers\n",
    "    4> Models 5 & 6 were run with large number of epochs and we got Training accuracy scores greater than 0.99. However , \n",
    "        Test accuracy scores were lower. This indicates over fitting of model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
