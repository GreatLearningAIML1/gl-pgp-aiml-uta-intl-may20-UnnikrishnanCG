{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0             1    15634602   Hargrave          619    France  Female   42   \n",
       "1             2    15647311       Hill          608     Spain  Female   41   \n",
       "2             3    15619304       Onio          502    France  Female   42   \n",
       "3             4    15701354       Boni          699    France  Female   39   \n",
       "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
       "...         ...         ...        ...          ...       ...     ...  ...   \n",
       "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
       "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
       "9997       9998    15584532        Liu          709    France  Female   36   \n",
       "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
       "9999      10000    15628319     Walker          792    France  Female   28   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          2       0.00              1          1               1   \n",
       "1          1   83807.86              1          0               1   \n",
       "2          8  159660.80              3          1               0   \n",
       "3          1       0.00              2          0               0   \n",
       "4          2  125510.82              1          1               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "9995       5       0.00              2          1               0   \n",
       "9996      10   57369.61              1          1               1   \n",
       "9997       7       0.00              1          0               1   \n",
       "9998       3   75075.31              2          1               0   \n",
       "9999       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "...               ...     ...  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bankdata = pd.read_csv('bank.csv')\n",
    "bankdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dropping columns like CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Surname  CreditScore Geography  Gender  Age  Tenure    Balance  \\\n",
       "0      Hargrave          619    France  Female   42       2       0.00   \n",
       "1          Hill          608     Spain  Female   41       1   83807.86   \n",
       "2          Onio          502    France  Female   42       8  159660.80   \n",
       "3          Boni          699    France  Female   39       1       0.00   \n",
       "4      Mitchell          850     Spain  Female   43       2  125510.82   \n",
       "...         ...          ...       ...     ...  ...     ...        ...   \n",
       "9995   Obijiaku          771    France    Male   39       5       0.00   \n",
       "9996  Johnstone          516    France    Male   35      10   57369.61   \n",
       "9997        Liu          709    France  Female   36       7       0.00   \n",
       "9998  Sabbatini          772   Germany    Male   42       3   75075.31   \n",
       "9999     Walker          792    France  Female   28       4  130142.79   \n",
       "\n",
       "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0                 1          1               1        101348.88       1  \n",
       "1                 1          0               1        112542.58       0  \n",
       "2                 3          1               0        113931.57       1  \n",
       "3                 2          0               0         93826.63       0  \n",
       "4                 1          1               1         79084.10       0  \n",
       "...             ...        ...             ...              ...     ...  \n",
       "9995              2          1               0         96270.64       0  \n",
       "9996              1          1               1        101699.77       0  \n",
       "9997              1          0               1         42085.58       1  \n",
       "9998              2          1               0         92888.52       1  \n",
       "9999              1          1               0         38190.78       0  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_1 = bankdata.drop(['RowNumber','CustomerId'], axis=1)\n",
    "bankdata_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Surname          10000 non-null  object \n",
      " 1   CreditScore      10000 non-null  int64  \n",
      " 2   Geography        10000 non-null  object \n",
      " 3   Gender           10000 non-null  object \n",
      " 4   Age              10000 non-null  int64  \n",
      " 5   Tenure           10000 non-null  int64  \n",
      " 6   Balance          10000 non-null  float64\n",
      " 7   NumOfProducts    10000 non-null  int64  \n",
      " 8   HasCrCard        10000 non-null  int64  \n",
      " 9   IsActiveMember   10000 non-null  int64  \n",
      " 10  EstimatedSalary  10000 non-null  float64\n",
      " 11  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "bankdata_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Splitting Features as X_bankdata nd Target variable 'Exited' as y_bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Surname  CreditScore Geography  Gender  Age  Tenure    Balance  \\\n",
       "0  Hargrave          619    France  Female   42       2       0.00   \n",
       "1      Hill          608     Spain  Female   41       1   83807.86   \n",
       "2      Onio          502    France  Female   42       8  159660.80   \n",
       "3      Boni          699    France  Female   39       1       0.00   \n",
       "4  Mitchell          850     Spain  Female   43       2  125510.82   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0              1          1               1        101348.88  \n",
       "1              1          0               1        112542.58  \n",
       "2              3          1               0        113931.57  \n",
       "3              2          0               0         93826.63  \n",
       "4              1          1               1         79084.10  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata = bankdata_1.iloc[:, :-1]\n",
    "X_bankdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dropping 'surname' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surname is not a criteria to detarmine 'churn'. Hence dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0             619    France  Female   42       2       0.00              1   \n",
       "1             608     Spain  Female   41       1   83807.86              1   \n",
       "2             502    France  Female   42       8  159660.80              3   \n",
       "3             699    France  Female   39       1       0.00              2   \n",
       "4             850     Spain  Female   43       2  125510.82              1   \n",
       "...           ...       ...     ...  ...     ...        ...            ...   \n",
       "9995          771    France    Male   39       5       0.00              2   \n",
       "9996          516    France    Male   35      10   57369.61              1   \n",
       "9997          709    France  Female   36       7       0.00              1   \n",
       "9998          772   Germany    Male   42       3   75075.31              2   \n",
       "9999          792    France  Female   28       4  130142.79              1   \n",
       "\n",
       "      HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0             1               1        101348.88  \n",
       "1             0               1        112542.58  \n",
       "2             1               0        113931.57  \n",
       "3             0               0         93826.63  \n",
       "4             1               1         79084.10  \n",
       "...         ...             ...              ...  \n",
       "9995          1               0         96270.64  \n",
       "9996          1               1        101699.77  \n",
       "9997          0               1         42085.58  \n",
       "9998          1               0         92888.52  \n",
       "9999          1               0         38190.78  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata.drop(['Surname'], axis=1, inplace=True)\n",
    "X_bankdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding for all 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>28</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore  Age    Balance  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
       "0             619   42       0.00          1               1        101348.88   \n",
       "1             608   41   83807.86          0               1        112542.58   \n",
       "2             502   42  159660.80          1               0        113931.57   \n",
       "3             699   39       0.00          0               0         93826.63   \n",
       "4             850   43  125510.82          1               1         79084.10   \n",
       "...           ...  ...        ...        ...             ...              ...   \n",
       "9995          771   39       0.00          1               0         96270.64   \n",
       "9996          516   35   57369.61          1               1        101699.77   \n",
       "9997          709   36       0.00          0               1         42085.58   \n",
       "9998          772   42   75075.31          1               0         92888.52   \n",
       "9999          792   28  130142.79          1               0         38190.78   \n",
       "\n",
       "      NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  NumOfProducts_4  \\\n",
       "0                   1                0                0                0   \n",
       "1                   1                0                0                0   \n",
       "2                   0                0                1                0   \n",
       "3                   0                1                0                0   \n",
       "4                   1                0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "9995                0                1                0                0   \n",
       "9996                1                0                0                0   \n",
       "9997                1                0                0                0   \n",
       "9998                0                1                0                0   \n",
       "9999                1                0                0                0   \n",
       "\n",
       "      Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  Tenure_5  Tenure_6  \\\n",
       "0            0         0         1         0         0         0         0   \n",
       "1            0         1         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         1         0         0         0         0         0   \n",
       "4            0         0         1         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995         0         0         0         0         0         1         0   \n",
       "9996         0         0         0         0         0         0         0   \n",
       "9997         0         0         0         0         0         0         0   \n",
       "9998         0         0         0         1         0         0         0   \n",
       "9999         0         0         0         0         1         0         0   \n",
       "\n",
       "      Tenure_7  Tenure_8  Tenure_9  Tenure_10  Geography_France  \\\n",
       "0            0         0         0          0                 1   \n",
       "1            0         0         0          0                 0   \n",
       "2            0         1         0          0                 1   \n",
       "3            0         0         0          0                 1   \n",
       "4            0         0         0          0                 0   \n",
       "...        ...       ...       ...        ...               ...   \n",
       "9995         0         0         0          0                 1   \n",
       "9996         0         0         0          1                 1   \n",
       "9997         1         0         0          0                 1   \n",
       "9998         0         0         0          0                 0   \n",
       "9999         0         0         0          0                 1   \n",
       "\n",
       "      Geography_Germany  Geography_Spain  Gender_Female  Gender_Male  \n",
       "0                     0                0              1            0  \n",
       "1                     0                1              1            0  \n",
       "2                     0                0              1            0  \n",
       "3                     0                0              1            0  \n",
       "4                     0                1              1            0  \n",
       "...                 ...              ...            ...          ...  \n",
       "9995                  0                0              0            1  \n",
       "9996                  0                0              0            1  \n",
       "9997                  0                0              1            0  \n",
       "9998                  1                0              0            1  \n",
       "9999                  0                0              1            0  \n",
       "\n",
       "[10000 rows x 26 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotCols = ['NumOfProducts', 'Tenure','Geography','Gender']\n",
    "X_bankdata = pd.get_dummies(X_bankdata, columns = oneHotCols)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "X_bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 26)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to float since during neural network calculations, float is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bankdata = X_bankdata.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CreditScore        10000 non-null  float64\n",
      " 1   Age                10000 non-null  float64\n",
      " 2   Balance            10000 non-null  float64\n",
      " 3   HasCrCard          10000 non-null  float64\n",
      " 4   IsActiveMember     10000 non-null  float64\n",
      " 5   EstimatedSalary    10000 non-null  float64\n",
      " 6   NumOfProducts_1    10000 non-null  float64\n",
      " 7   NumOfProducts_2    10000 non-null  float64\n",
      " 8   NumOfProducts_3    10000 non-null  float64\n",
      " 9   NumOfProducts_4    10000 non-null  float64\n",
      " 10  Tenure_0           10000 non-null  float64\n",
      " 11  Tenure_1           10000 non-null  float64\n",
      " 12  Tenure_2           10000 non-null  float64\n",
      " 13  Tenure_3           10000 non-null  float64\n",
      " 14  Tenure_4           10000 non-null  float64\n",
      " 15  Tenure_5           10000 non-null  float64\n",
      " 16  Tenure_6           10000 non-null  float64\n",
      " 17  Tenure_7           10000 non-null  float64\n",
      " 18  Tenure_8           10000 non-null  float64\n",
      " 19  Tenure_9           10000 non-null  float64\n",
      " 20  Tenure_10          10000 non-null  float64\n",
      " 21  Geography_France   10000 non-null  float64\n",
      " 22  Geography_Germany  10000 non-null  float64\n",
      " 23  Geography_Spain    10000 non-null  float64\n",
      " 24  Gender_Female      10000 non-null  float64\n",
      " 25  Gender_Male        10000 non-null  float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "X_bankdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "0           619.0  42.0       0.00        1.0             1.0   \n",
       "1           608.0  41.0   83807.86        0.0             1.0   \n",
       "2           502.0  42.0  159660.80        1.0             0.0   \n",
       "3           699.0  39.0       0.00        0.0             0.0   \n",
       "4           850.0  43.0  125510.82        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "9995        771.0  39.0       0.00        1.0             0.0   \n",
       "9996        516.0  35.0   57369.61        1.0             1.0   \n",
       "9997        709.0  36.0       0.00        0.0             1.0   \n",
       "9998        772.0  42.0   75075.31        1.0             0.0   \n",
       "9999        792.0  28.0  130142.79        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "0           101348.88              1.0              0.0              0.0   \n",
       "1           112542.58              1.0              0.0              0.0   \n",
       "2           113931.57              0.0              0.0              1.0   \n",
       "3            93826.63              0.0              1.0              0.0   \n",
       "4            79084.10              1.0              0.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "9995         96270.64              0.0              1.0              0.0   \n",
       "9996        101699.77              1.0              0.0              0.0   \n",
       "9997         42085.58              1.0              0.0              0.0   \n",
       "9998         92888.52              0.0              1.0              0.0   \n",
       "9999         38190.78              1.0              0.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "0                 0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "1                 0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2                 0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3                 0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "4                 0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "9995              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9996              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9997              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9998              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "9999              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "0          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "2          0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "4          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "9995       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9996       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "9997       0.0       0.0       1.0       0.0       0.0        0.0   \n",
       "9998       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9999       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "0                  1.0                0.0              0.0            1.0   \n",
       "1                  0.0                0.0              1.0            1.0   \n",
       "2                  1.0                0.0              0.0            1.0   \n",
       "3                  1.0                0.0              0.0            1.0   \n",
       "4                  0.0                0.0              1.0            1.0   \n",
       "...                ...                ...              ...            ...   \n",
       "9995               1.0                0.0              0.0            0.0   \n",
       "9996               1.0                0.0              0.0            0.0   \n",
       "9997               1.0                0.0              0.0            1.0   \n",
       "9998               0.0                1.0              0.0            0.0   \n",
       "9999               1.0                0.0              0.0            1.0   \n",
       "\n",
       "      Gender_Male  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "...           ...  \n",
       "9995          1.0  \n",
       "9996          1.0  \n",
       "9997          0.0  \n",
       "9998          1.0  \n",
       "9999          0.0  \n",
       "\n",
       "[10000 rows x 26 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.326205</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>0.021885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.440014</td>\n",
       "      <td>0.198154</td>\n",
       "      <td>0.117344</td>\n",
       "      <td>0.216523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.536717</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>1.332987</td>\n",
       "      <td>0.240675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501496</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.108912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.063781</td>\n",
       "      <td>0.388852</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>-0.365258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.246426</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.066416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1.391870</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>-0.306363</td>\n",
       "      <td>0.027987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.604958</td>\n",
       "      <td>-0.278590</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-1.008593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.256772</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>-0.022606</td>\n",
       "      <td>-0.125224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.463698</td>\n",
       "      <td>-1.041381</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>-1.076316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore       Age   Balance  EstimatedSalary\n",
       "0       -0.326205  0.293503 -1.225786         0.021885\n",
       "1       -0.440014  0.198154  0.117344         0.216523\n",
       "2       -1.536717  0.293503  1.332987         0.240675\n",
       "3        0.501496  0.007456 -1.225786        -0.108912\n",
       "4        2.063781  0.388852  0.785689        -0.365258\n",
       "...           ...       ...       ...              ...\n",
       "9995     1.246426  0.007456 -1.225786        -0.066416\n",
       "9996    -1.391870 -0.373939 -0.306363         0.027987\n",
       "9997     0.604958 -0.278590 -1.225786        -1.008593\n",
       "9998     1.256772  0.293503 -0.022606        -0.125224\n",
       "9999     1.463698 -1.041381  0.859922        -1.076316\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_bankdata_norm = X_bankdata.normalize(X_bankdata)\n",
    "X_bankdata_norm = ( X_bankdata[['CreditScore','Age','Balance','EstimatedSalary']] - X_bankdata[['CreditScore','Age','Balance','EstimatedSalary']].mean() ) / (X_bankdata[['CreditScore','Age','Balance','EstimatedSalary']].std())\n",
    "\n",
    "X_bankdata_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_bankdata_norm.rename(columns={\"CreditScore\":\"CreditScore_norm\", \"Age\": \"Age_norm\", \"Balance\": \"Balance_norm\", \"EstimatedSalary\": \"EstimatedSalary_norm\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Balance</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.326205</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>619.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.440014</td>\n",
       "      <td>0.198154</td>\n",
       "      <td>0.117344</td>\n",
       "      <td>0.216523</td>\n",
       "      <td>608.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.536717</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>1.332987</td>\n",
       "      <td>0.240675</td>\n",
       "      <td>502.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501496</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>699.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.063781</td>\n",
       "      <td>0.388852</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>-0.365258</td>\n",
       "      <td>850.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.246426</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.066416</td>\n",
       "      <td>771.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1.391870</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>-0.306363</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>516.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.604958</td>\n",
       "      <td>-0.278590</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-1.008593</td>\n",
       "      <td>709.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.256772</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>-0.022606</td>\n",
       "      <td>-0.125224</td>\n",
       "      <td>772.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.463698</td>\n",
       "      <td>-1.041381</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>-1.076316</td>\n",
       "      <td>792.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore_norm  Age_norm  Balance_norm  EstimatedSalary_norm  \\\n",
       "0            -0.326205  0.293503     -1.225786              0.021885   \n",
       "1            -0.440014  0.198154      0.117344              0.216523   \n",
       "2            -1.536717  0.293503      1.332987              0.240675   \n",
       "3             0.501496  0.007456     -1.225786             -0.108912   \n",
       "4             2.063781  0.388852      0.785689             -0.365258   \n",
       "...                ...       ...           ...                   ...   \n",
       "9995          1.246426  0.007456     -1.225786             -0.066416   \n",
       "9996         -1.391870 -0.373939     -0.306363              0.027987   \n",
       "9997          0.604958 -0.278590     -1.225786             -1.008593   \n",
       "9998          1.256772  0.293503     -0.022606             -0.125224   \n",
       "9999          1.463698 -1.041381      0.859922             -1.076316   \n",
       "\n",
       "      CreditScore   Age    Balance  HasCrCard  IsActiveMember  \\\n",
       "0           619.0  42.0       0.00        1.0             1.0   \n",
       "1           608.0  41.0   83807.86        0.0             1.0   \n",
       "2           502.0  42.0  159660.80        1.0             0.0   \n",
       "3           699.0  39.0       0.00        0.0             0.0   \n",
       "4           850.0  43.0  125510.82        1.0             1.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "9995        771.0  39.0       0.00        1.0             0.0   \n",
       "9996        516.0  35.0   57369.61        1.0             1.0   \n",
       "9997        709.0  36.0       0.00        0.0             1.0   \n",
       "9998        772.0  42.0   75075.31        1.0             0.0   \n",
       "9999        792.0  28.0  130142.79        1.0             0.0   \n",
       "\n",
       "      EstimatedSalary  NumOfProducts_1  NumOfProducts_2  NumOfProducts_3  \\\n",
       "0           101348.88              1.0              0.0              0.0   \n",
       "1           112542.58              1.0              0.0              0.0   \n",
       "2           113931.57              0.0              0.0              1.0   \n",
       "3            93826.63              0.0              1.0              0.0   \n",
       "4            79084.10              1.0              0.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "9995         96270.64              0.0              1.0              0.0   \n",
       "9996        101699.77              1.0              0.0              0.0   \n",
       "9997         42085.58              1.0              0.0              0.0   \n",
       "9998         92888.52              0.0              1.0              0.0   \n",
       "9999         38190.78              1.0              0.0              0.0   \n",
       "\n",
       "      NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  Tenure_3  Tenure_4  \\\n",
       "0                 0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "1                 0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2                 0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3                 0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "4                 0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "9995              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9996              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9997              0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9998              0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "9999              0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "\n",
       "      Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  Tenure_10  \\\n",
       "0          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "2          0.0       0.0       0.0       1.0       0.0        0.0   \n",
       "3          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "4          0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "9995       1.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9996       0.0       0.0       0.0       0.0       0.0        1.0   \n",
       "9997       0.0       0.0       1.0       0.0       0.0        0.0   \n",
       "9998       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "9999       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "      Geography_France  Geography_Germany  Geography_Spain  Gender_Female  \\\n",
       "0                  1.0                0.0              0.0            1.0   \n",
       "1                  0.0                0.0              1.0            1.0   \n",
       "2                  1.0                0.0              0.0            1.0   \n",
       "3                  1.0                0.0              0.0            1.0   \n",
       "4                  0.0                0.0              1.0            1.0   \n",
       "...                ...                ...              ...            ...   \n",
       "9995               1.0                0.0              0.0            0.0   \n",
       "9996               1.0                0.0              0.0            0.0   \n",
       "9997               1.0                0.0              0.0            1.0   \n",
       "9998               0.0                1.0              0.0            0.0   \n",
       "9999               1.0                0.0              0.0            1.0   \n",
       "\n",
       "      Gender_Male  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "...           ...  \n",
       "9995          1.0  \n",
       "9996          1.0  \n",
       "9997          0.0  \n",
       "9998          1.0  \n",
       "9999          0.0  \n",
       "\n",
       "[10000 rows x 30 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata = pd.concat([X_bankdata_norm,X_bankdata], axis = 1)\n",
    "X_bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CreditScore_norm', 'Age_norm', 'Balance_norm', 'EstimatedSalary_norm',\n",
       "       'CreditScore', 'Age', 'Balance', 'HasCrCard', 'IsActiveMember',\n",
       "       'EstimatedSalary', 'NumOfProducts_1', 'NumOfProducts_2',\n",
       "       'NumOfProducts_3', 'NumOfProducts_4', 'Tenure_0', 'Tenure_1',\n",
       "       'Tenure_2', 'Tenure_3', 'Tenure_4', 'Tenure_5', 'Tenure_6', 'Tenure_7',\n",
       "       'Tenure_8', 'Tenure_9', 'Tenure_10', 'Geography_France',\n",
       "       'Geography_Germany', 'Geography_Spain', 'Gender_Female', 'Gender_Male'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.326205</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.440014</td>\n",
       "      <td>0.198154</td>\n",
       "      <td>0.117344</td>\n",
       "      <td>0.216523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.536717</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>1.332987</td>\n",
       "      <td>0.240675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501496</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.063781</td>\n",
       "      <td>0.388852</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>-0.365258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.246426</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.066416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1.391870</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>-0.306363</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.604958</td>\n",
       "      <td>-0.278590</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-1.008593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.256772</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>-0.022606</td>\n",
       "      <td>-0.125224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.463698</td>\n",
       "      <td>-1.041381</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>-1.076316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore_norm  Age_norm  Balance_norm  EstimatedSalary_norm  \\\n",
       "0            -0.326205  0.293503     -1.225786              0.021885   \n",
       "1            -0.440014  0.198154      0.117344              0.216523   \n",
       "2            -1.536717  0.293503      1.332987              0.240675   \n",
       "3             0.501496  0.007456     -1.225786             -0.108912   \n",
       "4             2.063781  0.388852      0.785689             -0.365258   \n",
       "...                ...       ...           ...                   ...   \n",
       "9995          1.246426  0.007456     -1.225786             -0.066416   \n",
       "9996         -1.391870 -0.373939     -0.306363              0.027987   \n",
       "9997          0.604958 -0.278590     -1.225786             -1.008593   \n",
       "9998          1.256772  0.293503     -0.022606             -0.125224   \n",
       "9999          1.463698 -1.041381      0.859922             -1.076316   \n",
       "\n",
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "0           1.0             1.0              1.0              0.0   \n",
       "1           0.0             1.0              1.0              0.0   \n",
       "2           1.0             0.0              0.0              0.0   \n",
       "3           0.0             0.0              0.0              1.0   \n",
       "4           1.0             1.0              1.0              0.0   \n",
       "...         ...             ...              ...              ...   \n",
       "9995        1.0             0.0              0.0              1.0   \n",
       "9996        1.0             1.0              1.0              0.0   \n",
       "9997        0.0             1.0              1.0              0.0   \n",
       "9998        1.0             0.0              0.0              1.0   \n",
       "9999        1.0             0.0              1.0              0.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "0                 0.0              0.0       0.0       0.0       1.0   \n",
       "1                 0.0              0.0       0.0       1.0       0.0   \n",
       "2                 1.0              0.0       0.0       0.0       0.0   \n",
       "3                 0.0              0.0       0.0       1.0       0.0   \n",
       "4                 0.0              0.0       0.0       0.0       1.0   \n",
       "...               ...              ...       ...       ...       ...   \n",
       "9995              0.0              0.0       0.0       0.0       0.0   \n",
       "9996              0.0              0.0       0.0       0.0       0.0   \n",
       "9997              0.0              0.0       0.0       0.0       0.0   \n",
       "9998              0.0              0.0       0.0       0.0       0.0   \n",
       "9999              0.0              0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "0          0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1          0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2          0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3          0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "9996       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9997       0.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "9998       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9999       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "0           0.0               1.0                0.0              0.0   \n",
       "1           0.0               0.0                0.0              1.0   \n",
       "2           0.0               1.0                0.0              0.0   \n",
       "3           0.0               1.0                0.0              0.0   \n",
       "4           0.0               0.0                0.0              1.0   \n",
       "...         ...               ...                ...              ...   \n",
       "9995        0.0               1.0                0.0              0.0   \n",
       "9996        1.0               1.0                0.0              0.0   \n",
       "9997        0.0               1.0                0.0              0.0   \n",
       "9998        0.0               0.0                1.0              0.0   \n",
       "9999        0.0               1.0                0.0              0.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  \n",
       "0               1.0          0.0  \n",
       "1               1.0          0.0  \n",
       "2               1.0          0.0  \n",
       "3               1.0          0.0  \n",
       "4               1.0          0.0  \n",
       "...             ...          ...  \n",
       "9995            0.0          1.0  \n",
       "9996            0.0          1.0  \n",
       "9997            1.0          0.0  \n",
       "9998            0.0          1.0  \n",
       "9999            1.0          0.0  \n",
       "\n",
       "[10000 rows x 26 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bankdata.drop(X_bankdata[cols] , axis = 1 , inplace = True)\n",
    "X_bankdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bankdata = bankdata_1.iloc[:,-1]\n",
    "y_bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bankdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Dividing into Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bankdata, y_bankdata, test_size = 0.3, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>0.604958</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.762924</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>1.515429</td>\n",
       "      <td>-0.755334</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>0.309968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>-1.867798</td>\n",
       "      <td>-0.850683</td>\n",
       "      <td>0.395127</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>1.908587</td>\n",
       "      <td>-0.946032</td>\n",
       "      <td>0.864368</td>\n",
       "      <td>0.107568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>-0.253781</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.894301</td>\n",
       "      <td>-0.845451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore_norm  Age_norm  Balance_norm  EstimatedSalary_norm  \\\n",
       "5795          0.604958  0.007456     -1.225786             -0.762924   \n",
       "1490          1.515429 -0.755334     -1.225786              0.309968   \n",
       "3807         -1.867798 -0.850683      0.395127             -0.855211   \n",
       "3042          1.908587 -0.946032      0.864368              0.107568   \n",
       "4064         -0.253781  0.007456      0.894301             -0.845451   \n",
       "\n",
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "5795        1.0             0.0              0.0              1.0   \n",
       "1490        1.0             0.0              0.0              1.0   \n",
       "3807        1.0             1.0              1.0              0.0   \n",
       "3042        0.0             0.0              0.0              1.0   \n",
       "4064        1.0             1.0              0.0              0.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "5795              0.0              0.0       0.0       0.0       0.0   \n",
       "1490              0.0              0.0       0.0       0.0       0.0   \n",
       "3807              0.0              0.0       0.0       0.0       0.0   \n",
       "3042              0.0              0.0       0.0       0.0       0.0   \n",
       "4064              1.0              0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "5795       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "1490       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3807       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "5795        0.0               1.0                0.0              0.0   \n",
       "1490        0.0               0.0                0.0              1.0   \n",
       "3807        0.0               1.0                0.0              0.0   \n",
       "3042        1.0               0.0                1.0              0.0   \n",
       "4064        1.0               0.0                1.0              0.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  \n",
       "5795            0.0          1.0  \n",
       "1490            1.0          0.0  \n",
       "3807            0.0          1.0  \n",
       "3042            1.0          0.0  \n",
       "4064            0.0          1.0  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 26)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7000 entries, 5795 to 5994\n",
      "Data columns (total 26 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   CreditScore_norm      7000 non-null   float64\n",
      " 1   Age_norm              7000 non-null   float64\n",
      " 2   Balance_norm          7000 non-null   float64\n",
      " 3   EstimatedSalary_norm  7000 non-null   float64\n",
      " 4   HasCrCard             7000 non-null   float64\n",
      " 5   IsActiveMember        7000 non-null   float64\n",
      " 6   NumOfProducts_1       7000 non-null   float64\n",
      " 7   NumOfProducts_2       7000 non-null   float64\n",
      " 8   NumOfProducts_3       7000 non-null   float64\n",
      " 9   NumOfProducts_4       7000 non-null   float64\n",
      " 10  Tenure_0              7000 non-null   float64\n",
      " 11  Tenure_1              7000 non-null   float64\n",
      " 12  Tenure_2              7000 non-null   float64\n",
      " 13  Tenure_3              7000 non-null   float64\n",
      " 14  Tenure_4              7000 non-null   float64\n",
      " 15  Tenure_5              7000 non-null   float64\n",
      " 16  Tenure_6              7000 non-null   float64\n",
      " 17  Tenure_7              7000 non-null   float64\n",
      " 18  Tenure_8              7000 non-null   float64\n",
      " 19  Tenure_9              7000 non-null   float64\n",
      " 20  Tenure_10             7000 non-null   float64\n",
      " 21  Geography_France      7000 non-null   float64\n",
      " 22  Geography_Germany     7000 non-null   float64\n",
      " 23  Geography_Spain       7000 non-null   float64\n",
      " 24  Gender_Female         7000 non-null   float64\n",
      " 25  Gender_Male           7000 non-null   float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>0.553227</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.561548</td>\n",
       "      <td>1.728288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>-0.098587</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.120696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>-0.988366</td>\n",
       "      <td>0.674898</td>\n",
       "      <td>0.703669</td>\n",
       "      <td>1.349834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>-1.826413</td>\n",
       "      <td>-0.469288</td>\n",
       "      <td>1.599834</td>\n",
       "      <td>1.045427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-0.419321</td>\n",
       "      <td>0.102805</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.658274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore_norm  Age_norm  Balance_norm  EstimatedSalary_norm  \\\n",
       "5876          0.553227  0.007456      0.561548              1.728288   \n",
       "6555         -0.098587 -0.373939     -1.225786             -0.120696   \n",
       "1448         -0.988366  0.674898      0.703669              1.349834   \n",
       "3351         -1.826413 -0.469288      1.599834              1.045427   \n",
       "231          -0.419321  0.102805     -1.225786             -0.658274   \n",
       "\n",
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "5876        1.0             0.0              1.0              0.0   \n",
       "6555        1.0             0.0              0.0              1.0   \n",
       "1448        1.0             0.0              1.0              0.0   \n",
       "3351        1.0             0.0              1.0              0.0   \n",
       "231         1.0             0.0              0.0              1.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "5876              0.0              0.0       0.0       0.0       1.0   \n",
       "6555              0.0              0.0       0.0       0.0       0.0   \n",
       "1448              0.0              0.0       0.0       0.0       0.0   \n",
       "3351              0.0              0.0       0.0       0.0       0.0   \n",
       "231               0.0              0.0       1.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "6555       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "1448       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3351       0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "5876        0.0               1.0                0.0              0.0   \n",
       "6555        0.0               1.0                0.0              0.0   \n",
       "1448        0.0               0.0                1.0              0.0   \n",
       "3351        0.0               0.0                1.0              0.0   \n",
       "231         0.0               1.0                0.0              0.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  \n",
       "5876            0.0          1.0  \n",
       "6555            0.0          1.0  \n",
       "1448            1.0          0.0  \n",
       "3351            1.0          0.0  \n",
       "231             0.0          1.0  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 26)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3000 entries, 5876 to 676\n",
      "Data columns (total 26 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   CreditScore_norm      3000 non-null   float64\n",
      " 1   Age_norm              3000 non-null   float64\n",
      " 2   Balance_norm          3000 non-null   float64\n",
      " 3   EstimatedSalary_norm  3000 non-null   float64\n",
      " 4   HasCrCard             3000 non-null   float64\n",
      " 5   IsActiveMember        3000 non-null   float64\n",
      " 6   NumOfProducts_1       3000 non-null   float64\n",
      " 7   NumOfProducts_2       3000 non-null   float64\n",
      " 8   NumOfProducts_3       3000 non-null   float64\n",
      " 9   NumOfProducts_4       3000 non-null   float64\n",
      " 10  Tenure_0              3000 non-null   float64\n",
      " 11  Tenure_1              3000 non-null   float64\n",
      " 12  Tenure_2              3000 non-null   float64\n",
      " 13  Tenure_3              3000 non-null   float64\n",
      " 14  Tenure_4              3000 non-null   float64\n",
      " 15  Tenure_5              3000 non-null   float64\n",
      " 16  Tenure_6              3000 non-null   float64\n",
      " 17  Tenure_7              3000 non-null   float64\n",
      " 18  Tenure_8              3000 non-null   float64\n",
      " 19  Tenure_9              3000 non-null   float64\n",
      " 20  Tenure_10             3000 non-null   float64\n",
      " 21  Geography_France      3000 non-null   float64\n",
      " 22  Geography_Germany     3000 non-null   float64\n",
      " 23  Geography_Spain       3000 non-null   float64\n",
      " 24  Gender_Female         3000 non-null   float64\n",
      " 25  Gender_Male           3000 non-null   float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 632.8 KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5795    0\n",
       "1490    0\n",
       "3807    0\n",
       "3042    0\n",
       "4064    1\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5876    0\n",
       "6555    0\n",
       "1448    1\n",
       "3351    0\n",
       "231     0\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of True 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2381, 1: 619}\n"
     ]
    }
   ],
   "source": [
    "# Print predicted 1 and 0's \n",
    "unique1, counts1 = np.unique(y_test, return_counts = True)\n",
    "print('\\nThe number of True 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique1, counts1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using the  NormalizedTrain and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>0.604958</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.762924</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>1.515429</td>\n",
       "      <td>-0.755334</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>0.309968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>-1.867798</td>\n",
       "      <td>-0.850683</td>\n",
       "      <td>0.395127</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>1.908587</td>\n",
       "      <td>-0.946032</td>\n",
       "      <td>0.864368</td>\n",
       "      <td>0.107568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>-0.253781</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.894301</td>\n",
       "      <td>-0.845451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>0.263532</td>\n",
       "      <td>-0.850683</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>1.373251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>1.318850</td>\n",
       "      <td>-1.422776</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>1.090566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>0.284224</td>\n",
       "      <td>1.533037</td>\n",
       "      <td>0.851975</td>\n",
       "      <td>1.461209</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>-0.512438</td>\n",
       "      <td>0.198154</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>1.052274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>-0.729709</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>0.958301</td>\n",
       "      <td>-1.309166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore_norm  Age_norm  Balance_norm  EstimatedSalary_norm  \\\n",
       "5795          0.604958  0.007456     -1.225786             -0.762924   \n",
       "1490          1.515429 -0.755334     -1.225786              0.309968   \n",
       "3807         -1.867798 -0.850683      0.395127             -0.855211   \n",
       "3042          1.908587 -0.946032      0.864368              0.107568   \n",
       "4064         -0.253781  0.007456      0.894301             -0.845451   \n",
       "...                ...       ...           ...                   ...   \n",
       "6400          0.263532 -0.850683     -1.225786              1.373251   \n",
       "9160          1.318850 -1.422776     -1.225786              1.090566   \n",
       "9859          0.284224  1.533037      0.851975              1.461209   \n",
       "1688         -0.512438  0.198154     -1.225786              1.052274   \n",
       "5994         -0.729709 -0.373939      0.958301             -1.309166   \n",
       "\n",
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "5795        1.0             0.0              0.0              1.0   \n",
       "1490        1.0             0.0              0.0              1.0   \n",
       "3807        1.0             1.0              1.0              0.0   \n",
       "3042        0.0             0.0              0.0              1.0   \n",
       "4064        1.0             1.0              0.0              0.0   \n",
       "...         ...             ...              ...              ...   \n",
       "6400        0.0             0.0              0.0              1.0   \n",
       "9160        1.0             1.0              0.0              1.0   \n",
       "9859        1.0             1.0              1.0              0.0   \n",
       "1688        0.0             1.0              0.0              1.0   \n",
       "5994        1.0             1.0              0.0              1.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "5795              0.0              0.0       0.0       0.0       0.0   \n",
       "1490              0.0              0.0       0.0       0.0       0.0   \n",
       "3807              0.0              0.0       0.0       0.0       0.0   \n",
       "3042              0.0              0.0       0.0       0.0       0.0   \n",
       "4064              1.0              0.0       0.0       0.0       0.0   \n",
       "...               ...              ...       ...       ...       ...   \n",
       "6400              0.0              0.0       0.0       0.0       0.0   \n",
       "9160              0.0              0.0       0.0       0.0       0.0   \n",
       "9859              0.0              0.0       0.0       0.0       0.0   \n",
       "1688              0.0              0.0       0.0       1.0       0.0   \n",
       "5994              0.0              0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "5795       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "1490       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "3807       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3042       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4064       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6400       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "9160       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "9859       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1688       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5994       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "5795        0.0               1.0                0.0              0.0   \n",
       "1490        0.0               0.0                0.0              1.0   \n",
       "3807        0.0               1.0                0.0              0.0   \n",
       "3042        1.0               0.0                1.0              0.0   \n",
       "4064        1.0               0.0                1.0              0.0   \n",
       "...         ...               ...                ...              ...   \n",
       "6400        0.0               0.0                0.0              1.0   \n",
       "9160        0.0               1.0                0.0              0.0   \n",
       "9859        0.0               0.0                1.0              0.0   \n",
       "1688        0.0               1.0                0.0              0.0   \n",
       "5994        1.0               0.0                1.0              0.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  \n",
       "5795            0.0          1.0  \n",
       "1490            1.0          0.0  \n",
       "3807            0.0          1.0  \n",
       "3042            1.0          0.0  \n",
       "4064            0.0          1.0  \n",
       "...             ...          ...  \n",
       "6400            1.0          0.0  \n",
       "9160            0.0          1.0  \n",
       "9859            0.0          1.0  \n",
       "1688            1.0          0.0  \n",
       "5994            0.0          1.0  \n",
       "\n",
       "[7000 rows x 26 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Balance_norm</th>\n",
       "      <th>EstimatedSalary_norm</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts_1</th>\n",
       "      <th>NumOfProducts_2</th>\n",
       "      <th>NumOfProducts_3</th>\n",
       "      <th>NumOfProducts_4</th>\n",
       "      <th>Tenure_0</th>\n",
       "      <th>Tenure_1</th>\n",
       "      <th>Tenure_2</th>\n",
       "      <th>Tenure_3</th>\n",
       "      <th>Tenure_4</th>\n",
       "      <th>Tenure_5</th>\n",
       "      <th>Tenure_6</th>\n",
       "      <th>Tenure_7</th>\n",
       "      <th>Tenure_8</th>\n",
       "      <th>Tenure_9</th>\n",
       "      <th>Tenure_10</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>0.553227</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.561548</td>\n",
       "      <td>1.728288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>-0.098587</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.120696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>-0.988366</td>\n",
       "      <td>0.674898</td>\n",
       "      <td>0.703669</td>\n",
       "      <td>1.349834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>-1.826413</td>\n",
       "      <td>-0.469288</td>\n",
       "      <td>1.599834</td>\n",
       "      <td>1.045427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-0.419321</td>\n",
       "      <td>0.102805</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.658274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-1.971260</td>\n",
       "      <td>0.674898</td>\n",
       "      <td>0.818529</td>\n",
       "      <td>0.467085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>1.298157</td>\n",
       "      <td>-0.850683</td>\n",
       "      <td>-1.225786</td>\n",
       "      <td>-0.629127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.822230</td>\n",
       "      <td>-0.373939</td>\n",
       "      <td>1.265832</td>\n",
       "      <td>-0.806322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5657</th>\n",
       "      <td>-1.598795</td>\n",
       "      <td>-0.850683</td>\n",
       "      <td>0.232022</td>\n",
       "      <td>-1.256957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>-0.595208</td>\n",
       "      <td>-0.087893</td>\n",
       "      <td>0.196331</td>\n",
       "      <td>-0.575029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore_norm  Age_norm  Balance_norm  EstimatedSalary_norm  \\\n",
       "5876          0.553227  0.007456      0.561548              1.728288   \n",
       "6555         -0.098587 -0.373939     -1.225786             -0.120696   \n",
       "1448         -0.988366  0.674898      0.703669              1.349834   \n",
       "3351         -1.826413 -0.469288      1.599834              1.045427   \n",
       "231          -0.419321  0.102805     -1.225786             -0.658274   \n",
       "...                ...       ...           ...                   ...   \n",
       "749          -1.971260  0.674898      0.818529              0.467085   \n",
       "5373          1.298157 -0.850683     -1.225786             -0.629127   \n",
       "485           0.822230 -0.373939      1.265832             -0.806322   \n",
       "5657         -1.598795 -0.850683      0.232022             -1.256957   \n",
       "676          -0.595208 -0.087893      0.196331             -0.575029   \n",
       "\n",
       "      HasCrCard  IsActiveMember  NumOfProducts_1  NumOfProducts_2  \\\n",
       "5876        1.0             0.0              1.0              0.0   \n",
       "6555        1.0             0.0              0.0              1.0   \n",
       "1448        1.0             0.0              1.0              0.0   \n",
       "3351        1.0             0.0              1.0              0.0   \n",
       "231         1.0             0.0              0.0              1.0   \n",
       "...         ...             ...              ...              ...   \n",
       "749         1.0             1.0              0.0              1.0   \n",
       "5373        0.0             1.0              0.0              1.0   \n",
       "485         1.0             1.0              1.0              0.0   \n",
       "5657        0.0             1.0              1.0              0.0   \n",
       "676         1.0             0.0              0.0              1.0   \n",
       "\n",
       "      NumOfProducts_3  NumOfProducts_4  Tenure_0  Tenure_1  Tenure_2  \\\n",
       "5876              0.0              0.0       0.0       0.0       1.0   \n",
       "6555              0.0              0.0       0.0       0.0       0.0   \n",
       "1448              0.0              0.0       0.0       0.0       0.0   \n",
       "3351              0.0              0.0       0.0       0.0       0.0   \n",
       "231               0.0              0.0       1.0       0.0       0.0   \n",
       "...               ...              ...       ...       ...       ...   \n",
       "749               0.0              0.0       0.0       0.0       0.0   \n",
       "5373              0.0              0.0       0.0       0.0       0.0   \n",
       "485               0.0              0.0       1.0       0.0       0.0   \n",
       "5657              0.0              0.0       1.0       0.0       0.0   \n",
       "676               0.0              0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_3  Tenure_4  Tenure_5  Tenure_6  Tenure_7  Tenure_8  Tenure_9  \\\n",
       "5876       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "6555       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "1448       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3351       0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "231        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "749        0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5373       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "485        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5657       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "676        0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      Tenure_10  Geography_France  Geography_Germany  Geography_Spain  \\\n",
       "5876        0.0               1.0                0.0              0.0   \n",
       "6555        0.0               1.0                0.0              0.0   \n",
       "1448        0.0               0.0                1.0              0.0   \n",
       "3351        0.0               0.0                1.0              0.0   \n",
       "231         0.0               1.0                0.0              0.0   \n",
       "...         ...               ...                ...              ...   \n",
       "749         0.0               0.0                1.0              0.0   \n",
       "5373        0.0               0.0                0.0              1.0   \n",
       "485         0.0               1.0                0.0              0.0   \n",
       "5657        0.0               1.0                0.0              0.0   \n",
       "676         0.0               0.0                0.0              1.0   \n",
       "\n",
       "      Gender_Female  Gender_Male  \n",
       "5876            0.0          1.0  \n",
       "6555            0.0          1.0  \n",
       "1448            1.0          0.0  \n",
       "3351            1.0          0.0  \n",
       "231             0.0          1.0  \n",
       "...             ...          ...  \n",
       "749             0.0          1.0  \n",
       "5373            0.0          1.0  \n",
       "485             1.0          0.0  \n",
       "5657            0.0          1.0  \n",
       "676             1.0          0.0  \n",
       "\n",
       "[3000 rows x 26 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Initializing & Building multiple models and implementaing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Case 1- Compiling model . Will use the gradient descent optimization , with 'binary_crossentropy' loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model.add(Dense(32, activation = 'relu'))\n",
    "bankdata_model.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer = optimizers.Adam(lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model.compile(optimizer = gd_optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s 463us/sample - loss: 0.5619 - accuracy: 0.7759\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.4531 - accuracy: 0.7986\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4203 - accuracy: 0.8043\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.4009 - accuracy: 0.8184\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3892 - accuracy: 0.8227\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.3811 - accuracy: 0.8284\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3748 - accuracy: 0.8341\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3703 - accuracy: 0.8387\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3663 - accuracy: 0.8410\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3628 - accuracy: 0.8436\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3603 - accuracy: 0.8464\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3577 - accuracy: 0.8483\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3550 - accuracy: 0.8517\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3528 - accuracy: 0.8514\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3511 - accuracy: 0.8529\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3491 - accuracy: 0.8543\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3473 - accuracy: 0.8554\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3459 - accuracy: 0.8560\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3444 - accuracy: 0.8567\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3435 - accuracy: 0.8580\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3418 - accuracy: 0.8611\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3403 - accuracy: 0.8587\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3387 - accuracy: 0.8590\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3379 - accuracy: 0.8601\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3372 - accuracy: 0.8620\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3358 - accuracy: 0.8610\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3344 - accuracy: 0.8623\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3334 - accuracy: 0.8629\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3324 - accuracy: 0.8630\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3316 - accuracy: 0.8633\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3309 - accuracy: 0.8644\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3298 - accuracy: 0.8631\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3294 - accuracy: 0.8647\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3283 - accuracy: 0.8656\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3276 - accuracy: 0.8636\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3267 - accuracy: 0.8646\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3260 - accuracy: 0.8644\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3255 - accuracy: 0.8651\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3243 - accuracy: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2862d1c9088>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loss is : [0.5618745642048972, 0.4530858908380781, 0.42027885147503447, 0.40089368394442965, 0.38921252318790983, 0.3811006716319493, 0.374801550592695, 0.37025623662131174, 0.36628484087330954, 0.36281521192618776, 0.3603008006300245, 0.3576554570879255, 0.3550064606325967, 0.35279819795063566, 0.351118483713695, 0.34908572903701235, 0.34729265528065817, 0.3459270851952689, 0.3443725194249834, 0.3435491642781666, 0.3417921875204359, 0.3402589112520218, 0.33873256189482553, 0.3379341619355338, 0.337166143315179, 0.3357935015644346, 0.33436048243727, 0.3333751644407, 0.33239981532096863, 0.3315837596143995, 0.33094710963112967, 0.32981944722788675, 0.32938454832349506, 0.32832723430224825, 0.3276375894035612, 0.3266885642494474, 0.32649547713143484, 0.326016475047384, 0.3254755714109966, 0.3243172935077122]\n"
     ]
    }
   ],
   "source": [
    "print('Train data loss is :', bankdata_model.history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loss is : 0.3243172935077122\n"
     ]
    }
   ],
   "source": [
    "print('Train data loss is :', bankdata_model.history.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data accuracy is : 0.86542857\n"
     ]
    }
   ],
   "source": [
    "print('Train data accuracy is :', bankdata_model.history.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 123us/sample - loss: 0.3552 - accuracy: 0.8597\n"
     ]
    }
   ],
   "source": [
    "eva_results = bankdata_model.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.3551533633073171, 0.85966665]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model.metrics_names)\n",
    "print(eva_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data loss is : 0.3551533633073171\n"
     ]
    }
   ],
   "source": [
    "print('Test data loss is :', eva_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data accuracy is : 0.85966665\n"
     ]
    }
   ],
   "source": [
    "print('Test data accuracy is :', eva_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 18us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 10us/sample\n",
      "[[0.20951578]\n",
      " [0.0121375 ]\n",
      " [0.6606856 ]\n",
      " ...\n",
      " [0.1487247 ]\n",
      " [0.05378688]\n",
      " [0.1836236 ]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value = bankdata_model.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class = (Y_pred_value > 0.5).astype(int)\n",
    "print(Y_pred_value_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 32us/sample - loss: 0.3552 - accuracy: 0.8597\n",
      "Accuracy of Model with Adam optimizer Case1:0.85966665\n",
      "Recall_score: 0.44749596122778673\n",
      "Precision_score: 0.7780898876404494\n",
      "F-score: 0.5682051282051283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2302,   79],\n",
       "       [ 342,  277]], dtype=int64)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer Case1:'+ str(bankdata_model.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class)))\n",
    "confusion_matrix(y_test.values, Y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.3552 - accuracy: 0.8597\n",
      "Accuracy of Model with Adam optimizer Case 1 :0.85966665\n",
      "Recall_score: 0.44749596122778673\n",
      "Precision_score: 0.7780898876404494\n",
      "F-score: 0.5682051282051283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2302,   79],\n",
       "       [ 342,  277]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer Case 1 :'+ str(bankdata_model.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam = confusion_matrix(y_test.values, Y_pred_value_class)\n",
    "df_cm_adam = pd.DataFrame(cm_adam, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a results df to hold results of diff runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([[1,'Adam',3,64,'elu',32,'relu',1,'sigmoid', 0, 'No layer',0,'No layer',0,0, 0,0,0,0,0]], \n",
    "                          columns = ['Case X','Model details','Num of layers','Layer 1 nodes','Layer1 Act F','Layer 2 nodes','Layer2 Act F',\n",
    "                            'Layer 3 nodes','Layer3 Act F', 'Layer 4 nodes', 'Layer4 Act F','Layer 5 nodes', 'Layer5 Act F',\n",
    "                                     'Train data-loss','Train data-Accuracy', 'Test data-Loss',\n",
    "                            'Test data-Accuracy','Recall score','Precision score','F score'])\n",
    "\n",
    "row_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 1,'Adam',3,64,'elu',32,'relu',1,'sigmoid', 0, 'No layer', 0,'No layer',\n",
    "                   bankdata_model.history.history['loss'][-1],\n",
    "                            bankdata_model.history.history['accuracy'][-1], eva_results[0],eva_results[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class), precision_score(y_test.values, Y_pred_value_class), \n",
    "                            f1_score(y_test.values,Y_pred_value_class)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "results_df = results_df.drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.77809</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "\n",
       "   Precision score   F score  \n",
       "1          0.77809  0.568205  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2  Case 2 using Nadam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model_nadam.add(Dense(32, activation = 'relu'))\n",
    "bankdata_model_nadam.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam = optimizers.Nadam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam.compile(optimizer = gd_optimizer_nadam, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 1s 114us/sample - loss: 0.5520 - accuracy: 0.7413\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.4486 - accuracy: 0.8021\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4125 - accuracy: 0.8097\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3930 - accuracy: 0.8223\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3808 - accuracy: 0.8331\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3720 - accuracy: 0.8370\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3662 - accuracy: 0.8416\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3621 - accuracy: 0.8421\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3584 - accuracy: 0.8466\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3554 - accuracy: 0.8499\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3530 - accuracy: 0.8499\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3504 - accuracy: 0.8514\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3485 - accuracy: 0.8537\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3464 - accuracy: 0.8556\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3446 - accuracy: 0.8543\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3433 - accuracy: 0.8560\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3417 - accuracy: 0.8571\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3402 - accuracy: 0.8584\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3391 - accuracy: 0.8579\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3377 - accuracy: 0.8597\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3362 - accuracy: 0.8619\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3357 - accuracy: 0.8621\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3342 - accuracy: 0.8620\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.3328 - accuracy: 0.8643\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3325 - accuracy: 0.8633\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3310 - accuracy: 0.8637\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3301 - accuracy: 0.8647\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3290 - accuracy: 0.8624\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3281 - accuracy: 0.8654\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3276 - accuracy: 0.8636\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3267 - accuracy: 0.8650\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3264 - accuracy: 0.8649\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3252 - accuracy: 0.8670\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3245 - accuracy: 0.8666\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3241 - accuracy: 0.8649\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3233 - accuracy: 0.8683\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3227 - accuracy: 0.8673\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3221 - accuracy: 0.8686\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3221 - accuracy: 0.8683\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3213 - accuracy: 0.8689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2862e9afb08>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 69us/sample - loss: 0.3508 - accuracy: 0.8613\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam = bankdata_model_nadam.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.3507893248399099, 0.8613333]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam.metrics_names)\n",
    "print(eva_results_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 23us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam = bankdata_model_nadam.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 5us/sample\n",
      "[[0.19366656]\n",
      " [0.00911786]\n",
      " [0.7622342 ]\n",
      " ...\n",
      " [0.19083332]\n",
      " [0.03651527]\n",
      " [0.14715558]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam = bankdata_model_nadam.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam = (Y_pred_value_nadam > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.3508 - accuracy: 0.8613\n",
      "Accuracy of Model with Nadam optimizer :0.8613333\n",
      "Recall_score: 0.46526655896607433\n",
      "Precision_score: 0.7721179624664879\n",
      "F-score: 0.5806451612903225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2296,   85],\n",
       "       [ 331,  288]], dtype=int64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.3508 - accuracy: 0.8613\n",
      "Accuracy of Model with Nadam optimizer :0.8613333\n",
      "Recall_score: 0.46526655896607433\n",
      "Precision_score: 0.7721179624664879\n",
      "F-score: 0.5806451612903225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2296,   85],\n",
       "       [ 331,  288]], dtype=int64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam = confusion_matrix(y_test.values, Y_pred_value_class_nadam)\n",
    "df_cm_nadam = pd.DataFrame(cm_nadam, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 2,'Nadam',3,64,'elu',32,'relu',1,'sigmoid', 0, 'No layer', 0,'No layer',\n",
    "                   bankdata_model_nadam.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam.history.history['accuracy'][-1], eva_results_nadam[0],eva_results_nadam[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam), f1_score(y_test.values,Y_pred_value_class_nadam)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3  Case 3 using Adam optimizer  with LeakyRelu activation function , since it gives a small graddient when z<0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam2.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model_adam2.add(Dense(32, LeakyReLU(alpha=0.1)))\n",
    "bankdata_model_adam2.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_adam2 = optimizers.Adam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam2.compile(optimizer = gd_optimizer_adam2, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_adam2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 0s 70us/sample - loss: 0.7105 - accuracy: 0.4873\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.5243 - accuracy: 0.7946\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.4522 - accuracy: 0.8001\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4247 - accuracy: 0.8049\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.4080 - accuracy: 0.8134\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3959 - accuracy: 0.8196\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3861 - accuracy: 0.8263\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3784 - accuracy: 0.8331\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3729 - accuracy: 0.8380\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3680 - accuracy: 0.8411\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3643 - accuracy: 0.8449\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3613 - accuracy: 0.8450\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3584 - accuracy: 0.8450\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3557 - accuracy: 0.8480\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3531 - accuracy: 0.8491\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.3511 - accuracy: 0.8503\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3489 - accuracy: 0.8527\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3475 - accuracy: 0.8537\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3458 - accuracy: 0.8549\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3440 - accuracy: 0.8557\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3425 - accuracy: 0.8560\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3411 - accuracy: 0.8579\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3399 - accuracy: 0.8591\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3391 - accuracy: 0.8594\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3377 - accuracy: 0.8597\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3363 - accuracy: 0.8603\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3349 - accuracy: 0.8627\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3341 - accuracy: 0.8627\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3330 - accuracy: 0.8630\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3326 - accuracy: 0.8621\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3313 - accuracy: 0.8640\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3311 - accuracy: 0.8616\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3297 - accuracy: 0.8656\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3295 - accuracy: 0.8644\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3283 - accuracy: 0.8646\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3272 - accuracy: 0.8649\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3267 - accuracy: 0.8644\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3260 - accuracy: 0.8651\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3251 - accuracy: 0.8657\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3251 - accuracy: 0.8657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2862fca5fc8>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_adam2.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 77us/sample - loss: 0.3503 - accuracy: 0.8630\n"
     ]
    }
   ],
   "source": [
    "eva_results_adam2 = bankdata_model_adam2.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.3502750973701477, 0.863]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_adam2.metrics_names)\n",
    "print(eva_results_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 27us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_adam2 = bankdata_model_adam2.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 5us/sample\n",
      "[[0.15356085]\n",
      " [0.01089112]\n",
      " [0.7225771 ]\n",
      " ...\n",
      " [0.14705773]\n",
      " [0.04153422]\n",
      " [0.14662361]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_adam2 = bankdata_model_adam2.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_adam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2632, 1: 368}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_adam2 = (Y_pred_value_adam2 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_adam2)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_adam2, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.3503 - accuracy: 0.8630\n",
      "Accuracy of Model with Adam optimizer :0.863\n",
      "Recall_score: 0.46526655896607433\n",
      "Precision_score: 0.782608695652174\n",
      "F-score: 0.5835866261398177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2301,   80],\n",
       "       [ 331,  288]], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_adam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_adam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_adam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.3503 - accuracy: 0.8630\n",
      "Accuracy of Model with Adam optimizer :0.863\n",
      "Recall_score: 0.46526655896607433\n",
      "Precision_score: 0.782608695652174\n",
      "F-score: 0.5835866261398177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2301,   80],\n",
       "       [ 331,  288]], dtype=int64)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_adam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_adam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_adam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_adam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam2 = confusion_matrix(y_test.values, Y_pred_value_class_adam2)\n",
    "df_cm_adam2 = pd.DataFrame(cm_adam2, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam2, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 3,'Adam',3,64,'elu',32,'LeakyReLU',1,'sigmoid', 0, 'No layer',0,'No layer',\n",
    "                   bankdata_model_adam2.history.history['loss'][-1],\n",
    "                            bankdata_model_adam2.history.history['accuracy'][-1], eva_results_adam2[0],eva_results_adam2[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_adam2), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_adam2), f1_score(y_test.values,Y_pred_value_class_adam2)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "3     No layer              0     No layer         0.325063   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  \n",
       "3         0.782609  0.583587  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Case 4 using Adam optimizer  with relu in first layer and also using LeakyRelu activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam3 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam3.add(Dense(64, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_adam3.add(Dense(32, LeakyReLU(alpha=0.1)))\n",
    "bankdata_model_adam3.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_adam3 = optimizers.Adam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam3.compile(optimizer = gd_optimizer_adam3, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_adam3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 0s 67us/sample - loss: 0.5060 - accuracy: 0.7973\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4538 - accuracy: 0.7974\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4262 - accuracy: 0.7991\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.4093 - accuracy: 0.8043\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3962 - accuracy: 0.8156\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3852 - accuracy: 0.8250\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3758 - accuracy: 0.8333\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.3686 - accuracy: 0.8396\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3625 - accuracy: 0.8444\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3576 - accuracy: 0.8483\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3532 - accuracy: 0.8491\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3494 - accuracy: 0.8533\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3458 - accuracy: 0.8554\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3437 - accuracy: 0.8570\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3410 - accuracy: 0.8576\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3373 - accuracy: 0.8596\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3355 - accuracy: 0.8599\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3334 - accuracy: 0.8623\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3313 - accuracy: 0.8636\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3289 - accuracy: 0.8640\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3271 - accuracy: 0.8647\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3258 - accuracy: 0.8659\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3240 - accuracy: 0.8667\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 3us/sample - loss: 0.3221 - accuracy: 0.8673\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3206 - accuracy: 0.8679\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3199 - accuracy: 0.8691\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3179 - accuracy: 0.8689\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3165 - accuracy: 0.8699\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3146 - accuracy: 0.8677\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3134 - accuracy: 0.8707\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3119 - accuracy: 0.8707\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3113 - accuracy: 0.8706\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3101 - accuracy: 0.8696\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3089 - accuracy: 0.8729\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3076 - accuracy: 0.8709\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3060 - accuracy: 0.8721\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3052 - accuracy: 0.8731\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3050 - accuracy: 0.8727\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3036 - accuracy: 0.8730\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3019 - accuracy: 0.8741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2862ea32c88>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_adam3.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 72us/sample - loss: 0.3566 - accuracy: 0.8573\n"
     ]
    }
   ],
   "source": [
    "eva_results_adam3 = bankdata_model_adam3.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.35663781547546386, 0.85733336]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_adam3.metrics_names)\n",
    "print(eva_results_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 21us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_adam3 = bankdata_model_adam3.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 10us/sample\n",
      "[[0.1800493 ]\n",
      " [0.00652079]\n",
      " [0.68678874]\n",
      " ...\n",
      " [0.10693097]\n",
      " [0.04807061]\n",
      " [0.20234911]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_adam3 = bankdata_model_adam3.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_adam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2597, 1: 403}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_adam3 = (Y_pred_value_adam3 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_adam3)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_adam3, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.3566 - accuracy: 0.8573\n",
      "Accuracy of Model with Adam optimizer :0.85733336\n",
      "Recall_score: 0.4798061389337641\n",
      "Precision_score: 0.7369727047146402\n",
      "F-score: 0.5812133072407045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2275,  106],\n",
       "       [ 322,  297]], dtype=int64)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_adam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_adam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_adam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.3566 - accuracy: 0.8573\n",
      "Accuracy of Model  with Adam optimizer :0.85733336\n",
      "Recall_score: 0.4798061389337641\n",
      "Precision_score: 0.7369727047146402\n",
      "F-score: 0.5812133072407045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2275,  106],\n",
       "       [ 322,  297]], dtype=int64)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model  with Adam optimizer :'+ str(bankdata_model_adam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_adam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_adam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_adam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_adam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam3 = confusion_matrix(y_test.values, Y_pred_value_class_adam3)\n",
    "df_cm_adam3 = pd.DataFrame(cm_adam3, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam3, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 4,'Adam',3,64,'relu',32,'LeakyReLU',1,'sigmoid', 0, 'No layer',0, 'No layer',\n",
    "                   bankdata_model_adam3.history.history['loss'][-1], bankdata_model_adam3.history.history['accuracy'][-1], \n",
    "                   eva_results_adam3[0],eva_results_adam3[1],recall_score(y_test.values,Y_pred_value_class_adam3), \n",
    "                   precision_score(y_test.values, Y_pred_value_class_adam3), \n",
    "                   f1_score(y_test.values,Y_pred_value_class_adam3)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "3     No layer              0     No layer         0.325063   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  \n",
       "3         0.782609  0.583587  \n",
       "4         0.736973  0.581213  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5  Case 5 using Adam optimizer  with LeakyRelu activation function , with 4 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam4 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model , with total 5 layers. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam4.add(Dense(128, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_adam4.add(Dense(64, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_adam4.add(Dense(32, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_adam4.add(Dense(16, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_adam4.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_adam4 = optimizers.Adam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_adam4.compile(optimizer = gd_optimizer_adam4, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 14,337\n",
      "Trainable params: 14,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_adam4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/280\n",
      "7000/7000 [==============================] - 1s 98us/sample - loss: 0.5294 - accuracy: 0.7911\n",
      "Epoch 2/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4355 - accuracy: 0.7979\n",
      "Epoch 3/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3975 - accuracy: 0.8177\n",
      "Epoch 4/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3804 - accuracy: 0.8287\n",
      "Epoch 5/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3680 - accuracy: 0.8391\n",
      "Epoch 6/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3583 - accuracy: 0.8486\n",
      "Epoch 7/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3497 - accuracy: 0.8543\n",
      "Epoch 8/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3432 - accuracy: 0.8573\n",
      "Epoch 9/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3387 - accuracy: 0.8623\n",
      "Epoch 10/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3312 - accuracy: 0.8639\n",
      "Epoch 11/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3275 - accuracy: 0.8670\n",
      "Epoch 12/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3227 - accuracy: 0.8681\n",
      "Epoch 13/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3186 - accuracy: 0.8709\n",
      "Epoch 14/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3159 - accuracy: 0.8717\n",
      "Epoch 15/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3108 - accuracy: 0.8746\n",
      "Epoch 16/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3077 - accuracy: 0.8750\n",
      "Epoch 17/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3048 - accuracy: 0.8759\n",
      "Epoch 18/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3001 - accuracy: 0.8774\n",
      "Epoch 19/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2987 - accuracy: 0.8787\n",
      "Epoch 20/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2962 - accuracy: 0.8797\n",
      "Epoch 21/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2956 - accuracy: 0.8801\n",
      "Epoch 22/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2940 - accuracy: 0.8806\n",
      "Epoch 23/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2897 - accuracy: 0.8801\n",
      "Epoch 24/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2894 - accuracy: 0.8814\n",
      "Epoch 25/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2826 - accuracy: 0.8871\n",
      "Epoch 26/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2781 - accuracy: 0.8883\n",
      "Epoch 27/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2769 - accuracy: 0.8894\n",
      "Epoch 28/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2733 - accuracy: 0.8897\n",
      "Epoch 29/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2712 - accuracy: 0.8909\n",
      "Epoch 30/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2676 - accuracy: 0.8934\n",
      "Epoch 31/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2628 - accuracy: 0.8963\n",
      "Epoch 32/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2622 - accuracy: 0.8954\n",
      "Epoch 33/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2573 - accuracy: 0.9000\n",
      "Epoch 34/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2561 - accuracy: 0.8974\n",
      "Epoch 35/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2526 - accuracy: 0.8997\n",
      "Epoch 36/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2486 - accuracy: 0.9024\n",
      "Epoch 37/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2481 - accuracy: 0.9020\n",
      "Epoch 38/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2490 - accuracy: 0.9029\n",
      "Epoch 39/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2450 - accuracy: 0.9017\n",
      "Epoch 40/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2504 - accuracy: 0.8994\n",
      "Epoch 41/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2411 - accuracy: 0.9060\n",
      "Epoch 42/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2398 - accuracy: 0.9073\n",
      "Epoch 43/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2314 - accuracy: 0.9096\n",
      "Epoch 44/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2312 - accuracy: 0.9091\n",
      "Epoch 45/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2286 - accuracy: 0.9089\n",
      "Epoch 46/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2236 - accuracy: 0.9151\n",
      "Epoch 47/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2237 - accuracy: 0.9130\n",
      "Epoch 48/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2180 - accuracy: 0.9177\n",
      "Epoch 49/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2165 - accuracy: 0.9177\n",
      "Epoch 50/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2159 - accuracy: 0.9161\n",
      "Epoch 51/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2119 - accuracy: 0.9179\n",
      "Epoch 52/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2157 - accuracy: 0.9184\n",
      "Epoch 53/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2170 - accuracy: 0.9130\n",
      "Epoch 54/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2063 - accuracy: 0.9197\n",
      "Epoch 55/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2008 - accuracy: 0.9244\n",
      "Epoch 56/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1978 - accuracy: 0.9263\n",
      "Epoch 57/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1911 - accuracy: 0.9289\n",
      "Epoch 58/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2031 - accuracy: 0.9219\n",
      "Epoch 59/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1978 - accuracy: 0.9250\n",
      "Epoch 60/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1957 - accuracy: 0.9259\n",
      "Epoch 61/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1919 - accuracy: 0.9283\n",
      "Epoch 62/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1918 - accuracy: 0.9271\n",
      "Epoch 63/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1865 - accuracy: 0.9296\n",
      "Epoch 64/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1813 - accuracy: 0.9301\n",
      "Epoch 65/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1741 - accuracy: 0.9346\n",
      "Epoch 66/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1721 - accuracy: 0.9371\n",
      "Epoch 67/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1660 - accuracy: 0.9419\n",
      "Epoch 68/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1684 - accuracy: 0.9359\n",
      "Epoch 69/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1685 - accuracy: 0.9369\n",
      "Epoch 70/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1609 - accuracy: 0.9396\n",
      "Epoch 71/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1610 - accuracy: 0.9416\n",
      "Epoch 72/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1593 - accuracy: 0.9426\n",
      "Epoch 73/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1577 - accuracy: 0.9419\n",
      "Epoch 74/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1508 - accuracy: 0.9456\n",
      "Epoch 75/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1521 - accuracy: 0.9446\n",
      "Epoch 76/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1495 - accuracy: 0.9440\n",
      "Epoch 77/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1454 - accuracy: 0.9457\n",
      "Epoch 78/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1491 - accuracy: 0.9431\n",
      "Epoch 79/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1473 - accuracy: 0.9454\n",
      "Epoch 80/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1451 - accuracy: 0.9434\n",
      "Epoch 81/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1399 - accuracy: 0.9491\n",
      "Epoch 82/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1318 - accuracy: 0.9550\n",
      "Epoch 83/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1417 - accuracy: 0.9464\n",
      "Epoch 84/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1428 - accuracy: 0.9464\n",
      "Epoch 85/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1546 - accuracy: 0.9400\n",
      "Epoch 86/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1336 - accuracy: 0.9494\n",
      "Epoch 87/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1238 - accuracy: 0.9577\n",
      "Epoch 88/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1258 - accuracy: 0.9561\n",
      "Epoch 89/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1193 - accuracy: 0.9584\n",
      "Epoch 90/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1177 - accuracy: 0.9593\n",
      "Epoch 91/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1375 - accuracy: 0.9487\n",
      "Epoch 92/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1602 - accuracy: 0.9373\n",
      "Epoch 93/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1206 - accuracy: 0.9576\n",
      "Epoch 94/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1246 - accuracy: 0.9550\n",
      "Epoch 95/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1165 - accuracy: 0.9559\n",
      "Epoch 96/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1116 - accuracy: 0.9611\n",
      "Epoch 97/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1123 - accuracy: 0.9606\n",
      "Epoch 98/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1106 - accuracy: 0.9591\n",
      "Epoch 99/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1032 - accuracy: 0.9673\n",
      "Epoch 100/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1045 - accuracy: 0.9646\n",
      "Epoch 101/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1106 - accuracy: 0.9607\n",
      "Epoch 102/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1040 - accuracy: 0.9649\n",
      "Epoch 103/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0965 - accuracy: 0.9674\n",
      "Epoch 104/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0984 - accuracy: 0.9671\n",
      "Epoch 105/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1027 - accuracy: 0.9629\n",
      "Epoch 106/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1008 - accuracy: 0.9631\n",
      "Epoch 107/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0930 - accuracy: 0.9694\n",
      "Epoch 108/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0910 - accuracy: 0.9699\n",
      "Epoch 109/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0893 - accuracy: 0.9697\n",
      "Epoch 110/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0863 - accuracy: 0.9743\n",
      "Epoch 111/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0832 - accuracy: 0.9724\n",
      "Epoch 112/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0881 - accuracy: 0.9704\n",
      "Epoch 113/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0814 - accuracy: 0.9743\n",
      "Epoch 114/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0811 - accuracy: 0.9743\n",
      "Epoch 115/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0817 - accuracy: 0.9716\n",
      "Epoch 116/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0777 - accuracy: 0.9749\n",
      "Epoch 117/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0750 - accuracy: 0.9787\n",
      "Epoch 118/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0744 - accuracy: 0.9761\n",
      "Epoch 119/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0790 - accuracy: 0.9730\n",
      "Epoch 120/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0815 - accuracy: 0.9703\n",
      "Epoch 121/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0760 - accuracy: 0.9760\n",
      "Epoch 122/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0720 - accuracy: 0.9764\n",
      "Epoch 123/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0676 - accuracy: 0.9797\n",
      "Epoch 124/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0712 - accuracy: 0.9767\n",
      "Epoch 125/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0864 - accuracy: 0.9664\n",
      "Epoch 126/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0693 - accuracy: 0.9771\n",
      "Epoch 127/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0645 - accuracy: 0.9801\n",
      "Epoch 128/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0643 - accuracy: 0.9814\n",
      "Epoch 129/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0568 - accuracy: 0.9840\n",
      "Epoch 130/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0588 - accuracy: 0.9831\n",
      "Epoch 131/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0627 - accuracy: 0.9807\n",
      "Epoch 132/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0619 - accuracy: 0.9799\n",
      "Epoch 133/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0637 - accuracy: 0.9771\n",
      "Epoch 134/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0687 - accuracy: 0.9753\n",
      "Epoch 135/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0592 - accuracy: 0.9809\n",
      "Epoch 136/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0583 - accuracy: 0.9816\n",
      "Epoch 137/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0756 - accuracy: 0.9707\n",
      "Epoch 138/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0538 - accuracy: 0.9850\n",
      "Epoch 139/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0580 - accuracy: 0.9811\n",
      "Epoch 140/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0518 - accuracy: 0.9836\n",
      "Epoch 141/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0510 - accuracy: 0.9834\n",
      "Epoch 142/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0497 - accuracy: 0.9846\n",
      "Epoch 143/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0579 - accuracy: 0.9797\n",
      "Epoch 144/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0649 - accuracy: 0.9757\n",
      "Epoch 145/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0652 - accuracy: 0.9741\n",
      "Epoch 146/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0495 - accuracy: 0.9847\n",
      "Epoch 147/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0405 - accuracy: 0.9894\n",
      "Epoch 148/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0408 - accuracy: 0.9884\n",
      "Epoch 149/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0403 - accuracy: 0.9880\n",
      "Epoch 150/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0466 - accuracy: 0.9857\n",
      "Epoch 151/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0630 - accuracy: 0.9769\n",
      "Epoch 152/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0575 - accuracy: 0.9799\n",
      "Epoch 153/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0534 - accuracy: 0.9819\n",
      "Epoch 154/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0414 - accuracy: 0.9884\n",
      "Epoch 155/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0366 - accuracy: 0.9906\n",
      "Epoch 156/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0365 - accuracy: 0.9899\n",
      "Epoch 157/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0364 - accuracy: 0.9910\n",
      "Epoch 158/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0370 - accuracy: 0.9907\n",
      "Epoch 159/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0374 - accuracy: 0.9887\n",
      "Epoch 160/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0339 - accuracy: 0.9911\n",
      "Epoch 161/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0344 - accuracy: 0.9909\n",
      "Epoch 162/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0368 - accuracy: 0.9891\n",
      "Epoch 163/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0337 - accuracy: 0.9901\n",
      "Epoch 164/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0312 - accuracy: 0.9919\n",
      "Epoch 165/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0321 - accuracy: 0.9920\n",
      "Epoch 166/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0304 - accuracy: 0.9920\n",
      "Epoch 167/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0327 - accuracy: 0.9909\n",
      "Epoch 168/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0342 - accuracy: 0.9880\n",
      "Epoch 169/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0359 - accuracy: 0.9871\n",
      "Epoch 170/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0325 - accuracy: 0.9904\n",
      "Epoch 171/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0281 - accuracy: 0.9923\n",
      "Epoch 172/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0278 - accuracy: 0.9926\n",
      "Epoch 173/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0248 - accuracy: 0.9930\n",
      "Epoch 174/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0234 - accuracy: 0.9944\n",
      "Epoch 175/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0231 - accuracy: 0.9953\n",
      "Epoch 176/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0223 - accuracy: 0.9951\n",
      "Epoch 177/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0215 - accuracy: 0.9956\n",
      "Epoch 178/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0221 - accuracy: 0.9951\n",
      "Epoch 179/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0246 - accuracy: 0.9937\n",
      "Epoch 180/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0227 - accuracy: 0.9944\n",
      "Epoch 181/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0225 - accuracy: 0.9953\n",
      "Epoch 182/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.0360 - accuracy: 0.9891\n",
      "Epoch 183/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0451 - accuracy: 0.9840\n",
      "Epoch 184/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0775 - accuracy: 0.9679\n",
      "Epoch 185/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0582 - accuracy: 0.9760\n",
      "Epoch 186/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0419 - accuracy: 0.9841\n",
      "Epoch 187/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0410 - accuracy: 0.9843\n",
      "Epoch 188/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0353 - accuracy: 0.9874\n",
      "Epoch 189/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.0326 - accuracy: 0.9896\n",
      "Epoch 190/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0265 - accuracy: 0.9920\n",
      "Epoch 191/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0171 - accuracy: 0.9960\n",
      "Epoch 192/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0164 - accuracy: 0.9967\n",
      "Epoch 193/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0150 - accuracy: 0.9974\n",
      "Epoch 194/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0188 - accuracy: 0.9960\n",
      "Epoch 195/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0529 - accuracy: 0.9794\n",
      "Epoch 196/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0460 - accuracy: 0.9837\n",
      "Epoch 197/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0423 - accuracy: 0.9840\n",
      "Epoch 198/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0233 - accuracy: 0.9936\n",
      "Epoch 199/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0194 - accuracy: 0.9953\n",
      "Epoch 200/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0186 - accuracy: 0.9963\n",
      "Epoch 201/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0203 - accuracy: 0.9940\n",
      "Epoch 202/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0216 - accuracy: 0.9940\n",
      "Epoch 203/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0163 - accuracy: 0.9963\n",
      "Epoch 204/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0153 - accuracy: 0.9971\n",
      "Epoch 205/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0126 - accuracy: 0.9980\n",
      "Epoch 206/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0124 - accuracy: 0.9980\n",
      "Epoch 207/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0138 - accuracy: 0.9967\n",
      "Epoch 208/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0144 - accuracy: 0.9973\n",
      "Epoch 209/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 210/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0127 - accuracy: 0.9983\n",
      "Epoch 211/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0135 - accuracy: 0.9971\n",
      "Epoch 212/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0129 - accuracy: 0.9974\n",
      "Epoch 213/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0130 - accuracy: 0.9973\n",
      "Epoch 214/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0134 - accuracy: 0.9976\n",
      "Epoch 215/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0100 - accuracy: 0.9989\n",
      "Epoch 216/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0090 - accuracy: 0.9991\n",
      "Epoch 217/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0083 - accuracy: 0.9986\n",
      "Epoch 218/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0096 - accuracy: 0.9983\n",
      "Epoch 219/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0087 - accuracy: 0.9986\n",
      "Epoch 220/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0079 - accuracy: 0.9986\n",
      "Epoch 221/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0077 - accuracy: 0.9993\n",
      "Epoch 222/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0104 - accuracy: 0.9977\n",
      "Epoch 223/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0112 - accuracy: 0.9986\n",
      "Epoch 224/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0124 - accuracy: 0.9967\n",
      "Epoch 225/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0137 - accuracy: 0.9966\n",
      "Epoch 226/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0396 - accuracy: 0.9859\n",
      "Epoch 227/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0400 - accuracy: 0.9853\n",
      "Epoch 228/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0261 - accuracy: 0.9909\n",
      "Epoch 229/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0323 - accuracy: 0.9884\n",
      "Epoch 230/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0442 - accuracy: 0.9839\n",
      "Epoch 231/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0826 - accuracy: 0.9709\n",
      "Epoch 232/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1170 - accuracy: 0.9623\n",
      "Epoch 233/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1321 - accuracy: 0.9600\n",
      "Epoch 234/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1097 - accuracy: 0.9613\n",
      "Epoch 235/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0984 - accuracy: 0.9644\n",
      "Epoch 236/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0441 - accuracy: 0.9841\n",
      "Epoch 237/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0337 - accuracy: 0.9884\n",
      "Epoch 238/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0185 - accuracy: 0.9951\n",
      "Epoch 239/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0155 - accuracy: 0.9956\n",
      "Epoch 240/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0110 - accuracy: 0.9981\n",
      "Epoch 241/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0086 - accuracy: 0.9987\n",
      "Epoch 242/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0091 - accuracy: 0.9983\n",
      "Epoch 243/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0078 - accuracy: 0.9991\n",
      "Epoch 244/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0069 - accuracy: 0.9996\n",
      "Epoch 245/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0073 - accuracy: 0.9990\n",
      "Epoch 246/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0068 - accuracy: 0.9994\n",
      "Epoch 247/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0083 - accuracy: 0.9989\n",
      "Epoch 248/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0073 - accuracy: 0.9989\n",
      "Epoch 249/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0065 - accuracy: 0.9993\n",
      "Epoch 250/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0064 - accuracy: 0.9991\n",
      "Epoch 251/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0067 - accuracy: 0.9993\n",
      "Epoch 252/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0064 - accuracy: 0.9993\n",
      "Epoch 253/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0054 - accuracy: 0.9999\n",
      "Epoch 254/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0059 - accuracy: 0.9994\n",
      "Epoch 255/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0066 - accuracy: 0.9996\n",
      "Epoch 256/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0073 - accuracy: 0.9991\n",
      "Epoch 257/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0067 - accuracy: 0.9991\n",
      "Epoch 258/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0062 - accuracy: 0.9994\n",
      "Epoch 259/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0066 - accuracy: 0.9989\n",
      "Epoch 260/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0068 - accuracy: 0.9990\n",
      "Epoch 261/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0068 - accuracy: 0.9990\n",
      "Epoch 262/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0062 - accuracy: 0.9994\n",
      "Epoch 263/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0054 - accuracy: 0.9993\n",
      "Epoch 264/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0062 - accuracy: 0.9994\n",
      "Epoch 265/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0060 - accuracy: 0.9993\n",
      "Epoch 266/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0051 - accuracy: 0.9993\n",
      "Epoch 267/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0045 - accuracy: 0.9996\n",
      "Epoch 268/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0052 - accuracy: 0.9991\n",
      "Epoch 269/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0058 - accuracy: 0.9993\n",
      "Epoch 270/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.0052 - accuracy: 0.9994\n",
      "Epoch 271/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0045 - accuracy: 0.9996\n",
      "Epoch 272/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 273/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0051 - accuracy: 0.9996\n",
      "Epoch 274/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0052 - accuracy: 0.9994\n",
      "Epoch 275/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0042 - accuracy: 0.9997\n",
      "Epoch 276/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0046 - accuracy: 0.9996\n",
      "Epoch 277/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 278/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0049 - accuracy: 0.9993\n",
      "Epoch 279/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0055 - accuracy: 0.9993\n",
      "Epoch 280/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0045 - accuracy: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x286314a0dc8>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_adam4.fit(X_train, y_train, batch_size = 500, epochs = 280, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows model overfitted to Train data , with accuracy reaching 0.9997 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 77us/sample - loss: 1.8451 - accuracy: 0.8097\n"
     ]
    }
   ],
   "source": [
    "eva_results_adam4 = bankdata_model_adam4.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[1.8450690313975016, 0.8096667]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_adam4.metrics_names)\n",
    "print(eva_results_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It can be seen that with Test data , accuracy drops , since model was overfitted to Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 19us/sample\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_adam4 = bankdata_model_adam4.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 11us/sample\n",
      "[[9.9995279e-01]\n",
      " [8.4196631e-07]\n",
      " [7.2057623e-01]\n",
      " ...\n",
      " [2.4892017e-03]\n",
      " [1.0972284e-06]\n",
      " [4.1654080e-02]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_adam4 = bankdata_model_adam4.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_adam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2418, 1: 582}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_adam4 = (Y_pred_value_adam4 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_adam4)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_adam4, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 39us/sample - loss: 1.8451 - accuracy: 0.8097\n",
      "Accuracy of Model with Adam optimizer :0.8096667\n",
      "Recall_score: 0.5088852988691438\n",
      "Precision_score: 0.5412371134020618\n",
      "F-score: 0.5245628642797668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2114,  267],\n",
       "       [ 304,  315]], dtype=int64)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_adam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_adam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_adam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 41us/sample - loss: 1.8451 - accuracy: 0.8097\n",
      "Accuracy of Model with Adam optimizer :0.8096667\n",
      "Recall_score: 0.5088852988691438\n",
      "Precision_score: 0.5412371134020618\n",
      "F-score: 0.5245628642797668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2114,  267],\n",
       "       [ 304,  315]], dtype=int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Adam optimizer :'+ str(bankdata_model_adam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_adam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_adam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_adam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_adam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adam4 = confusion_matrix(y_test.values, Y_pred_value_class_adam4)\n",
    "df_cm_adam4 = pd.DataFrame(cm_adam4, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_adam4, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 5,'Adam',5,128,'relu',64,'LeakyReLU',32,'LeakyReLU',16,'LeakyReLU',1,'sigmoid', \n",
    "                   bankdata_model_adam4.history.history['loss'][-1],\n",
    "                            bankdata_model_adam4.history.history['accuracy'][-1], eva_results_adam4[0],eva_results_adam4[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_adam4), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_adam4), f1_score(y_test.values,Y_pred_value_class_adam4)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.845069</td>\n",
       "      <td>0.809667</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.541237</td>\n",
       "      <td>0.524563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "3     No layer              0     No layer         0.325063   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "5    LeakyReLU              1      sigmoid         0.004469   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "5             0.999143        1.845069            0.809667      0.508885   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  \n",
       "3         0.782609  0.583587  \n",
       "4         0.736973  0.581213  \n",
       "5         0.541237  0.524563  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Case 6 using NAdam optimizer  with LeakyRelu activation function , with 4 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model , with total 5 layers. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam2.add(Dense(128, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_nadam2.add(Dense(64, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam2.add(Dense(32, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam2.add(Dense(16, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam2.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam2 = optimizers.Nadam(lr = 0.001, beta_1 = 0.8) ## use beta_1=0.8 (default=0.9) , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam2.compile(optimizer = gd_optimizer_nadam2, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 14,337\n",
      "Trainable params: 14,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/280\n",
      "7000/7000 [==============================] - 1s 174us/sample - loss: 0.5822 - accuracy: 0.7394\n",
      "Epoch 2/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4423 - accuracy: 0.8014\n",
      "Epoch 3/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3978 - accuracy: 0.8246\n",
      "Epoch 4/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3786 - accuracy: 0.8349\n",
      "Epoch 5/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3669 - accuracy: 0.8451\n",
      "Epoch 6/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3577 - accuracy: 0.8463\n",
      "Epoch 7/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3504 - accuracy: 0.8509\n",
      "Epoch 8/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3437 - accuracy: 0.8560\n",
      "Epoch 9/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3387 - accuracy: 0.8571\n",
      "Epoch 10/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3324 - accuracy: 0.8629\n",
      "Epoch 11/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3301 - accuracy: 0.8663\n",
      "Epoch 12/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3284 - accuracy: 0.8660\n",
      "Epoch 13/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3228 - accuracy: 0.8664\n",
      "Epoch 14/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3225 - accuracy: 0.8666\n",
      "Epoch 15/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3221 - accuracy: 0.8649\n",
      "Epoch 16/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3174 - accuracy: 0.8723\n",
      "Epoch 17/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3132 - accuracy: 0.8710\n",
      "Epoch 18/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3130 - accuracy: 0.8697\n",
      "Epoch 19/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3156 - accuracy: 0.8716\n",
      "Epoch 20/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3062 - accuracy: 0.8750\n",
      "Epoch 21/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3035 - accuracy: 0.8770\n",
      "Epoch 22/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3056 - accuracy: 0.8750\n",
      "Epoch 23/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3007 - accuracy: 0.8784\n",
      "Epoch 24/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2972 - accuracy: 0.8796\n",
      "Epoch 25/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2998 - accuracy: 0.8803\n",
      "Epoch 26/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3021 - accuracy: 0.8761\n",
      "Epoch 27/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.2937 - accuracy: 0.8817\n",
      "Epoch 28/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3011 - accuracy: 0.8777\n",
      "Epoch 29/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2969 - accuracy: 0.8764\n",
      "Epoch 30/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2934 - accuracy: 0.8814\n",
      "Epoch 31/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2868 - accuracy: 0.8843\n",
      "Epoch 32/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2842 - accuracy: 0.8856\n",
      "Epoch 33/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2910 - accuracy: 0.8841\n",
      "Epoch 34/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2813 - accuracy: 0.8831\n",
      "Epoch 35/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2808 - accuracy: 0.8887\n",
      "Epoch 36/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2817 - accuracy: 0.8860\n",
      "Epoch 37/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2818 - accuracy: 0.8877\n",
      "Epoch 38/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2853 - accuracy: 0.8840\n",
      "Epoch 39/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2746 - accuracy: 0.8883\n",
      "Epoch 40/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2817 - accuracy: 0.8880\n",
      "Epoch 41/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2684 - accuracy: 0.8924\n",
      "Epoch 42/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2836 - accuracy: 0.8847\n",
      "Epoch 43/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2760 - accuracy: 0.8874\n",
      "Epoch 44/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2673 - accuracy: 0.8901\n",
      "Epoch 45/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2599 - accuracy: 0.8939\n",
      "Epoch 46/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2793 - accuracy: 0.8871\n",
      "Epoch 47/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2587 - accuracy: 0.8956\n",
      "Epoch 48/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2651 - accuracy: 0.8933\n",
      "Epoch 49/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2667 - accuracy: 0.8904\n",
      "Epoch 50/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2603 - accuracy: 0.8960\n",
      "Epoch 51/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2528 - accuracy: 0.9004\n",
      "Epoch 52/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2483 - accuracy: 0.8996\n",
      "Epoch 53/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2692 - accuracy: 0.8901\n",
      "Epoch 54/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2691 - accuracy: 0.8924\n",
      "Epoch 55/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2511 - accuracy: 0.9001\n",
      "Epoch 56/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2519 - accuracy: 0.8989\n",
      "Epoch 57/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2646 - accuracy: 0.8921\n",
      "Epoch 58/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2417 - accuracy: 0.9051\n",
      "Epoch 59/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2577 - accuracy: 0.8941\n",
      "Epoch 60/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2472 - accuracy: 0.9024\n",
      "Epoch 61/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2389 - accuracy: 0.9047\n",
      "Epoch 62/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2311 - accuracy: 0.9084\n",
      "Epoch 63/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2590 - accuracy: 0.8934\n",
      "Epoch 64/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2296 - accuracy: 0.9090\n",
      "Epoch 65/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2602 - accuracy: 0.8904\n",
      "Epoch 66/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2281 - accuracy: 0.9066\n",
      "Epoch 67/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2289 - accuracy: 0.9071\n",
      "Epoch 68/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2517 - accuracy: 0.9003\n",
      "Epoch 69/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2423 - accuracy: 0.9004\n",
      "Epoch 70/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2450 - accuracy: 0.9021\n",
      "Epoch 71/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2291 - accuracy: 0.9067\n",
      "Epoch 72/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2339 - accuracy: 0.9051\n",
      "Epoch 73/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2266 - accuracy: 0.9113\n",
      "Epoch 74/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2319 - accuracy: 0.9029\n",
      "Epoch 75/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2101 - accuracy: 0.9149\n",
      "Epoch 76/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2538 - accuracy: 0.8984\n",
      "Epoch 77/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2183 - accuracy: 0.9110\n",
      "Epoch 78/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2111 - accuracy: 0.9180\n",
      "Epoch 79/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2258 - accuracy: 0.9069\n",
      "Epoch 80/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2235 - accuracy: 0.9089\n",
      "Epoch 81/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2231 - accuracy: 0.9084\n",
      "Epoch 82/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2027 - accuracy: 0.9196\n",
      "Epoch 83/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2195 - accuracy: 0.9091\n",
      "Epoch 84/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2073 - accuracy: 0.9181\n",
      "Epoch 85/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2287 - accuracy: 0.9054\n",
      "Epoch 86/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2246 - accuracy: 0.9099\n",
      "Epoch 87/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1973 - accuracy: 0.9214\n",
      "Epoch 88/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2007 - accuracy: 0.9163\n",
      "Epoch 89/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2262 - accuracy: 0.9107\n",
      "Epoch 90/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.2247 - accuracy: 0.9077\n",
      "Epoch 91/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1978 - accuracy: 0.9203\n",
      "Epoch 92/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1993 - accuracy: 0.9186\n",
      "Epoch 93/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2111 - accuracy: 0.9107\n",
      "Epoch 94/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1987 - accuracy: 0.9174\n",
      "Epoch 95/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1951 - accuracy: 0.9226\n",
      "Epoch 96/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2005 - accuracy: 0.9169\n",
      "Epoch 97/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2140 - accuracy: 0.9124\n",
      "Epoch 98/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2129 - accuracy: 0.9136\n",
      "Epoch 99/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1939 - accuracy: 0.9220\n",
      "Epoch 100/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1720 - accuracy: 0.9347\n",
      "Epoch 101/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2005 - accuracy: 0.9190\n",
      "Epoch 102/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.2075 - accuracy: 0.9143\n",
      "Epoch 103/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1999 - accuracy: 0.9154\n",
      "Epoch 104/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1968 - accuracy: 0.9174\n",
      "Epoch 105/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1738 - accuracy: 0.9321\n",
      "Epoch 106/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1745 - accuracy: 0.9316\n",
      "Epoch 107/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2177 - accuracy: 0.9177\n",
      "Epoch 108/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1756 - accuracy: 0.9306\n",
      "Epoch 109/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2006 - accuracy: 0.9170\n",
      "Epoch 110/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1863 - accuracy: 0.9284\n",
      "Epoch 111/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1874 - accuracy: 0.9264\n",
      "Epoch 112/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1866 - accuracy: 0.9254\n",
      "Epoch 113/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1721 - accuracy: 0.9281\n",
      "Epoch 114/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1889 - accuracy: 0.9219\n",
      "Epoch 115/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1740 - accuracy: 0.9314\n",
      "Epoch 116/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2165 - accuracy: 0.9086\n",
      "Epoch 117/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1625 - accuracy: 0.9379\n",
      "Epoch 118/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1691 - accuracy: 0.9309\n",
      "Epoch 119/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1856 - accuracy: 0.9276\n",
      "Epoch 120/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1523 - accuracy: 0.9414\n",
      "Epoch 121/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1795 - accuracy: 0.9259\n",
      "Epoch 122/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1531 - accuracy: 0.9397\n",
      "Epoch 123/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.2103 - accuracy: 0.9133\n",
      "Epoch 124/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1482 - accuracy: 0.9430\n",
      "Epoch 125/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1946 - accuracy: 0.9193\n",
      "Epoch 126/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1454 - accuracy: 0.9440\n",
      "Epoch 127/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1949 - accuracy: 0.9223\n",
      "Epoch 128/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1558 - accuracy: 0.9363\n",
      "Epoch 129/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1604 - accuracy: 0.9343\n",
      "Epoch 130/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1467 - accuracy: 0.9431\n",
      "Epoch 131/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1400 - accuracy: 0.9463\n",
      "Epoch 132/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1828 - accuracy: 0.9289\n",
      "Epoch 133/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1500 - accuracy: 0.9409\n",
      "Epoch 134/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1921 - accuracy: 0.9193\n",
      "Epoch 135/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1327 - accuracy: 0.9490\n",
      "Epoch 136/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2136 - accuracy: 0.9101\n",
      "Epoch 137/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1500 - accuracy: 0.9397\n",
      "Epoch 138/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1549 - accuracy: 0.9373\n",
      "Epoch 139/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1275 - accuracy: 0.9507\n",
      "Epoch 140/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1334 - accuracy: 0.9500\n",
      "Epoch 141/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1847 - accuracy: 0.9286\n",
      "Epoch 142/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1544 - accuracy: 0.9374\n",
      "Epoch 143/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1251 - accuracy: 0.9516\n",
      "Epoch 144/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1718 - accuracy: 0.9283\n",
      "Epoch 145/280\n",
      "7000/7000 [==============================] - 0s 13us/sample - loss: 0.1497 - accuracy: 0.9406\n",
      "Epoch 146/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1773 - accuracy: 0.9317\n",
      "Epoch 147/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1544 - accuracy: 0.9379\n",
      "Epoch 148/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1517 - accuracy: 0.9386\n",
      "Epoch 149/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1382 - accuracy: 0.9437\n",
      "Epoch 150/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1519 - accuracy: 0.9377\n",
      "Epoch 151/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1366 - accuracy: 0.9461\n",
      "Epoch 152/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1523 - accuracy: 0.9373\n",
      "Epoch 153/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1298 - accuracy: 0.9490\n",
      "Epoch 154/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1538 - accuracy: 0.9360\n",
      "Epoch 155/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1220 - accuracy: 0.9516\n",
      "Epoch 156/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1363 - accuracy: 0.9400\n",
      "Epoch 157/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1083 - accuracy: 0.9606\n",
      "Epoch 158/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1791 - accuracy: 0.9316\n",
      "Epoch 159/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1260 - accuracy: 0.9511\n",
      "Epoch 160/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.1312 - accuracy: 0.9469\n",
      "Epoch 161/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1089 - accuracy: 0.9610\n",
      "Epoch 162/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1691 - accuracy: 0.9366\n",
      "Epoch 163/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1261 - accuracy: 0.9509\n",
      "Epoch 164/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1264 - accuracy: 0.9483\n",
      "Epoch 165/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1467 - accuracy: 0.9407\n",
      "Epoch 166/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1408 - accuracy: 0.9429\n",
      "Epoch 167/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1061 - accuracy: 0.9614\n",
      "Epoch 168/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1463 - accuracy: 0.9391\n",
      "Epoch 169/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1256 - accuracy: 0.9493\n",
      "Epoch 170/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1065 - accuracy: 0.9614\n",
      "Epoch 171/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0945 - accuracy: 0.9659\n",
      "Epoch 172/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.2281 - accuracy: 0.9220\n",
      "Epoch 173/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.1163 - accuracy: 0.9574\n",
      "Epoch 174/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1019 - accuracy: 0.9620\n",
      "Epoch 175/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0957 - accuracy: 0.9659\n",
      "Epoch 176/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0920 - accuracy: 0.9700\n",
      "Epoch 177/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1451 - accuracy: 0.9441\n",
      "Epoch 178/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1051 - accuracy: 0.9576\n",
      "Epoch 179/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1035 - accuracy: 0.9604\n",
      "Epoch 180/280\n",
      "7000/7000 [==============================] - 0s 16us/sample - loss: 0.3081 - accuracy: 0.9043\n",
      "Epoch 181/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1070 - accuracy: 0.9609\n",
      "Epoch 182/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0936 - accuracy: 0.9681\n",
      "Epoch 183/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0951 - accuracy: 0.9670\n",
      "Epoch 184/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1154 - accuracy: 0.9556\n",
      "Epoch 185/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1333 - accuracy: 0.9497\n",
      "Epoch 186/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0847 - accuracy: 0.9733\n",
      "Epoch 187/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1406 - accuracy: 0.9483\n",
      "Epoch 188/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0914 - accuracy: 0.9673\n",
      "Epoch 189/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1040 - accuracy: 0.9607\n",
      "Epoch 190/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1688 - accuracy: 0.9297\n",
      "Epoch 191/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0931 - accuracy: 0.9640\n",
      "Epoch 192/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1279 - accuracy: 0.9486\n",
      "Epoch 193/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0909 - accuracy: 0.9651\n",
      "Epoch 194/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1266 - accuracy: 0.9500\n",
      "Epoch 195/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.1313 - accuracy: 0.9443\n",
      "Epoch 196/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.0866 - accuracy: 0.9707\n",
      "Epoch 197/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0824 - accuracy: 0.9721\n",
      "Epoch 198/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0778 - accuracy: 0.9747\n",
      "Epoch 199/280\n",
      "7000/7000 [==============================] - 0s 13us/sample - loss: 0.1811 - accuracy: 0.9336\n",
      "Epoch 200/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0894 - accuracy: 0.9674\n",
      "Epoch 201/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0889 - accuracy: 0.9650\n",
      "Epoch 202/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0889 - accuracy: 0.9667\n",
      "Epoch 203/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.1008 - accuracy: 0.9603\n",
      "Epoch 204/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1577 - accuracy: 0.9323\n",
      "Epoch 205/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0953 - accuracy: 0.9630\n",
      "Epoch 206/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0785 - accuracy: 0.9729\n",
      "Epoch 207/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0871 - accuracy: 0.9660\n",
      "Epoch 208/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1019 - accuracy: 0.9581\n",
      "Epoch 209/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0870 - accuracy: 0.9683\n",
      "Epoch 210/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0921 - accuracy: 0.9626\n",
      "Epoch 211/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0949 - accuracy: 0.9606\n",
      "Epoch 212/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0765 - accuracy: 0.9731\n",
      "Epoch 213/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0826 - accuracy: 0.9669\n",
      "Epoch 214/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.1958 - accuracy: 0.9286\n",
      "Epoch 215/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0770 - accuracy: 0.9729\n",
      "Epoch 216/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.0694 - accuracy: 0.9770\n",
      "Epoch 217/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0674 - accuracy: 0.9779\n",
      "Epoch 218/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0957 - accuracy: 0.9619\n",
      "Epoch 219/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.2170 - accuracy: 0.9334\n",
      "Epoch 220/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0673 - accuracy: 0.9790\n",
      "Epoch 221/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1338 - accuracy: 0.9504\n",
      "Epoch 222/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0734 - accuracy: 0.9747\n",
      "Epoch 223/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0847 - accuracy: 0.9667\n",
      "Epoch 224/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0638 - accuracy: 0.9784\n",
      "Epoch 225/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0584 - accuracy: 0.9834\n",
      "Epoch 226/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1110 - accuracy: 0.9581\n",
      "Epoch 227/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1040 - accuracy: 0.9586\n",
      "Epoch 228/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0664 - accuracy: 0.9761\n",
      "Epoch 229/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0712 - accuracy: 0.9759\n",
      "Epoch 230/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1990 - accuracy: 0.9356\n",
      "Epoch 231/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0619 - accuracy: 0.9819\n",
      "Epoch 232/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.0586 - accuracy: 0.9827\n",
      "Epoch 233/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0808 - accuracy: 0.9703\n",
      "Epoch 234/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0617 - accuracy: 0.9807\n",
      "Epoch 235/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0699 - accuracy: 0.9731\n",
      "Epoch 236/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0572 - accuracy: 0.9814\n",
      "Epoch 237/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1719 - accuracy: 0.9430\n",
      "Epoch 238/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0741 - accuracy: 0.9711\n",
      "Epoch 239/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.0559 - accuracy: 0.9834\n",
      "Epoch 240/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.0500 - accuracy: 0.9863\n",
      "Epoch 241/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0536 - accuracy: 0.9839\n",
      "Epoch 242/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0497 - accuracy: 0.9851\n",
      "Epoch 243/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0468 - accuracy: 0.9876\n",
      "Epoch 244/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.2860 - accuracy: 0.9309\n",
      "Epoch 245/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0691 - accuracy: 0.9783\n",
      "Epoch 246/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0545 - accuracy: 0.9844\n",
      "Epoch 247/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0475 - accuracy: 0.9880\n",
      "Epoch 248/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0470 - accuracy: 0.9869\n",
      "Epoch 249/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0690 - accuracy: 0.9751\n",
      "Epoch 250/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.1325 - accuracy: 0.9596\n",
      "Epoch 251/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0578 - accuracy: 0.9806\n",
      "Epoch 252/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0710 - accuracy: 0.9716\n",
      "Epoch 253/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0937 - accuracy: 0.9609\n",
      "Epoch 254/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0754 - accuracy: 0.9681\n",
      "Epoch 255/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0612 - accuracy: 0.9783\n",
      "Epoch 256/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0475 - accuracy: 0.9860\n",
      "Epoch 257/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0436 - accuracy: 0.9886\n",
      "Epoch 258/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0417 - accuracy: 0.9897\n",
      "Epoch 259/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0453 - accuracy: 0.9864\n",
      "Epoch 260/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0557 - accuracy: 0.9826\n",
      "Epoch 261/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1732 - accuracy: 0.9396\n",
      "Epoch 262/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0512 - accuracy: 0.9851\n",
      "Epoch 263/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0417 - accuracy: 0.9903\n",
      "Epoch 264/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0418 - accuracy: 0.9889\n",
      "Epoch 265/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.1293 - accuracy: 0.9439\n",
      "Epoch 266/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.1435 - accuracy: 0.9580\n",
      "Epoch 267/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0495 - accuracy: 0.9861\n",
      "Epoch 268/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0405 - accuracy: 0.9900\n",
      "Epoch 269/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0495 - accuracy: 0.9849\n",
      "Epoch 270/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0501 - accuracy: 0.9833\n",
      "Epoch 271/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0393 - accuracy: 0.9897\n",
      "Epoch 272/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.0352 - accuracy: 0.9924\n",
      "Epoch 273/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0424 - accuracy: 0.9883\n",
      "Epoch 274/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.2041 - accuracy: 0.9324\n",
      "Epoch 275/280\n",
      "7000/7000 [==============================] - 0s 11us/sample - loss: 0.0490 - accuracy: 0.9853\n",
      "Epoch 276/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0378 - accuracy: 0.9916\n",
      "Epoch 277/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.0349 - accuracy: 0.9930\n",
      "Epoch 278/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0342 - accuracy: 0.9924\n",
      "Epoch 279/280\n",
      "7000/7000 [==============================] - 0s 12us/sample - loss: 0.0373 - accuracy: 0.9911\n",
      "Epoch 280/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.0319 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x286328b05c8>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam2.fit(X_train, y_train, batch_size = 500, epochs = 280, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows model overfitted to Train data , with accuracy reaching 0.9997 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 116us/sample - loss: 1.0806 - accuracy: 0.8040\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam2 = bankdata_model_nadam2.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[1.080607545932134, 0.804]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam2.metrics_names)\n",
    "print(eva_results_nadam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It can be seen that with Test data , accuracy drops , since model was overfitted to Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6  Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 33us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam2 = bankdata_model_nadam2.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 11us/sample\n",
      "[[8.4153205e-02]\n",
      " [2.9145772e-06]\n",
      " [4.8174892e-02]\n",
      " ...\n",
      " [8.4420468e-04]\n",
      " [5.6957185e-02]\n",
      " [8.3362943e-01]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam2 = bankdata_model_nadam2.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2431, 1: 569}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam2 = (Y_pred_value_nadam2 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam2)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_nadam2, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 55us/sample - loss: 1.0806 - accuracy: 0.8040\n",
      "Accuracy of Model with Nadam optimizer :0.804\n",
      "Recall_score: 0.48465266558966075\n",
      "Precision_score: 0.5272407732864675\n",
      "F-score: 0.505050505050505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2112,  269],\n",
       "       [ 319,  300]], dtype=int64)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 61us/sample - loss: 1.0806 - accuracy: 0.8040\n",
      "Accuracy of Model with Nadam optimizer :0.804\n",
      "Recall_score: 0.48465266558966075\n",
      "Precision_score: 0.5272407732864675\n",
      "F-score: 0.505050505050505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2112,  269],\n",
       "       [ 319,  300]], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam2.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam2)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam2)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam2)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam2 = confusion_matrix(y_test.values, Y_pred_value_class_nadam2)\n",
    "df_cm_nadam2 = pd.DataFrame(cm_nadam2, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam2, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 6,'Nadam',5,128,'relu',64,'LeakyReLU',32,'LeakyReLU',16,'LeakyReLU',1,'sigmoid', \n",
    "                   bankdata_model_nadam2.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam2.history.history['accuracy'][-1], eva_results_nadam2[0],eva_results_nadam2[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam2), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam2), f1_score(y_test.values,Y_pred_value_class_nadam2)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.845069</td>\n",
       "      <td>0.809667</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.541237</td>\n",
       "      <td>0.524563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.993429</td>\n",
       "      <td>1.080608</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "3     No layer              0     No layer         0.325063   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "5    LeakyReLU              1      sigmoid         0.004469   \n",
       "6    LeakyReLU              1      sigmoid         0.031885   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "5             0.999143        1.845069            0.809667      0.508885   \n",
       "6             0.993429        1.080608            0.804000      0.484653   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  \n",
       "3         0.782609  0.583587  \n",
       "4         0.736973  0.581213  \n",
       "5         0.541237  0.524563  \n",
       "6         0.527241  0.505051  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Case 7 using NAdam optimizer  with LeakyRelu activation function , with 4 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam3 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model , with total 4 layers. Here we add LeakyReLU as a layer since advanced activation is to be added as layer and not called as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam3.add(Dense(64, input_shape = (26,), activation = 'relu'))\n",
    "bankdata_model_nadam3.add(Dense(32, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam3.add(Dense(16, LeakyReLU(alpha=0.3)))  # changed alpha to 0.3 from 0.1\n",
    "bankdata_model_nadam3.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam3 = optimizers.Nadam(lr = 0.001, beta_1 = 0.8) ## use beta_1=0.8 (default=0.9) , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam3.compile(optimizer = gd_optimizer_nadam3, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/280\n",
      "7000/7000 [==============================] - 1s 108us/sample - loss: 0.6053 - accuracy: 0.7210\n",
      "Epoch 2/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4715 - accuracy: 0.7974\n",
      "Epoch 3/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.4257 - accuracy: 0.8029\n",
      "Epoch 4/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.4026 - accuracy: 0.8167\n",
      "Epoch 5/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3876 - accuracy: 0.8266\n",
      "Epoch 6/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3775 - accuracy: 0.8361\n",
      "Epoch 7/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3698 - accuracy: 0.8409\n",
      "Epoch 8/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3629 - accuracy: 0.8459\n",
      "Epoch 9/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3580 - accuracy: 0.8489\n",
      "Epoch 10/280\n",
      "7000/7000 [==============================] - 0s 14us/sample - loss: 0.3535 - accuracy: 0.8513\n",
      "Epoch 11/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3493 - accuracy: 0.8544\n",
      "Epoch 12/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3465 - accuracy: 0.8573\n",
      "Epoch 13/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3428 - accuracy: 0.8574\n",
      "Epoch 14/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3403 - accuracy: 0.8603\n",
      "Epoch 15/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3374 - accuracy: 0.8616\n",
      "Epoch 16/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3356 - accuracy: 0.8621\n",
      "Epoch 17/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3331 - accuracy: 0.8631\n",
      "Epoch 18/280\n",
      "7000/7000 [==============================] - 0s 10us/sample - loss: 0.3310 - accuracy: 0.8649\n",
      "Epoch 19/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3291 - accuracy: 0.8666\n",
      "Epoch 20/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3271 - accuracy: 0.8657\n",
      "Epoch 21/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3250 - accuracy: 0.8670\n",
      "Epoch 22/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3236 - accuracy: 0.8666\n",
      "Epoch 23/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3227 - accuracy: 0.8680\n",
      "Epoch 24/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3210 - accuracy: 0.8670\n",
      "Epoch 25/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3189 - accuracy: 0.8703\n",
      "Epoch 26/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.3178 - accuracy: 0.8686\n",
      "Epoch 27/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3166 - accuracy: 0.8679\n",
      "Epoch 28/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3155 - accuracy: 0.8691\n",
      "Epoch 29/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3130 - accuracy: 0.8699\n",
      "Epoch 30/280\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3120 - accuracy: 0.8709\n",
      "Epoch 31/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3108 - accuracy: 0.8734\n",
      "Epoch 32/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3090 - accuracy: 0.8719\n",
      "Epoch 33/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3069 - accuracy: 0.8726\n",
      "Epoch 34/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3070 - accuracy: 0.8723\n",
      "Epoch 35/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3057 - accuracy: 0.8740\n",
      "Epoch 36/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3037 - accuracy: 0.8761\n",
      "Epoch 37/280\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.89 - 0s 5us/sample - loss: 0.3033 - accuracy: 0.8749\n",
      "Epoch 38/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.3022 - accuracy: 0.8731\n",
      "Epoch 39/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3013 - accuracy: 0.8749\n",
      "Epoch 40/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2987 - accuracy: 0.8776\n",
      "Epoch 41/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2981 - accuracy: 0.8754\n",
      "Epoch 42/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2986 - accuracy: 0.8770\n",
      "Epoch 43/280\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - 0s 7us/sample - loss: 0.2994 - accuracy: 0.8734\n",
      "Epoch 44/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2974 - accuracy: 0.8769\n",
      "Epoch 45/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2955 - accuracy: 0.8774\n",
      "Epoch 46/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2954 - accuracy: 0.8786\n",
      "Epoch 47/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2930 - accuracy: 0.8779\n",
      "Epoch 48/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2919 - accuracy: 0.8801\n",
      "Epoch 49/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2947 - accuracy: 0.8754\n",
      "Epoch 50/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2903 - accuracy: 0.8813\n",
      "Epoch 51/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2884 - accuracy: 0.8803\n",
      "Epoch 52/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2891 - accuracy: 0.8803\n",
      "Epoch 53/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2885 - accuracy: 0.8796\n",
      "Epoch 54/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2865 - accuracy: 0.8794\n",
      "Epoch 55/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2853 - accuracy: 0.8823\n",
      "Epoch 56/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2882 - accuracy: 0.8790\n",
      "Epoch 57/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2851 - accuracy: 0.8820\n",
      "Epoch 58/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2829 - accuracy: 0.8831\n",
      "Epoch 59/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2843 - accuracy: 0.8837\n",
      "Epoch 60/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2825 - accuracy: 0.8826\n",
      "Epoch 61/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2811 - accuracy: 0.8853\n",
      "Epoch 62/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2843 - accuracy: 0.8829\n",
      "Epoch 63/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2806 - accuracy: 0.8860\n",
      "Epoch 64/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2826 - accuracy: 0.8821\n",
      "Epoch 65/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2800 - accuracy: 0.8824\n",
      "Epoch 66/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2786 - accuracy: 0.8866\n",
      "Epoch 67/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2778 - accuracy: 0.8841\n",
      "Epoch 68/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2773 - accuracy: 0.8856\n",
      "Epoch 69/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2754 - accuracy: 0.8857\n",
      "Epoch 70/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2756 - accuracy: 0.8881\n",
      "Epoch 71/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2722 - accuracy: 0.8896\n",
      "Epoch 72/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2744 - accuracy: 0.8867\n",
      "Epoch 73/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2737 - accuracy: 0.8903\n",
      "Epoch 74/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2712 - accuracy: 0.8881\n",
      "Epoch 75/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2714 - accuracy: 0.8881\n",
      "Epoch 76/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2735 - accuracy: 0.8853\n",
      "Epoch 77/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2725 - accuracy: 0.8883\n",
      "Epoch 78/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2697 - accuracy: 0.8877\n",
      "Epoch 79/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2720 - accuracy: 0.8880\n",
      "Epoch 80/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2679 - accuracy: 0.8924\n",
      "Epoch 81/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2720 - accuracy: 0.8900\n",
      "Epoch 82/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2684 - accuracy: 0.8917\n",
      "Epoch 83/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2637 - accuracy: 0.8909\n",
      "Epoch 84/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2672 - accuracy: 0.8893\n",
      "Epoch 85/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2620 - accuracy: 0.8954\n",
      "Epoch 86/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2631 - accuracy: 0.8951\n",
      "Epoch 87/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2638 - accuracy: 0.8923\n",
      "Epoch 88/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2602 - accuracy: 0.8933\n",
      "Epoch 89/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2601 - accuracy: 0.8926\n",
      "Epoch 90/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2604 - accuracy: 0.8907\n",
      "Epoch 91/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2724 - accuracy: 0.8877\n",
      "Epoch 92/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2603 - accuracy: 0.8937\n",
      "Epoch 93/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2623 - accuracy: 0.8934\n",
      "Epoch 94/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2599 - accuracy: 0.8957\n",
      "Epoch 95/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2590 - accuracy: 0.8944\n",
      "Epoch 96/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2651 - accuracy: 0.8901\n",
      "Epoch 97/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2577 - accuracy: 0.8951\n",
      "Epoch 98/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2568 - accuracy: 0.8966\n",
      "Epoch 99/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2569 - accuracy: 0.8961\n",
      "Epoch 100/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2560 - accuracy: 0.8946\n",
      "Epoch 101/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2541 - accuracy: 0.8951\n",
      "Epoch 102/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2563 - accuracy: 0.8941\n",
      "Epoch 103/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2533 - accuracy: 0.8974\n",
      "Epoch 104/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2507 - accuracy: 0.8980\n",
      "Epoch 105/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2505 - accuracy: 0.8987\n",
      "Epoch 106/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2552 - accuracy: 0.8961\n",
      "Epoch 107/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2514 - accuracy: 0.8994\n",
      "Epoch 108/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2486 - accuracy: 0.8971\n",
      "Epoch 109/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2469 - accuracy: 0.9006\n",
      "Epoch 110/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2559 - accuracy: 0.8954\n",
      "Epoch 111/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2499 - accuracy: 0.8971\n",
      "Epoch 112/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2577 - accuracy: 0.8936\n",
      "Epoch 113/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2515 - accuracy: 0.8960\n",
      "Epoch 114/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2490 - accuracy: 0.8989\n",
      "Epoch 115/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2497 - accuracy: 0.8993\n",
      "Epoch 116/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2484 - accuracy: 0.8974\n",
      "Epoch 117/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2452 - accuracy: 0.9006\n",
      "Epoch 118/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2414 - accuracy: 0.9017\n",
      "Epoch 119/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2475 - accuracy: 0.8983\n",
      "Epoch 120/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2522 - accuracy: 0.8951\n",
      "Epoch 121/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2395 - accuracy: 0.9040\n",
      "Epoch 122/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2396 - accuracy: 0.9039\n",
      "Epoch 123/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2379 - accuracy: 0.9037\n",
      "Epoch 124/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2500 - accuracy: 0.8984\n",
      "Epoch 125/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2432 - accuracy: 0.9014\n",
      "Epoch 126/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2370 - accuracy: 0.9031\n",
      "Epoch 127/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2369 - accuracy: 0.9040\n",
      "Epoch 128/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2372 - accuracy: 0.9034\n",
      "Epoch 129/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2426 - accuracy: 0.9023\n",
      "Epoch 130/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2399 - accuracy: 0.9021\n",
      "Epoch 131/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2426 - accuracy: 0.8973\n",
      "Epoch 132/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2441 - accuracy: 0.9001\n",
      "Epoch 133/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2337 - accuracy: 0.9057\n",
      "Epoch 134/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2425 - accuracy: 0.9021\n",
      "Epoch 135/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2355 - accuracy: 0.9041\n",
      "Epoch 136/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2331 - accuracy: 0.9064\n",
      "Epoch 137/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2290 - accuracy: 0.9090\n",
      "Epoch 138/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2354 - accuracy: 0.9050\n",
      "Epoch 139/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2324 - accuracy: 0.9066\n",
      "Epoch 140/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2323 - accuracy: 0.9063\n",
      "Epoch 141/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2315 - accuracy: 0.9040\n",
      "Epoch 142/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2280 - accuracy: 0.9084\n",
      "Epoch 143/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2288 - accuracy: 0.9080\n",
      "Epoch 144/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2408 - accuracy: 0.8999\n",
      "Epoch 145/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2333 - accuracy: 0.9029\n",
      "Epoch 146/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2250 - accuracy: 0.9106\n",
      "Epoch 147/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2378 - accuracy: 0.9024\n",
      "Epoch 148/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2293 - accuracy: 0.9054\n",
      "Epoch 149/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2251 - accuracy: 0.9081\n",
      "Epoch 150/280\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.2240 - accuracy: 0.9091\n",
      "Epoch 151/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2199 - accuracy: 0.9130\n",
      "Epoch 152/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2401 - accuracy: 0.9001\n",
      "Epoch 153/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2265 - accuracy: 0.9110\n",
      "Epoch 154/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2193 - accuracy: 0.9129\n",
      "Epoch 155/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2202 - accuracy: 0.9116\n",
      "Epoch 156/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2251 - accuracy: 0.9096\n",
      "Epoch 157/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2216 - accuracy: 0.9103\n",
      "Epoch 158/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2207 - accuracy: 0.9101\n",
      "Epoch 159/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2182 - accuracy: 0.9104\n",
      "Epoch 160/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2184 - accuracy: 0.9133\n",
      "Epoch 161/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2207 - accuracy: 0.9094\n",
      "Epoch 162/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2178 - accuracy: 0.9127\n",
      "Epoch 163/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2159 - accuracy: 0.9111\n",
      "Epoch 164/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2294 - accuracy: 0.9031\n",
      "Epoch 165/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2264 - accuracy: 0.9091\n",
      "Epoch 166/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2492 - accuracy: 0.8943\n",
      "Epoch 167/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2205 - accuracy: 0.9100\n",
      "Epoch 168/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2224 - accuracy: 0.9099\n",
      "Epoch 169/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2154 - accuracy: 0.9133\n",
      "Epoch 170/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2146 - accuracy: 0.9114\n",
      "Epoch 171/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2106 - accuracy: 0.9146\n",
      "Epoch 172/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2101 - accuracy: 0.9154\n",
      "Epoch 173/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2119 - accuracy: 0.9129\n",
      "Epoch 174/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2208 - accuracy: 0.9074\n",
      "Epoch 175/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2112 - accuracy: 0.9139\n",
      "Epoch 176/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2159 - accuracy: 0.9130\n",
      "Epoch 177/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2121 - accuracy: 0.9126\n",
      "Epoch 178/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2097 - accuracy: 0.9150\n",
      "Epoch 179/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2194 - accuracy: 0.9063\n",
      "Epoch 180/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2197 - accuracy: 0.9066\n",
      "Epoch 181/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2134 - accuracy: 0.9103\n",
      "Epoch 182/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2155 - accuracy: 0.9107\n",
      "Epoch 183/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2049 - accuracy: 0.9163\n",
      "Epoch 184/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2177 - accuracy: 0.9093\n",
      "Epoch 185/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2148 - accuracy: 0.9117\n",
      "Epoch 186/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2072 - accuracy: 0.9153\n",
      "Epoch 187/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2056 - accuracy: 0.9160\n",
      "Epoch 188/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2102 - accuracy: 0.9143\n",
      "Epoch 189/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2042 - accuracy: 0.9179\n",
      "Epoch 190/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2037 - accuracy: 0.9210\n",
      "Epoch 191/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2030 - accuracy: 0.9174\n",
      "Epoch 192/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2010 - accuracy: 0.9189\n",
      "Epoch 193/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2064 - accuracy: 0.9154\n",
      "Epoch 194/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2067 - accuracy: 0.9139\n",
      "Epoch 195/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2017 - accuracy: 0.9196\n",
      "Epoch 196/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2038 - accuracy: 0.9184\n",
      "Epoch 197/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2011 - accuracy: 0.9170\n",
      "Epoch 198/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2096 - accuracy: 0.9137\n",
      "Epoch 199/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2055 - accuracy: 0.9173\n",
      "Epoch 200/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2026 - accuracy: 0.9167\n",
      "Epoch 201/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1979 - accuracy: 0.9209\n",
      "Epoch 202/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2016 - accuracy: 0.9174\n",
      "Epoch 203/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2034 - accuracy: 0.9180\n",
      "Epoch 204/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2065 - accuracy: 0.9163\n",
      "Epoch 205/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2017 - accuracy: 0.9183\n",
      "Epoch 206/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2008 - accuracy: 0.9199\n",
      "Epoch 207/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2047 - accuracy: 0.9173\n",
      "Epoch 208/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1997 - accuracy: 0.9187\n",
      "Epoch 209/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1942 - accuracy: 0.9214\n",
      "Epoch 210/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1980 - accuracy: 0.9176\n",
      "Epoch 211/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2062 - accuracy: 0.9139\n",
      "Epoch 212/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2091 - accuracy: 0.9141\n",
      "Epoch 213/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1974 - accuracy: 0.9184\n",
      "Epoch 214/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1927 - accuracy: 0.9206\n",
      "Epoch 215/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1930 - accuracy: 0.9237\n",
      "Epoch 216/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2099 - accuracy: 0.9110\n",
      "Epoch 217/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1939 - accuracy: 0.9209\n",
      "Epoch 218/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.2004 - accuracy: 0.9166\n",
      "Epoch 219/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.2032 - accuracy: 0.9171\n",
      "Epoch 220/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1976 - accuracy: 0.9177\n",
      "Epoch 221/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1904 - accuracy: 0.9227\n",
      "Epoch 222/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1893 - accuracy: 0.9244\n",
      "Epoch 223/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1937 - accuracy: 0.9213\n",
      "Epoch 224/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1899 - accuracy: 0.9221\n",
      "Epoch 225/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1942 - accuracy: 0.9199\n",
      "Epoch 226/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1880 - accuracy: 0.9226\n",
      "Epoch 227/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1989 - accuracy: 0.9141\n",
      "Epoch 228/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.2041 - accuracy: 0.9141\n",
      "Epoch 229/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.2021 - accuracy: 0.9194\n",
      "Epoch 230/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1881 - accuracy: 0.9250\n",
      "Epoch 231/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1877 - accuracy: 0.9234\n",
      "Epoch 232/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1983 - accuracy: 0.9186\n",
      "Epoch 233/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1887 - accuracy: 0.9236\n",
      "Epoch 234/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1902 - accuracy: 0.9240\n",
      "Epoch 235/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1848 - accuracy: 0.9254\n",
      "Epoch 236/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1909 - accuracy: 0.9203\n",
      "Epoch 237/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1880 - accuracy: 0.9240\n",
      "Epoch 238/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1927 - accuracy: 0.9173\n",
      "Epoch 239/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1873 - accuracy: 0.9207\n",
      "Epoch 240/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1831 - accuracy: 0.9264\n",
      "Epoch 241/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1823 - accuracy: 0.9259\n",
      "Epoch 242/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1907 - accuracy: 0.9219\n",
      "Epoch 243/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1851 - accuracy: 0.9244\n",
      "Epoch 244/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1840 - accuracy: 0.9246\n",
      "Epoch 245/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1871 - accuracy: 0.9240\n",
      "Epoch 246/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1804 - accuracy: 0.9274\n",
      "Epoch 247/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1936 - accuracy: 0.9193\n",
      "Epoch 248/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1988 - accuracy: 0.9149\n",
      "Epoch 249/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1858 - accuracy: 0.9254\n",
      "Epoch 250/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1995 - accuracy: 0.9177\n",
      "Epoch 251/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1843 - accuracy: 0.9259\n",
      "Epoch 252/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1824 - accuracy: 0.9240\n",
      "Epoch 253/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1843 - accuracy: 0.9246\n",
      "Epoch 254/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1752 - accuracy: 0.9294\n",
      "Epoch 255/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1761 - accuracy: 0.9291\n",
      "Epoch 256/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1844 - accuracy: 0.9241\n",
      "Epoch 257/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1845 - accuracy: 0.9253\n",
      "Epoch 258/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1828 - accuracy: 0.9257\n",
      "Epoch 259/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1894 - accuracy: 0.9186\n",
      "Epoch 260/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1746 - accuracy: 0.9281\n",
      "Epoch 261/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1770 - accuracy: 0.9286\n",
      "Epoch 262/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1811 - accuracy: 0.9239\n",
      "Epoch 263/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1821 - accuracy: 0.9244\n",
      "Epoch 264/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1841 - accuracy: 0.9261\n",
      "Epoch 265/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1764 - accuracy: 0.9291\n",
      "Epoch 266/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1720 - accuracy: 0.9281\n",
      "Epoch 267/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1783 - accuracy: 0.9256\n",
      "Epoch 268/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1754 - accuracy: 0.9281\n",
      "Epoch 269/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1908 - accuracy: 0.9199\n",
      "Epoch 270/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1703 - accuracy: 0.9326\n",
      "Epoch 271/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1725 - accuracy: 0.9317\n",
      "Epoch 272/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1689 - accuracy: 0.9324\n",
      "Epoch 273/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1794 - accuracy: 0.9236\n",
      "Epoch 274/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1781 - accuracy: 0.9271\n",
      "Epoch 275/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1889 - accuracy: 0.9176\n",
      "Epoch 276/280\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.1673 - accuracy: 0.9341\n",
      "Epoch 277/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1708 - accuracy: 0.9324\n",
      "Epoch 278/280\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.1754 - accuracy: 0.9291\n",
      "Epoch 279/280\n",
      "7000/7000 [==============================] - 0s 5us/sample - loss: 0.1747 - accuracy: 0.9279\n",
      "Epoch 280/280\n",
      "7000/7000 [==============================] - 0s 4us/sample - loss: 0.1700 - accuracy: 0.9287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28633b3df48>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam3.fit(X_train, y_train, batch_size = 500, epochs = 280, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 83us/sample - loss: 0.5479 - accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam3 = bankdata_model_nadam3.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.5479014535744985, 0.823]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam3.metrics_names)\n",
    "print(eva_results_nadam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7  Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 23us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam3 = bankdata_model_nadam3.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 6us/sample\n",
      "[[0.06079349]\n",
      " [0.00165664]\n",
      " [0.67682034]\n",
      " ...\n",
      " [0.00151679]\n",
      " [0.00511691]\n",
      " [0.52763903]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam3 = bankdata_model_nadam3.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "The number of predicted 1 (Exited) and 0 ( Not Exited) are :\n",
      " {0: 2480, 1: 520}\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam3 = (Y_pred_value_nadam3 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam3)\n",
    "\n",
    "# Print predicted 1 and 0's \n",
    "unique, counts = np.unique(Y_pred_value_class_nadam3, return_counts = True)\n",
    "print('\\nThe number of predicted 1 (Exited) and 0 ( Not Exited) are :\\n', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.5479 - accuracy: 0.8230\n",
      "Accuracy of Model with Nadam optimizer :0.823\n",
      "Recall_score: 0.4911147011308562\n",
      "Precision_score: 0.5846153846153846\n",
      "F-score: 0.5338015803336259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2165,  216],\n",
       "       [ 315,  304]], dtype=int64)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.5479 - accuracy: 0.8230\n",
      "Accuracy of Model with Nadam optimizer :0.823\n",
      "Recall_score: 0.4911147011308562\n",
      "Precision_score: 0.5846153846153846\n",
      "F-score: 0.5338015803336259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2165,  216],\n",
       "       [ 315,  304]], dtype=int64)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam3.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam3)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam3)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam3)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam3 = confusion_matrix(y_test.values, Y_pred_value_class_nadam3)\n",
    "df_cm_nadam3 = pd.DataFrame(cm_nadam3, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam3, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 7,'Nadam',4,64,'relu',32,'LeakyReLU',16,'LeakyReLU',1,'sigmoid',0,'No layer', \n",
    "                   bankdata_model_nadam3.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam3.history.history['accuracy'][-1], eva_results_nadam3[0],eva_results_nadam3[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam3), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam3), f1_score(y_test.values,Y_pred_value_class_nadam3)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.845069</td>\n",
       "      <td>0.809667</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.541237</td>\n",
       "      <td>0.524563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.993429</td>\n",
       "      <td>1.080608</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.505051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.547901</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.533802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "3     No layer              0     No layer         0.325063   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "5    LeakyReLU              1      sigmoid         0.004469   \n",
       "6    LeakyReLU              1      sigmoid         0.031885   \n",
       "7      sigmoid              0     No layer         0.169967   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "5             0.999143        1.845069            0.809667      0.508885   \n",
       "6             0.993429        1.080608            0.804000      0.484653   \n",
       "7             0.928714        0.547901            0.823000      0.491115   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  \n",
       "3         0.782609  0.583587  \n",
       "4         0.736973  0.581213  \n",
       "5         0.541237  0.524563  \n",
       "6         0.527241  0.505051  \n",
       "7         0.584615  0.533802  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.547901</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.533802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.845069</td>\n",
       "      <td>0.809667</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.541237</td>\n",
       "      <td>0.524563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.993429</td>\n",
       "      <td>1.080608</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "3       3          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "1       1          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "1             32         relu              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "3     No layer              0     No layer         0.325063   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "1     No layer              0     No layer         0.324317   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "7      sigmoid              0     No layer         0.169967   \n",
       "5    LeakyReLU              1      sigmoid         0.004469   \n",
       "6    LeakyReLU              1      sigmoid         0.031885   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "7             0.928714        0.547901            0.823000      0.491115   \n",
       "5             0.999143        1.845069            0.809667      0.508885   \n",
       "6             0.993429        1.080608            0.804000      0.484653   \n",
       "\n",
       "   Precision score   F score  \n",
       "3         0.782609  0.583587  \n",
       "2         0.772118  0.580645  \n",
       "1         0.778090  0.568205  \n",
       "4         0.736973  0.581213  \n",
       "7         0.584615  0.533802  \n",
       "5         0.541237  0.524563  \n",
       "6         0.527241  0.505051  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df1 = results_df.sort_values(by = ['Test data-Accuracy', 'F score'], ascending = False)\n",
    "results_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8  Case 8 using Nadam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam4 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam4.add(Dense(64, input_shape = (26,), activation = 'elu'))\n",
    "bankdata_model_nadam4.add(Dense(32, activation = 'selu'))\n",
    "bankdata_model_nadam4.add(Dense(1, activation = 'sigmoid'))  # sigmoid since we are classifying the data , to find out 'churn' possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_optimizer_nadam4 = optimizers.Nadam(lr = 0.001) ## use beta_1 , beta_2 , epsilon as deafults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_model_nadam4.compile(optimizer = gd_optimizer_nadam4, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 64)                1728      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bankdata_model_nadam4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with forward pass and backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 1s 88us/sample - loss: 0.5025 - accuracy: 0.7730\n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.4178 - accuracy: 0.8121\n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3957 - accuracy: 0.8240\n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3842 - accuracy: 0.8277\n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3780 - accuracy: 0.8330\n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3731 - accuracy: 0.8361\n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3696 - accuracy: 0.8387\n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3668 - accuracy: 0.8407\n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3645 - accuracy: 0.8436\n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3628 - accuracy: 0.8441\n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 0s 9us/sample - loss: 0.3608 - accuracy: 0.8450\n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3591 - accuracy: 0.8479\n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3577 - accuracy: 0.8491\n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3562 - accuracy: 0.8510\n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3549 - accuracy: 0.8521\n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3538 - accuracy: 0.8517\n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3525 - accuracy: 0.8543\n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 0s 7us/sample - loss: 0.3513 - accuracy: 0.8534\n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3501 - accuracy: 0.8567\n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3490 - accuracy: 0.8574\n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3481 - accuracy: 0.8564\n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3470 - accuracy: 0.8563\n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3459 - accuracy: 0.8567\n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3454 - accuracy: 0.8559\n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3447 - accuracy: 0.8573\n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3432 - accuracy: 0.8580\n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3424 - accuracy: 0.8590\n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3418 - accuracy: 0.8591\n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3414 - accuracy: 0.8597\n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3398 - accuracy: 0.8591\n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3395 - accuracy: 0.8596\n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3386 - accuracy: 0.8613\n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3376 - accuracy: 0.8593\n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3380 - accuracy: 0.8604\n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3371 - accuracy: 0.8607\n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3361 - accuracy: 0.8613\n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 0s 8us/sample - loss: 0.3359 - accuracy: 0.8621\n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3350 - accuracy: 0.8634\n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3346 - accuracy: 0.8624\n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 0s 6us/sample - loss: 0.3341 - accuracy: 0.8629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28631272988>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_model_nadam4.fit(X_train, y_train, batch_size = 500, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 81us/sample - loss: 0.3587 - accuracy: 0.8547\n"
     ]
    }
   ],
   "source": [
    "eva_results_nadam4 = bankdata_model_nadam4.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.3587287753423055, 0.85466665]\n"
     ]
    }
   ],
   "source": [
    "print(bankdata_model_nadam4.metrics_names)\n",
    "print(eva_results_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Prediting using classes and probability , with threshold of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1- using predict_classes ( without using threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 23us/sample\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_class_nadam4 = bankdata_model_nadam4.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_class_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - using predict to get predicted values of 'Exited' , based on which threshold of 0.5 can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 11us/sample\n",
      "[[0.20049213]\n",
      " [0.01622402]\n",
      " [0.7731446 ]\n",
      " ...\n",
      " [0.12699987]\n",
      " [0.04603673]\n",
      " [0.11327477]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_nadam4 = bankdata_model_nadam4.predict(X_test, batch_size=200, verbose=1)\n",
    "print(Y_pred_value_nadam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_value_class_nadam4 = (Y_pred_value_nadam4 > 0.5).astype(int)\n",
    "print(Y_pred_value_class_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Printing Accuracy scores and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict_classes method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.3587 - accuracy: 0.8547\n",
      "Accuracy of Model with Nadam optimizer :0.85466665\n",
      "Recall_score: 0.4717285945072698\n",
      "Precision_score: 0.7281795511221946\n",
      "F-score: 0.5725490196078432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2272,  109],\n",
       "       [ 327,  292]], dtype=int64)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_class_nadam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_class_nadam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_class_nadam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_class_nadam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion metrics - when using predict method that gives values , and where threshold of 0.5 has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.3587 - accuracy: 0.8547\n",
      "Accuracy of Model with Nadam optimizer :0.85466665\n",
      "Recall_score: 0.4717285945072698\n",
      "Precision_score: 0.7281795511221946\n",
      "F-score: 0.5725490196078432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2272,  109],\n",
       "       [ 327,  292]], dtype=int64)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred_value_class_nadam = bankdata_model.predict_classes(X_test, batch_size=200, verbose=1) # obtained earlier\n",
    "print('Accuracy of Model with Nadam optimizer :'+ str(bankdata_model_nadam4.evaluate(X_test,y_test.values)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test.values,Y_pred_value_class_nadam4)))\n",
    "print('Precision_score: ' + str(precision_score(y_test.values, Y_pred_value_class_nadam4)))\n",
    "print('F-score: ' + str(f1_score(y_test.values,Y_pred_value_class_nadam4)))\n",
    "confusion_matrix(y_test.values, Y_pred_value_class_nadam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_nadam4 = confusion_matrix(y_test.values, Y_pred_value_class_nadam4)\n",
    "df_cm_nadam4 = pd.DataFrame(cm_nadam4, index = [i for i in [\"True 0\", \"True 1\"]],\n",
    "                     columns = [i for i in [\"Predict 0\", \"Predict 1\"]] )\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm_nadam4, annot=True, fmt = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = row_index + 1\n",
    "\n",
    "results_df_data = [ 8,'Nadam',3,64,'elu',32,'selu',1,'sigmoid', 0, 'No layer', 0,'No layer',\n",
    "                   bankdata_model_nadam4.history.history['loss'][-1],\n",
    "                            bankdata_model_nadam4.history.history['accuracy'][-1], eva_results_nadam4[0],eva_results_nadam4[1], \n",
    "                            recall_score(y_test.values,Y_pred_value_class_nadam4), precision_score(y_test.values, \n",
    "                            Y_pred_value_class_nadam4), f1_score(y_test.values,Y_pred_value_class_nadam4)]\n",
    "results_df.loc[row_index] = results_df_data\n",
    "#results_df = results_df.drop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.845069</td>\n",
       "      <td>0.809667</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.541237</td>\n",
       "      <td>0.524563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.993429</td>\n",
       "      <td>1.080608</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.505051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.547901</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.533802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>selu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.334057</td>\n",
       "      <td>0.862857</td>\n",
       "      <td>0.358729</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>0.471729</td>\n",
       "      <td>0.728180</td>\n",
       "      <td>0.572549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "1       1          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "3       3          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "8       8         Nadam              3             64          elu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "1             32         relu              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "8             32         selu              1      sigmoid              0   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "1     No layer              0     No layer         0.324317   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "3     No layer              0     No layer         0.325063   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "5    LeakyReLU              1      sigmoid         0.004469   \n",
       "6    LeakyReLU              1      sigmoid         0.031885   \n",
       "7      sigmoid              0     No layer         0.169967   \n",
       "8     No layer              0     No layer         0.334057   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "5             0.999143        1.845069            0.809667      0.508885   \n",
       "6             0.993429        1.080608            0.804000      0.484653   \n",
       "7             0.928714        0.547901            0.823000      0.491115   \n",
       "8             0.862857        0.358729            0.854667      0.471729   \n",
       "\n",
       "   Precision score   F score  \n",
       "1         0.778090  0.568205  \n",
       "2         0.772118  0.580645  \n",
       "3         0.782609  0.583587  \n",
       "4         0.736973  0.581213  \n",
       "5         0.541237  0.524563  \n",
       "6         0.527241  0.505051  \n",
       "7         0.584615  0.533802  \n",
       "8         0.728180  0.572549  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case X</th>\n",
       "      <th>Model details</th>\n",
       "      <th>Num of layers</th>\n",
       "      <th>Layer 1 nodes</th>\n",
       "      <th>Layer1 Act F</th>\n",
       "      <th>Layer 2 nodes</th>\n",
       "      <th>Layer2 Act F</th>\n",
       "      <th>Layer 3 nodes</th>\n",
       "      <th>Layer3 Act F</th>\n",
       "      <th>Layer 4 nodes</th>\n",
       "      <th>Layer4 Act F</th>\n",
       "      <th>Layer 5 nodes</th>\n",
       "      <th>Layer5 Act F</th>\n",
       "      <th>Train data-loss</th>\n",
       "      <th>Train data-Accuracy</th>\n",
       "      <th>Test data-Loss</th>\n",
       "      <th>Test data-Accuracy</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>F score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.350275</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.583587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.321304</td>\n",
       "      <td>0.868857</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.324317</td>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.778090</td>\n",
       "      <td>0.568205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.479806</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.581213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>32</td>\n",
       "      <td>selu</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.334057</td>\n",
       "      <td>0.862857</td>\n",
       "      <td>0.358729</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>0.471729</td>\n",
       "      <td>0.728180</td>\n",
       "      <td>0.572549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>No layer</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.547901</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.533802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.845069</td>\n",
       "      <td>0.809667</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.541237</td>\n",
       "      <td>0.524563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>32</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>16</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.993429</td>\n",
       "      <td>1.080608</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case X Model details  Num of layers  Layer 1 nodes Layer1 Act F  \\\n",
       "3       3          Adam              3             64          elu   \n",
       "2       2         Nadam              3             64          elu   \n",
       "1       1          Adam              3             64          elu   \n",
       "4       4          Adam              3             64         relu   \n",
       "8       8         Nadam              3             64          elu   \n",
       "7       7         Nadam              4             64         relu   \n",
       "5       5          Adam              5            128         relu   \n",
       "6       6         Nadam              5            128         relu   \n",
       "\n",
       "   Layer 2 nodes Layer2 Act F  Layer 3 nodes Layer3 Act F  Layer 4 nodes  \\\n",
       "3             32    LeakyReLU              1      sigmoid              0   \n",
       "2             32         relu              1      sigmoid              0   \n",
       "1             32         relu              1      sigmoid              0   \n",
       "4             32    LeakyReLU              1      sigmoid              0   \n",
       "8             32         selu              1      sigmoid              0   \n",
       "7             32    LeakyReLU             16    LeakyReLU              1   \n",
       "5             64    LeakyReLU             32    LeakyReLU             16   \n",
       "6             64    LeakyReLU             32    LeakyReLU             16   \n",
       "\n",
       "  Layer4 Act F  Layer 5 nodes Layer5 Act F  Train data-loss  \\\n",
       "3     No layer              0     No layer         0.325063   \n",
       "2     No layer              0     No layer         0.321304   \n",
       "1     No layer              0     No layer         0.324317   \n",
       "4     No layer              0     No layer         0.301872   \n",
       "8     No layer              0     No layer         0.334057   \n",
       "7      sigmoid              0     No layer         0.169967   \n",
       "5    LeakyReLU              1      sigmoid         0.004469   \n",
       "6    LeakyReLU              1      sigmoid         0.031885   \n",
       "\n",
       "   Train data-Accuracy  Test data-Loss  Test data-Accuracy  Recall score  \\\n",
       "3             0.865714        0.350275            0.863000      0.465267   \n",
       "2             0.868857        0.350789            0.861333      0.465267   \n",
       "1             0.865429        0.355153            0.859667      0.447496   \n",
       "4             0.874143        0.356638            0.857333      0.479806   \n",
       "8             0.862857        0.358729            0.854667      0.471729   \n",
       "7             0.928714        0.547901            0.823000      0.491115   \n",
       "5             0.999143        1.845069            0.809667      0.508885   \n",
       "6             0.993429        1.080608            0.804000      0.484653   \n",
       "\n",
       "   Precision score   F score  \n",
       "3         0.782609  0.583587  \n",
       "2         0.772118  0.580645  \n",
       "1         0.778090  0.568205  \n",
       "4         0.736973  0.581213  \n",
       "8         0.728180  0.572549  \n",
       "7         0.584615  0.533802  \n",
       "5         0.541237  0.524563  \n",
       "6         0.527241  0.505051  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df1 = results_df.sort_values(by = ['Test data-Accuracy', 'F score'], ascending = False)\n",
    "results_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations - with all one hot encoding for Tenure etc , and with normalization , results are best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
